#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Extracteur et analyseur de menus de restaurants utilisant GPT
Version optimis√©e avec une approche multi-phase et un chunking intelligent.

Cette version apporte plusieurs am√©liorations critiques:
1. Approche multi-phase: identification des sections ‚Üí extraction des plats ‚Üí structuration finale
2. Chunks plus petits (800 caract√®res) pour am√©liorer la performance
3. Prompts adapt√©s √† chaque √©tape pour maximiser les performances
4. Syst√®me de d√©tection d'erreurs intelligent
5. Support am√©lior√© pour Google Drive et autres types de fichiers
"""

import os
import re
import time
import json
import hashlib
import requests
import tempfile
from bs4 import BeautifulSoup
import fitz  # PyMuPDF
from bson.objectid import ObjectId
from urllib.parse import urljoin
from PIL import Image
import io
import logging
from dotenv import load_dotenv
from pymongo import MongoClient
import openai
import pickle

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("menu_processor_gpt_enhanced.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Ajouter le chemin scripts/Restauration/ au PYTHONPATH
import sys
import os
restauration_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'Restauration')
if restauration_path not in sys.path:
    sys.path.append(restauration_path)

# ---- Configuration des r√©pertoires ----
TMP_DIR = "tmp_files"
TMP_PDF_DIR = os.path.join(TMP_DIR, "pdf")
TMP_IMG_DIR = os.path.join(TMP_DIR, "img")
CACHE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "api_cache")
CHECKPOINTS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "checkpoints")

# Cr√©ation de tous les r√©pertoires n√©cessaires
for directory in [TMP_DIR, TMP_PDF_DIR, TMP_IMG_DIR, CACHE_DIR, CHECKPOINTS_DIR]:
    os.makedirs(directory, exist_ok=True)

# ---- Fonctions utilitaires pour cache et checkpoints ----
def sanitize_filename(filename):
    """
    Nettoie un nom de fichier pour qu'il soit valide dans le syst√®me de fichiers
    - Remplace les caract√®res sp√©ciaux par '_'
    - Tronque les noms trop longs avec un hash
    """
    # Remplacer les caract√®res non autoris√©s dans les noms de fichiers
    import re
    safe_name = re.sub(r'[\\/*?:"<>|]', '_', str(filename))
    safe_name = safe_name.replace('http://', 'http_').replace('https://', 'https_')
    safe_name = safe_name.replace('/', '_').replace('\\', '_')
    
    # Limiter la longueur du nom de fichier
    if len(safe_name) > 100:
        # G√©n√©rer un hash pour la partie tronqu√©e
        hash_suffix = hashlib.md5(filename.encode()).hexdigest()[:10]
        safe_name = f"{safe_name[:50]}_{hash_suffix}"
    
    return safe_name

def get_from_cache(key, max_age_hours=24, prefix=""):
    """R√©cup√®re une valeur depuis le cache si elle existe et n'est pas expir√©e"""
    # Ajouter le pr√©fixe au nom du fichier et le nettoyer
    raw_filename = f"{prefix}_{key}" if prefix else str(key)
    safe_filename = sanitize_filename(raw_filename)
    cache_file = os.path.join(CACHE_DIR, f"{safe_filename}.json")
    
    if os.path.exists(cache_file):
        file_age_hours = (time.time() - os.path.getmtime(cache_file)) / 3600
        
        # V√©rifier si le cache est expir√©
        if file_age_hours <= max_age_hours:
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Erreur lors de la lecture du cache: {e}")
                
    return None

def save_to_cache(key, value, prefix=""):
    """Sauvegarde une valeur dans le cache"""
    # Ajouter le pr√©fixe au nom du fichier et le nettoyer
    raw_filename = f"{prefix}_{key}" if prefix else str(key)
    safe_filename = sanitize_filename(raw_filename)
    cache_file = os.path.join(CACHE_DIR, f"{safe_filename}.json")
    
    try:
        # S'assurer que le r√©pertoire parent existe
        os.makedirs(os.path.dirname(cache_file), exist_ok=True)
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(value, f, ensure_ascii=False)
        return True
    except Exception as e:
        logger.warning(f"Erreur lors de l'√©criture dans le cache: {e}")
        return False

def save_checkpoint(checkpoint_data, checkpoint_name="gpt_progress"):
    """Sauvegarde l'√©tat d'avancement pour pouvoir reprendre apr√®s d√©connexion"""
    checkpoint_file = os.path.join(CHECKPOINTS_DIR, f"{checkpoint_name}.pkl")
    
    try:
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(checkpoint_data, f)
        logger.info(f"Checkpoint sauvegard√© dans {checkpoint_file}")
        return True
    except Exception as e:
        logger.warning(f"Erreur lors de la sauvegarde du checkpoint: {e}")
        return False

def load_checkpoint(checkpoint_name="gpt_progress"):
    """Charge le dernier √©tat d'avancement sauvegard√©"""
    checkpoint_file = os.path.join(CHECKPOINTS_DIR, f"{checkpoint_name}.pkl")
    
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'rb') as f:
                checkpoint_data = pickle.load(f)
            logger.info(f"Checkpoint charg√© depuis {checkpoint_file}")
            return checkpoint_data
        except Exception as e:
            logger.warning(f"Erreur lors du chargement du checkpoint: {e}")
            
    logger.info("Aucun checkpoint trouv√©, d√©marrage depuis le d√©but")
    return None

# Charger les variables d'environnement
load_dotenv()

# Configuration OpenAI
# Cl√© API OpenAI en dur pour le test
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# En production, il faudrait plut√¥t utiliser:
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# D√©terminer la version du client OpenAI et configurer en cons√©quence
client = None
OPENAI_API_VERSION = "unknown"

try:
    # Tenter d'initialiser avec la nouvelle API (v1.0+)
    if hasattr(openai, 'OpenAI'):
        client = openai.OpenAI(api_key=OPENAI_API_KEY)
        OPENAI_API_VERSION = "v1"
        logger.info("Utilisation de l'API OpenAI v1.0+")
    else:
        # Fallback vers l'ancienne API (v0.28.x)
        openai.api_key = OPENAI_API_KEY
        OPENAI_API_VERSION = "legacy"
        logger.info("Utilisation de l'API OpenAI legacy (v0.28.x)")
except Exception as e:
    logger.warning(f"Erreur lors de l'initialisation d'OpenAI: {e}. Utilisation du mode legacy.")
    try:
        # Derni√®re tentative avec m√©thode legacy
        openai.api_key = OPENAI_API_KEY
        OPENAI_API_VERSION = "legacy"
        logger.info("Fallback: utilisation de l'API OpenAI legacy")
    except Exception as e2:
        logger.error(f"√âchec de l'initialisation d'OpenAI dans tous les modes: {e2}")

# --- Configuration MongoDB ---
# URI MongoDB en dur pour le test
MONGO_URI = "mongodb+srv://remibarbier:Calvi8Pierc2@lieuxrestauration.szq31.mongodb.net/?retryWrites=true&w=majority&appName=lieuxrestauration"
DB_NAME = "Restauration_Officielle"
COLLECTION_NAME = "producers"

# Cl√© API Google Cloud Vision en dur pour le test
GOOGLE_VISION_API_KEY = os.getenv("GOOGLE_VISION_API_KEY")

# --- Fonctions de base de donn√©es ---
def get_db_connection():
    """√âtablit une connexion √† MongoDB"""
    client = MongoClient(MONGO_URI)
    db = client[DB_NAME]
    collection = db[COLLECTION_NAME]
    logger.info(f"Connexion √©tablie √† MongoDB: {DB_NAME}.{COLLECTION_NAME}")
    return db, collection

# √âtablir la connexion √† MongoDB
db, collection = get_db_connection()

# --- Fonction OpenAI pour g√©n√©ration de texte ---
def generate_ai_response_gpt(prompt, max_tokens=1024, temperature=0.7, model="gpt-3.5-turbo", retry_limit=3):
    """
    G√©n√®re une r√©ponse en utilisant l'API OpenAI (GPT).
    
    Args:
        prompt (str): Le prompt √† envoyer au mod√®le
        max_tokens (int): Nombre maximum de tokens √† g√©n√©rer
        temperature (float): Temp√©rature pour le sampling (0.0 = d√©terministe, > 0.0 = plus cr√©atif)
        model (str): Mod√®le GPT √† utiliser (par d√©faut: gpt-3.5-turbo)
        retry_limit (int): Nombre maximum de tentatives en cas d'erreur
        
    Returns:
        str: La r√©ponse g√©n√©r√©e ou une cha√Æne vide en cas d'√©chec
    """
    # Calculer un hash du prompt pour le caching
    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
    cache_key = f"gpt_{prompt_hash}_{max_tokens}_{temperature}_{model}"
    
    # V√©rifier dans le cache d'abord
    cached_response = get_from_cache(cache_key, max_age_hours=72, prefix="ai_responses")
    if cached_response:
        logger.info("Utilisation d'une r√©ponse mise en cache pour √©conomiser du calcul")
        return cached_response
    
    attempt = 0
    while attempt < retry_limit:
        try:
            # Utiliser l'API appropri√©e selon la version
            if OPENAI_API_VERSION == "v1":
                # Nouvelle API (v1.0+)
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "Vous √™tes un assistant expert en analyse de menus de restaurants."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                # Extraire la r√©ponse
                result = response.choices[0].message.content.strip()
            else:
                # Ancienne API (v0.28.x)
                response = openai.ChatCompletion.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "Vous √™tes un assistant expert en analyse de menus de restaurants."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                # Extraire la r√©ponse
                result = response.choices[0].message['content'].strip()
            
            # Sauvegarder dans le cache
            save_to_cache(cache_key, result, prefix="ai_responses")
            
            return result
        
        except Exception as e:
            attempt += 1
            error_msg = str(e)
            
            # Log l'erreur et attendre avant de r√©essayer
            logger.warning(f"Erreur lors de l'appel √† l'API GPT (tentative {attempt}/{retry_limit}): {error_msg}")
            
            # Attendre de plus en plus longtemps entre les tentatives
            if attempt < retry_limit:
                sleep_time = 2 ** attempt  # Backoff exponentiel: 2, 4, 8, 16...
                logger.info(f"Nouvelle tentative dans {sleep_time} secondes...")
                time.sleep(sleep_time)
    
    logger.error(f"√âchec de l'appel √† GPT apr√®s {retry_limit} tentatives")
    return ""

# --- Fonctions pour les requ√™tes API ---
def make_api_request(url, params=None, method="GET", timeout=10, retries=3, backoff_factor=2):
    """
    Effectue une requ√™te API avec gestion des erreurs et retry.
    
    Args:
        url (str): URL de l'API
        params (dict): Param√®tres de la requ√™te
        method (str): M√©thode HTTP ("GET" ou "POST")
        timeout (int): D√©lai d'attente en secondes
        retries (int): Nombre de tentatives en cas d'√©chec
        backoff_factor (int): Facteur d'attente entre les tentatives
    
    Returns:
        dict/None: R√©ponse JSON ou None en cas d'√©chec
    """
    attempt = 0
    while attempt < retries:
        try:
            if method.upper() == "GET":
                response = requests.get(url, params=params, timeout=timeout)
            else:  # POST
                response = requests.post(url, json=params, timeout=timeout)
            
            response.raise_for_status()
            return response.json()
        
        except requests.exceptions.Timeout:
            attempt += 1
            logger.warning(f"Timeout lors de la tentative {attempt} pour {url}. R√©essayer...")
            time.sleep(backoff_factor * attempt)
        
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur lors de la connexion √† {url}: {e}")
            break
    
    logger.error(f"√âchec de la requ√™te API apr√®s {retries} tentatives pour {url}")
    return None

# --- Fonctions Utilitaires ---
def is_valid_url(url):
    """V√©rifie si une URL est valide et bien form√©e."""
    if not url:
        return False
    
    # V√©rifier le format de base de l'URL
    if not isinstance(url, str):
        return False
    
    # V√©rifier que l'URL commence par http:// ou https://
    if not url.startswith(('http://', 'https://')):
        return False
    
    # V√©rifier la syntaxe de l'URL avec une expression r√©guli√®re basique
    url_pattern = re.compile(
        r'^(?:http|https)://'  # http:// ou https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|'  # domaine
        r'localhost|'  # localhost
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ou adresse IP
        r'(?::\d+)?'  # port optionnel
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)
    
    return url_pattern.match(url) is not None

def fetch_restaurant_websites(limit=3, processed_only=True):
    """
    R√©cup√®re les restaurants ayant un site web non vide et valide dans MongoDB.
    Avec l'option processed_only=False, r√©cup√®re seulement les restaurants
    sans menus_structures.
    """
    try:
        # Requ√™te pour r√©cup√©rer les restaurants avec un site web non vide et non null
        query = {
            "website": {
                "$exists": True,
                "$nin": ["", None]
            }
        }
        if not processed_only:
            query["menus_structures"] = {"$exists": False}
        
        all_restaurants = list(collection.find(query, {"_id": 1, "name": 1, "website": 1, "rating": 1}).limit(limit))
        
        # Filtrer pour garder uniquement les restaurants avec des URLs valides
        restaurants = []
        for r in all_restaurants:
            if is_valid_url(r.get("website")):
                restaurants.append(r)
            else:
                logger.warning(f"Restaurant ignor√© - URL invalide: {r.get('name')} - URL: {r.get('website')}")
        
        logger.info(f"Trouv√© {len(restaurants)} restaurants avec un site web valide")
        
        # Afficher les 3 premiers sites web pour debug
        for i, r in enumerate(restaurants[:3]):
            logger.info(f"Restaurant {i+1}: {r.get('name')} - Site web: {r.get('website')}")
        
        return restaurants
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des restaurants : {e}")
        return []

def extract_links_from_website(url, retries=3, backoff_factor=2):
    """
    Extrait tous les liens d'un site web (PDFs, plateformes externes, images) avec gestion de cache et des retries.
    """
    if not is_valid_url(url):
        logger.error(f"URL invalide, impossible d'extraire les liens: {url}")
        return []

    cache_key = f"links_{url}"
    cached_links = get_from_cache(cache_key, max_age_hours=168, prefix="websites")
    if cached_links:
        return cached_links

    external_platforms = [
        'drive.google.com', 'dropbox.com', 'docdroid.net',
        'calameo.com', 'issuu.com', 'yumpu.com'
    ]

    attempt = 0
    while attempt < retries:
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }
            response = requests.get(url, headers=headers, timeout=20)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

            links = []

            # Liens classiques <a href=...>
            for link in soup.find_all("a", href=True):
                href = link["href"]
                text = link.get_text(strip=True)

                if href.startswith("/"):
                    href = urljoin(url, href)

                if href.lower().endswith(".pdf") or any(platform in href.lower() for platform in external_platforms):
                    logger.info(f"üîó Lien vers menu d√©tect√© : {href}")
                    links.append({"href": href, "text": text})

            # Images <img src=...>
            for img in soup.find_all("img", src=True):
                img_src = img["src"]
                if not img_src.startswith(("http", "https", "data:")):
                    img_src = urljoin(url, img_src)
                img_alt = img.get("alt", "")
                links.append({"href": img_src, "text": img_alt})

            # Iframes <iframe src=...> (souvent menus embedded type Issuu/Calameo)
            for iframe in soup.find_all("iframe", src=True):
                src = iframe["src"]
                if any(platform in src.lower() for platform in external_platforms):
                    full_url = urljoin(url, src)
                    logger.info(f"üñºÔ∏è Iframe vers menu d√©tect√©e : {full_url}")
                    links.append({"href": full_url, "text": "iframe"})

            save_to_cache(cache_key, links, prefix="websites")
            return links

        except requests.exceptions.Timeout:
            attempt += 1
            logger.warning(f"Timeout lors de la tentative {attempt} pour {url}. R√©essayer...")
            time.sleep(backoff_factor * attempt)
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur lors de la connexion √† {url}: {e}")
            break

    logger.error(f"√âchec de l'extraction des liens apr√®s {retries} tentatives pour {url}")
    return []

def filter_menu_links(all_links, base_url):
    """
    Filtre les liens pour ne conserver que ceux li√©s aux menus et compl√®te les liens relatifs.
    D√©tecte aussi les liens vers des images qui pourraient √™tre des menus.
    Version am√©lior√©e avec une meilleure d√©tection des formats pdf et images, et exclusion des SVG.
    """
    menu_links = []
    seen_links = set()
    
    # Mots-cl√©s pour le menu - √©largi pour meilleure d√©tection
    menu_keywords = [
        "menu", "carte", "plats", "boissons", "pdf", 
        "dejeuner", "diner", "d√©jeuner", "d√Æner", "formule",
        "nos plats", "nos sp√©cialit√©s", "√† la carte", "notre cuisine",
        "entr√©es", "plats principaux", "desserts", "boissons",
        "tarifs", "prix", "emporter", "livraison", "voir le menu", "notre carte", # Nouveaux mots-cl√©s
        "menu du jour", "suggestions"
    ]
    
    # Extensions d'images potentiellement utiles
    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']
    
    # Extensions de documents
    document_extensions = ['.pdf', '.doc', '.docx']
    
    for link in all_links:
        href = link.get("href", "") # Utiliser .get avec d√©faut
        text = link.get("text", "").lower() # Utiliser .get avec d√©faut
        
        # Ignorer les liens vides ou ancres
        if not href or href.startswith('#'):
            continue
            
        # Ignorer explicitement les data URLs SVG
        if href.startswith("data:image/svg+xml"):
            continue
            
        # Convertir en URL absolue si c'est un lien relatif
        if not href.startswith(("http", "https", "data:")): # Garder data: pour les images base64 potentielles
            try:
                href = urljoin(base_url, href)
            except ValueError:
                 logger.warning(f"Impossible de joindre base_url '{base_url}' et href '{link.get('href')}'")
                 continue # Ignorer si l'URL ne peut √™tre form√©e
        
        # √âviter les doublons apr√®s normalisation de l'URL
        if href in seen_links:
            continue
            
        is_relevant = False
        href_lower = href.lower()
        
        # 1. V√©rifier les mots-cl√©s de menu dans le texte du lien ou l'URL
        if any(keyword in text or keyword in href_lower for keyword in menu_keywords):
            is_relevant = True
        
        # 2. V√©rifier si c'est une image qui pourrait √™tre un menu (bas√© sur contexte)
        # Ne pas consid√©rer les data:svg comme image ici
        if any(href_lower.endswith(ext) for ext in image_extensions) or href.startswith("data:image/"):
            # Si le texte (alt) √©voque un menu ou la carte
            if any(keyword in text for keyword in menu_keywords):
                is_relevant = True
            # Ou si le nom de fichier (href) a "menu", "carte", etc.
            elif any(keyword in href_lower for keyword in ["menu", "carte", "tarif"]):
                is_relevant = True
                
        # 3. V√©rifier si c'est un document (PDF, DOC, etc.)
        if any(href_lower.endswith(ext) for ext in document_extensions):
             # Les documents sont souvent pertinents s'ils contiennent les mots-cl√©s
            if any(keyword in href_lower or keyword in text for keyword in menu_keywords):
                is_relevant = True
        
        # 4. Cas sp√©cial: Google Drive et autres plateformes de partage
        if any(platform in href_lower for platform in ['drive.google.com', 'dropbox.com', 'docdroid.net']):
            # Pertinent si le lien ou le texte contient des mots-cl√©s
            if any(keyword in href_lower or keyword in text for keyword in menu_keywords):
                is_relevant = True
        
        # Ajouter le lien s'il est pertinent
        if is_relevant:
            seen_links.add(href)
            menu_links.append({"href": href, "text": link["text"]})
    
    return menu_links

def extract_text_from_google_drive(url):
    """
    Extrait le texte d'un document Google Drive.
    Convertit l'URL de partage en lien direct de t√©l√©chargement.
    G√®re les autorisations et les acc√®s restreints.
    
    Args:
        url (str): L'URL Google Drive (format standard de partage)
        
    Returns:
        str: Le texte extrait du document
    """
    try:
        # V√©rifier si c'est une URL Google Drive
        if not ('drive.google.com' in url.lower() or 'docs.google.com' in url.lower()):
            logger.warning(f"L'URL fournie n'est pas une URL Google Drive: {url}")
            return ""
        
        # Extraire l'ID du document
        file_id = None
        
        # Format: https://drive.google.com/file/d/{FILE_ID}/view
        file_match = re.search(r"/file/d/([a-zA-Z0-9_-]+)", url)
        if file_match:
            file_id = file_match.group(1)
        
        # Format: https://drive.google.com/open?id={FILE_ID}
        elif "open?id=" in url:
            file_id = url.split("open?id=")[1].split("&")[0]
        
        # Format: https://docs.google.com/document/d/{FILE_ID}/edit
        elif "/document/d/" in url:
            file_id = url.split("/document/d/")[1].split("/")[0]
        
        # Format: https://drive.google.com/uc?id={FILE_ID}
        elif "uc?id=" in url or "uc?export=download&id=" in url:
            if "id=" in url:
                file_id = url.split("id=")[1].split("&")[0]
        
        if not file_id:
            logger.error(f"Impossible d'extraire l'ID du document Google Drive: {url}")
            return ""
        
        # Construire l'URL de t√©l√©chargement direct
        download_url = f"https://drive.google.com/uc?export=download&id={file_id}"
        
        logger.info(f"T√©l√©chargement du document Google Drive: {download_url}")
        
        # T√©l√©charger le fichier avec un User-Agent
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # Premi√®re requ√™te pour v√©rifier le type et la taille
        response = requests.get(download_url, headers=headers, stream=True, timeout=30)
        response.raise_for_status()
        
        # V√©rifier si c'est une page de confirmation (fichier volumineux)
        if "Content-Disposition" not in response.headers and "confirm=" in response.text:
            # Extraire le code de confirmation
            confirm_match = re.search(r'confirm=([0-9A-Za-z]+)', response.text)
            if confirm_match:
                confirm_code = confirm_match.group(1)
                download_url = f"{download_url}&confirm={confirm_code}"
                
                # Nouvelle requ√™te avec le code de confirmation
                response = requests.get(download_url, headers=headers, timeout=30)
                response.raise_for_status()
        
        # D√©terminer le type de contenu
        content_type = response.headers.get('content-type', '').lower()
        
        # Traiter selon le type de contenu
        if 'pdf' in content_type or url.lower().endswith('.pdf'):
            # Sauvegarder le PDF temporairement
            temp_path = os.path.join(TMP_PDF_DIR, f"gdrive_{file_id}.pdf")
            with open(temp_path, 'wb') as f:
                f.write(response.content)
            
            # Extraire le texte du PDF
            try:
                pdf = fitz.open(temp_path)
                text = "".join(page.get_text() for page in pdf)
                pdf.close()
                
                # Si peu de texte, essayer OCR
                if len(text.strip()) < 200:
                    text = extract_text_from_pdf(f"file://{temp_path}")
                
                return text
            except Exception as e:
                logger.error(f"Erreur lors de l'extraction PDF Google Drive: {e}")
                # Tentative d'OCR direct sur le PDF
                return extract_text_from_pdf(f"file://{temp_path}")
            
        elif 'image' in content_type or any(url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif']):
            # Sauvegarder l'image temporairement
            temp_path = os.path.join(TMP_IMG_DIR, f"gdrive_{file_id}.jpg")
            with open(temp_path, 'wb') as f:
                f.write(response.content)
                
            # OCR sur l'image
            return extract_text_from_image(f"file://{temp_path}")
            
        elif 'text/plain' in content_type:
            return response.text
            
        elif 'html' in content_type:
            # Utiliser BeautifulSoup pour extraire le texte
            soup = BeautifulSoup(response.content, 'html.parser')
            return soup.get_text(separator="\n", strip=True)
            
        elif 'application/vnd.openxmlformats-officedocument.wordprocessingml.document' in content_type or url.lower().endswith('.docx'):
            # C'est un document Word
            try:
                # Sauvegarder temporairement
                temp_path = os.path.join(TMP_DIR, f"gdrive_{file_id}.docx")
                with open(temp_path, 'wb') as f:
                    f.write(response.content)
                
                # Utiliser docx2txt si disponible
                try:
                    import docx2txt
                    text = docx2txt.process(temp_path)
                    return text
                except ImportError:
                    # Alternative: essayer d'extraire du texte via OCR
                    logger.warning("Module docx2txt non disponible, tentative OCR")
                    return extract_text_from_image(f"file://{temp_path}")
            except Exception as e:
                logger.error(f"Erreur lors de l'extraction DOCX: {e}")
                return ""
        else:
            # Type inconnu, essayer d'extraire comme donn√©es binaires
            temp_path = os.path.join(TMP_DIR, f"gdrive_{file_id}.bin")
            with open(temp_path, 'wb') as f:
                f.write(response.content)
            
            # Essayer les diff√©rentes m√©thodes d'extraction
            try:
                # Essayer PDF
                pdf = fitz.open(temp_path)
                text = "".join(page.get_text() for page in pdf)
                pdf.close()
                return text
            except Exception:
                try:
                    # Essayer OCR
                    return extract_text_from_image(f"file://{temp_path}")
                except Exception as e:
                    logger.error(f"√âchec de l'extraction du document Google Drive: {e}")
                    return ""
    
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction du document Google Drive: {e}")
        return ""

def extract_text_from_link(url):
    """
    Extrait le texte d'un lien, qu'il soit PDF, HTML ou image.
    D√©tecte automatiquement le type de contenu.
    Version am√©lior√©e avec meilleure gestion des erreurs et des types de contenu.
    """
    # Ignorer directement les SVG Data URLs qui ne sont pas support√©es
    if url.startswith("data:image/svg+xml"):
        logger.warning(f"Type de data URL non support√© (SVG) ignor√© : {url[:60]}...")
        return ""
        
    if not is_valid_url(url) and not url.startswith("file://") and not url.startswith("data:"):
        logger.error(f"URL invalide ou non support√©e, impossible d'extraire le texte: {url}")
        return ""
    
    # V√©rifier le cache
    cache_key = f"text_{url}"
    cached_text = get_from_cache(cache_key, max_age_hours=168, prefix="menu_text")
    if cached_text:
        return cached_text
    
    # Nombre maximal de tentatives en cas d'√©chec
    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            # Cas sp√©cial: Google Drive
            if 'drive.google.com' in url.lower() or 'docs.google.com' in url.lower():
                logger.info(f"Extraction d'un document Google Drive: {url}")
                text = extract_text_from_google_drive(url)
                if text:
                    # Nettoyer et sauvegarder dans le cache
                    text = re.sub(r'\n{3,}', '\n\n', text)
                    save_to_cache(cache_key, text, prefix="menu_text")
                    return text
            
            if url.startswith("file://"):
                # Cas d'un fichier local
                local_path = url[7:]  # Enlever le pr√©fixe "file://"
                
                if local_path.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                    text = extract_text_from_image(url)
                elif local_path.lower().endswith('.pdf'):
                    pdf = fitz.open(local_path)
                    text = "".join(page.get_text() for page in pdf)
                    pdf.close()
                else:
                    with open(local_path, 'r', encoding='utf-8', errors='ignore') as f:
                        text = f.read()
            
            else:
                # Cas d'une URL distante
                # Obtenir les en-t√™tes pour v√©rifier le type de contenu
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                    }
                    response = requests.head(url, headers=headers, timeout=15)
                    content_type = response.headers.get('content-type', '').lower()
                except Exception:
                    # Si on ne peut pas obtenir le content-type, on devine √† partir de l'extension
                    content_type = ''
                
                # D√©terminer le type de contenu et extraire le texte en cons√©quence
                if url.lower().endswith(".pdf") or "application/pdf" in content_type:
                    text = extract_text_from_pdf(url)
                elif url.startswith("data:image"):
                    # Cas d'une image encod√©e en base64
                    text = extract_text_from_data_url(url)
                elif (url.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')) or 
                    any(img_type in content_type for img_type in ['image/jpeg', 'image/png', 'image/gif', 'image/webp'])):
                    text = extract_text_from_image(url)
                else:
                    text = extract_text_from_html(url)
            
            if text:
                # Nettoyer le texte (supprimer les lignes vides multiples, etc.)
                text = re.sub(r'\n{3,}', '\n\n', text)
                
                # Sauvegarder dans le cache
                save_to_cache(cache_key, text, prefix="menu_text")
                
                return text
            else:
                # Si le texte est vide, essayer une autre m√©thode
                logger.warning(f"Extraction sans r√©sultat pour {url}, tentative {attempt+1}/{max_retries}")
                
                # Si c'est la derni√®re tentative, essayer OCR en dernier recours
                if attempt == max_retries - 1 and not url.startswith("file://"):
                    try:
                        # T√©l√©charger le contenu et essayer OCR comme dernier recours
                        headers = {
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                        }
                        response = requests.get(url, headers=headers, timeout=30)
                        response.raise_for_status()
                        
                        # Sauvegarder temporairement
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".bin") as temp_file:
                            temp_file.write(response.content)
                            temp_path = temp_file.name
                        
                        # Essayer OCR
                        text = extract_text_from_image(f"file://{temp_path}")
                        
                        # Nettoyer
                        os.unlink(temp_path)
                        
                        if text:
                            save_to_cache(cache_key, text, prefix="menu_text")
                            return text
                    except Exception as e:
                        logger.error(f"√âchec de la tentative OCR finale pour {url}: {e}")
                
                # Attendre un peu avant de r√©essayer
                time.sleep(1)
        
        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"Erreur lors de l'extraction du texte de {url} (tentative {attempt+1}/{max_retries}): {e}")
                time.sleep(2)  # Pause avant de r√©essayer
            else:
                logger.error(f"√âchec final de l'extraction du texte de {url}: {e}")
    
    return ""

def extract_text_from_data_url(data_url):
    """Extrait le texte d'une URL de donn√©es (data URL) contenant une image."""
    try:
        # Extraire le type MIME et les donn√©es encod√©es
        header, encoded = data_url.split(",", 1)
        import base64
        
        # D√©coder les donn√©es base64
        if ";base64" in header:
            decoded = base64.b64decode(encoded)
        else:
            import urllib.parse
            decoded = urllib.parse.unquote_to_bytes(encoded)
        
        # Cr√©er un fichier temporaire pour l'image
        with tempfile.NamedTemporaryFile(delete=False, suffix=".png") as temp_file:
            temp_file.write(decoded)
            temp_path = temp_file.name
        
        # Utiliser extract_text_from_image avec le chemin du fichier temporaire
        text = extract_text_from_image(f"file://{temp_path}")
        
        # Supprimer le fichier temporaire
        os.unlink(temp_path)
        
        return text
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction du texte depuis data URL: {e}")
        return ""

def extract_text_from_html(url):
    """
    Extrait le texte brut d'une page HTML avec focus sur les sections pertinentes pour les menus.
    Version am√©lior√©e avec extraction cibl√©e des sections de menu.
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        # D√©tecter l'encodage
        encoding = response.encoding
        
        soup = BeautifulSoup(response.content, "html.parser")
        
        # Supprimer les √©l√©ments non pertinents
        for element in soup.find_all(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
        
        # Strat√©gie 1: Recherche d'√©l√©ments avec des classes/IDs sp√©cifiques aux menus
        menu_content = ""
        
        # Priorit√© 1: √âl√©ments explicitement identifi√©s comme des menus
        menu_elements = soup.find_all(id=lambda x: x and any(keyword in x.lower() 
                                                  for keyword in ['menu', 'carte', 'food', 'dish', 'plat']))
        menu_elements.extend(soup.find_all(class_=lambda x: x and any(keyword in x.lower() 
                                                         for keyword in ['menu', 'carte', 'food', 'dish', 'plat'])))
        
        if menu_elements:
            for element in menu_elements:
                menu_content += element.get_text(separator="\n", strip=True) + "\n\n"
        
        # Priorit√© 2: Sections avec titres li√©s aux menus
        if not menu_content:
            headers = soup.find_all(['h1', 'h2', 'h3'], string=lambda x: x and any(keyword in x.lower() 
                                                                    for keyword in ['menu', 'carte', 'nos plats', 'formule']))
            for header in headers:
                # Extraire la section qui suit le titre
                section_content = ""
                for sibling in header.next_siblings:
                    if sibling.name in ['h1', 'h2', 'h3']:
                        break
                    if sibling.name:
                        section_content += sibling.get_text(separator="\n", strip=True) + "\n"
                
                if section_content:
                    menu_content += header.get_text(strip=True) + "\n" + section_content + "\n\n"
        
        # Priorit√© 3: √âl√©ments de liste qui ressemblent √† des menus (avec prix)
        if not menu_content:
            price_pattern = re.compile(r'\d+[.,]?\d*\s*(?:‚Ç¨|\$|EUR|euros?)')
            
            for list_element in soup.find_all(['ul', 'ol', 'dl', 'table']):
                list_text = list_element.get_text(separator="\n", strip=True)
                # Si le texte contient des prix, probablement un menu
                if price_pattern.search(list_text):
                    menu_content += list_text + "\n\n"
        
        # Fallback: Utiliser le contenu entier si aucun contenu de menu n'a √©t√© trouv√©
        if not menu_content:
            # Diviser en paragraphes et filtrer ceux qui pourraient contenir des informations de menu
            potential_menu_content = []
            price_pattern = re.compile(r'\d+[.,]?\d*\s*(?:‚Ç¨|\$|EUR|euros?)')
            
            # Parcourir tous les paragraphes et rechercher des patterns de menu
            for p in soup.find_all('p'):
                p_text = p.get_text(strip=True)
                if price_pattern.search(p_text) or re.search(r'\b(?:menu|entr√©e|plat|dessert|boisson)\b', p_text.lower()):
                    potential_menu_content.append(p_text)
            
            if potential_menu_content:
                menu_content = '\n\n'.join(potential_menu_content)
            else:
                # Dernier recours: tout le contenu de la page
                menu_content = soup.get_text(separator="\n", strip=True)
        
        return menu_content
    except Exception as e:
        logger.error(f"[ERREUR] Probl√®me lors de l'extraction HTML ({url}) : {e}")
        return ""

def extract_text_from_image(image_url):
    """
    Extrait le texte d'une image de menu en utilisant l'OCR.
    Version am√©lior√©e avec pr√©traitement d'image et multiples m√©thodes d'OCR.
    """
    # V√©rifier si la fonctionnalit√© IA est activ√©e
    if not AI_ENABLED:
        logger.info("La fonctionnalit√© IA est d√©sactiv√©e. OCR non disponible pour l'image.")
        return ""
        
    try:
        # Cr√©er un nom de fichier unique bas√© sur l'URL
        img_filename = hashlib.md5(image_url.encode()).hexdigest() + ".jpg"
        img_path = os.path.join(TMP_IMG_DIR, img_filename)
        
        # T√©l√©charger l'image si non pr√©sente
        if not os.path.exists(img_path):
            if image_url.startswith("file://"):
                local_path = image_url[7:]  # Enlever le pr√©fixe "file://"
                if os.path.exists(local_path):
                    # Copier le fichier au lieu de le t√©l√©charger
                    from shutil import copyfile
                    copyfile(local_path, img_path)
                else:
                    logger.error(f"Fichier local non trouv√©: {local_path}")
                    return ""
            else:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(image_url, headers=headers, timeout=20)
                response.raise_for_status()
                with open(img_path, "wb") as f:
                    f.write(response.content)
        
        # M√©thode 1: Google Cloud Vision API (si cl√© disponible)
        if GOOGLE_VISION_API_KEY:
            try:
                from google.cloud import vision
                
                client = vision.ImageAnnotatorClient()
                
                with open(img_path, "rb") as image_file:
                    content = image_file.read()
                
                image = vision.Image(content=content)
                response = client.text_detection(image=image)
                texts = response.text_annotations
                
                if texts:
                    # Le premier √©l√©ment contient tout le texte
                    extracted_text = texts[0].description
                    logger.info(f"Texte extrait avec Google Vision: {len(extracted_text)} caract√®res")
                    return extracted_text
            
            except Exception as e:
                logger.error(f"Erreur lors de l'extraction avec Google Vision: {e}")
        
        # M√©thode 2: Utiliser pytesseract (OCR local) avec pr√©traitement avanc√©
        try:
            import pytesseract
            from PIL import Image, ImageEnhance, ImageFilter
            
            img = Image.open(img_path)
            
            # Pr√©traitement de l'image pour am√©liorer l'OCR
            # 1. Redimensionner l'image si trop grande pour am√©liorer la pr√©cision
            if max(img.width, img.height) > 3000:
                ratio = 3000 / max(img.width, img.height)
                new_width = int(img.width * ratio)
                new_height = int(img.height * ratio)
                img = img.resize((new_width, new_height), Image.LANCZOS)
            
            # 2. Conversion en niveaux de gris
            img_gray = img.convert('L')
            
            # 3. Augmentation du contraste
            enhancer = ImageEnhance.Contrast(img_gray)
            img_contrast = enhancer.enhance(2.0)
            
            # 4. Nettet√© am√©lior√©e
            img_sharp = img_contrast.filter(ImageFilter.SHARPEN)
            
            # 5. Binarisation pour aider √† la d√©tection du texte
            threshold = 150
            img_bin = img_sharp.point(lambda p: 255 if p > threshold else 0)
            
            # Effectuer l'OCR avec les diff√©rentes versions pr√©trait√©es
            # commen√ßant par la plus sophistiqu√©e
            ocr_results = []
            
            # Version 1: Image avec nettet√© et binarisation
            ocr_results.append(pytesseract.image_to_string(img_bin, lang='fra+eng'))
            
            # Version 2: Image avec contraste am√©lior√©
            ocr_results.append(pytesseract.image_to_string(img_contrast, lang='fra+eng'))
            
            # Version 3: Image en niveaux de gris simple
            ocr_results.append(pytesseract.image_to_string(img_gray, lang='fra+eng'))
            
            # Version 4: Image originale
            ocr_results.append(pytesseract.image_to_string(img, lang='fra+eng'))
            
            # Trouver la meilleure version (celle avec le plus de texte)
            best_text = max(ocr_results, key=lambda x: len(x.strip()) if x else 0)
            
            if best_text and len(best_text.strip()) > 50:
                logger.info(f"Texte extrait avec Tesseract (meilleure version): {len(best_text)} caract√®res")
                return best_text
        
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction avec Tesseract: {e}")
        
        logger.warning(f"Aucune m√©thode d'OCR n'a pu extraire du texte de {image_url}")
        return ""
    
    except Exception as e:
        logger.error(f"Erreur g√©n√©rale lors de l'extraction d'image ({image_url}): {e}")
        return ""

def extract_text_from_pdf(pdf_url):
    """
    T√©l√©charge et extrait le texte brut d'un PDF, avec OCR pour les PDF scann√©s.
    Version am√©lior√©e avec meilleure d√©tection des PDFs scann√©s.
    """
    try:
        # Cr√©er un nom de fichier unique bas√© sur l'URL
        pdf_filename = hashlib.md5(pdf_url.encode()).hexdigest() + ".pdf"
        pdf_path = os.path.join(TMP_PDF_DIR, pdf_filename)
        
        # T√©l√©charger le PDF si non pr√©sent
        if not os.path.exists(pdf_path):
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(pdf_url, headers=headers, timeout=20)
            response.raise_for_status()
            with open(pdf_path, "wb") as f:
                f.write(response.content)
        
        # Extraire le texte normalement
        pdf = fitz.open(pdf_path)
        text = ""
        
        # V√©rifier chaque page
        for page_num in range(len(pdf)):
            page = pdf[page_num]
            page_text = page.get_text()
            
            # Si la page a peu de texte, elle est probablement scann√©e ou contient une image
            if len(page_text.strip()) < 100:
                logger.info(f"Page {page_num+1} du PDF avec peu de texte, tentative d'OCR...")
                
                # Extraire l'image de la page
                pix = page.get_pixmap(dpi=300)
                img_path = os.path.join(TMP_IMG_DIR, f"{pdf_filename}_page{page_num}.png")
                pix.save(img_path)
                
                # OCR sur l'image
                page_text = extract_text_from_image(f"file://{img_path}")
            
            text += page_text + "\n\n"
        
        pdf.close()
        
        # Si le texte total est trop court, tenter OCR sur tout le document
        if len(text.strip()) < 200:
            logger.info(f"PDF entier avec peu de texte, nouvelle tentative OCR compl√®te...")
            full_text = []
            
            # R√©essayer avec une r√©solution plus √©lev√©e
            for page_num in range(len(pdf)):
                page = pdf[page_num]
                pix = page.get_pixmap(dpi=600)  # R√©solution plus √©lev√©e
                img_path = os.path.join(TMP_IMG_DIR, f"{pdf_filename}_hires_page{page_num}.png")
                pix.save(img_path)
                
                # OCR avec options am√©lior√©es
                page_text = extract_text_from_image(f"file://{img_path}")
                full_text.append(page_text)
            
            # Si l'OCR a donn√© de meilleurs r√©sultats, l'utiliser
            new_text = "\n\n".join(full_text)
            if len(new_text.strip()) > len(text.strip()):
                logger.info(f"OCR haute r√©solution r√©ussi: {len(new_text)} caract√®res")
                text = new_text
        
        return text.strip()
    
    except Exception as e:
        logger.error(f"[ERREUR] Probl√®me lors de l'extraction PDF ({pdf_url}) : {e}")
        return ""

def preprocess_text_for_llm(text):
    """
    Pr√©traite le texte avant de l'envoyer au mod√®le pour am√©liorer les r√©sultats.
    Version am√©lior√©e avec nettoyage plus complet et normalisation.
    
    Args:
        text (str): Le texte brut √† pr√©traiter
        
    Returns:
        str: Le texte pr√©trait√©
    """
    if not text:
        return ""
    
    # 1. Supprimer les caract√®res sp√©ciaux qui pourraient perturber le mod√®le
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)  # Garder uniquement les caract√®res ASCII
    
    # 2. Nettoyer les balises HTML
    text = re.sub(r'<[^>]*>', ' ', text)
    
    # 3. Normaliser les prix (s'assurer que les euros sont correctement format√©s)
    text = re.sub(r'(\d+)[.,](\d+)\s*‚Ç¨', r'\1,\2 ‚Ç¨', text)
    text = re.sub(r'(\d+)[.,](\d+)\s*euros', r'\1,\2 ‚Ç¨', text)
    
    # 4. Remplacer les caract√®res sp√©ciaux par leurs √©quivalents simples
    text = text.replace('≈ì', 'oe').replace('≈í', 'OE')
    text = text.replace('√¶', 'ae').replace('√Ü', 'AE')
    text = text.replace('√ü', 'ss')
    text = text.replace('¬´', '"').replace('¬ª', '"')
    text = text.replace('‚Ä¶', '...')
    
    # 5. Supprimer les caract√®res de contr√¥le
    text = re.sub(r'[\x00-\x1F\x7F]', '', text)
    
    # 6. Normaliser les espaces et sauts de ligne pour une meilleure lisibilit√©
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # 7. Accentuer les d√©limiteurs de sections de menu pour am√©liorer le chunking
    text = re.sub(r'(?i)(menu|carte|entr√©es?|plats?|desserts?|boissons?|vins?)\s*:', r'\n\n\1:\n', text)
    
    # 8. Limiter la longueur totale
    if len(text) > 6000:
        text = text[:6000]
    
    return text.strip()

def detect_non_french_response(text):
    """
    D√©tecte si une r√©ponse est en anglais plut√¥t qu'en fran√ßais.
    Permet de rep√©rer les hallucinations plus efficacement que la simple d√©tection cyrillique.
    
    Args:
        text (str): Le texte √† analyser
        
    Returns:
        bool: True si le texte semble √™tre non-fran√ßais, False sinon
    """
    if not text:
        return False
        
    # Plages Unicode des caract√®res cyrilliques (d√©tection originale)
    cyrillic_pattern = re.compile('[\u0400-\u04FF\u0500-\u052F\u2DE0-\u2DFF\uA640-\uA69F]')
    if cyrillic_pattern.search(text):
        return True
    
    # Liste d'indices de langue anglaise (mots communs anglais)
    english_indicators = [
        " the ", " is ", " are ", " was ", " were ", " be ", " been ", " has ", " have ", " had ",
        " and ", " or ", " but ", " not ", " with ", " from ", " by ", " for ", " at ", " on ", " to ",
        " name ", " price ", " description ", " category ",
        " Appetizer", " Entree", " Seafood", " Dessert", " Salad", "Snack"
    ]
    
    # Liste d'indices de langue fran√ßaise (mots communs fran√ßais)
    french_indicators = [
        " le ", " la ", " les ", " un ", " une ", " des ", " du ", " de ", " est ", " sont ", " a ", " ont ",
        " et ", " ou ", " mais ", " pas ", " avec ", " depuis ", " par ", " pour ", " √† ", " sur ", " au ",
        " nom ", " prix ", " description ", " cat√©gorie ",
        " Entr√©e", " Plat", " Dessert", " Boisson"
    ]
    
    # Compter les indicateurs de langue
    text_lower = " " + text.lower() + " "  # Ajouter espaces pour √©viter faux positifs
    
    english_count = sum(text_lower.count(indicator) for indicator in english_indicators)
    french_count = sum(text_lower.count(indicator) for indicator in french_indicators)
    
    # Pr√©sence de JSON en anglais
    json_english_indicators = ["name", "price", "description", "category"]
    json_english_count = sum(text.count(indicator) for indicator in json_english_indicators)
    
    # Si le texte contient des indicateurs JSON anglais mais peu de fran√ßais
    if json_english_count > 1 and french_count < 5:
        logger.warning(f"D√©tection de structure JSON en anglais ({json_english_count} indicateurs)")
        return True
    
    # Si le ratio anglais/fran√ßais est √©lev√©
    if english_count > 0 and french_count > 0:
        ratio = english_count / french_count
        if ratio > 1.5:  # Si indicateurs anglais > 1.5 * indicateurs fran√ßais
            logger.warning(f"Ratio anglais/fran√ßais √©lev√©: {ratio:.2f} ({english_count}/{french_count})")
            return True
    
    # Si beaucoup d'anglais et peu ou pas de fran√ßais
    if english_count > 10 and french_count < 3:
        logger.warning(f"Beaucoup d'anglais ({english_count}) et peu de fran√ßais ({french_count})")
        return True
    
    return False

def contains_cyrillic(text):
    """
    D√©tecte si un texte contient des caract√®res cyrilliques.
    Maintenue pour compatibilit√© avec le code existant.
    
    Args:
        text (str): Le texte √† v√©rifier
        
    Returns:
        bool: True si le texte contient des caract√®res cyrilliques, False sinon
    """
    if not text:
        return False
        
    # Plages Unicode des caract√®res cyrilliques
    cyrillic_pattern = re.compile('[\u0400-\u04FF\u0500-\u052F\u2DE0-\u2DFF\uA640-\uA69F]')
    return bool(cyrillic_pattern.search(text))

def chunk_text(text, max_chunk_size=800, overlap=150):
    """
    Divise un texte en chunks de taille maximale sp√©cifi√©e avec un chevauchement intelligent.
    Version optimis√©e avec meilleure gestion des fronti√®res naturelles du texte.
    """
    if not text or len(text) <= max_chunk_size:
        return [text]
        
    chunks = []
    
    # Liste des d√©limiteurs par ordre de priorit√© pour une meilleure segmentation
    delimiters = [
        ('\n\n\n', 3),  # Sections principales (triple saut de ligne)
        ('\n\n', 2),    # Paragraphes (double saut de ligne)
        ('\n', 1),      # Lignes (saut de ligne simple)
        ('. ', 2),      # Phrases (point + espace)
        ('! ', 2),      # Phrases exclamatives
        ('? ', 2),      # Phrases interrogatives
        (', ', 2),      # Virgules (moins id√©al mais acceptable)
        (' ', 1)        # Dernier recours: couper aux espaces
    ]
    
    start = 0
    while start < len(text):
        # Position de fin maximale pour ce chunk
        max_end = min(start + max_chunk_size, len(text))
        
        # Position o√π on va effectivement couper (par d√©faut, la fin maximale)
        end = max_end
        
        # Si on n'est pas √† la fin du texte, chercher un point de coupure naturel
        if max_end < len(text):
            # Essayer chaque d√©limiteur par ordre de priorit√©
            for delimiter, extra_chars in delimiters:
                # Rechercher le d√©limiteur en partant de la fin du chunk potentiel
                break_pos = text.rfind(delimiter, start, max_end)
                
                # Si trouv√© et suffisamment loin du d√©but (au moins 40% de la taille maximale)
                # pour √©viter des chunks trop petits
                min_acceptable = start + int(max_chunk_size * 0.4)
                if break_pos > min_acceptable:
                    end = break_pos + extra_chars
                    break
        
        # Ajouter le chunk au r√©sultat
        chunks.append(text[start:end])
        
        # Calculer le d√©but du prochain chunk avec chevauchement intelligent
        # Le chevauchement devrait √™tre plus grand pour les sections importantes (menus)
        # et plus petit pour le texte g√©n√©ral
        if '\n\n' in text[max(start, end - overlap):end]:
            # Si le chevauchement contient un paragraphe, r√©duire le chevauchement
            # pour √©viter de dupliquer des sections enti√®res
            overlap_adjusted = min(overlap, 100)
        else:
            # Sinon, utiliser le chevauchement standard
            overlap_adjusted = overlap
            
        start = max(start + 1, end - overlap_adjusted)
    
    return chunks

def extract_json_from_text(text):
    """
    Extrait et valide une structure JSON √† partir d'un texte.
    Utilise plusieurs techniques pour trouver et r√©parer le JSON.
    
    Returns:
        dict/None: Le dictionnaire JSON extrait ou None si impossible √† extraire
    """
    if not text:
        return None
    
    # 1. Tenter d'extraire le JSON en utilisant diff√©rents patterns
    json_patterns = [
        # Pattern 1: Recherche un JSON entour√© par des accolades, en tenant compte des espaces/newlines
        r'({[\s\S]*?})(?:\s*$|\n)',
        # Pattern 2: Recherche un JSON complet qui commence par { et se termine par }
        r'({[\s\S]*})(?:\s*$)',
        # Pattern 3: Plus strict, recherche "{ ... }"
        r'({[^{]*?})(?:\s*$|\n)'
    ]
    
    json_text = None
    for pattern in json_patterns:
        matches = re.findall(pattern, text)
        if matches:
            for match in matches:
                try:
                    # Essayer de parser le JSON
                    data = json.loads(match)
                    # Si on arrive ici, c'est que le JSON est valide
                    json_text = match
                    break
                except json.JSONDecodeError:
                    # Continuer avec le prochain match
                    continue
            
            if json_text:
                break
    
    # Si on n'a pas trouv√© de JSON valide, essayer des corrections
    if not json_text:
        # Essayer de corriger les probl√®mes de JSON courants
        # 1. Probl√®me: Guillemets simples au lieu de doubles
        corrected_text = text.replace("'", '"')
        
        # 2. Probl√®me: Points-virgules √† la fin des lignes
        corrected_text = re.sub(r';\s*\n', ',\n', corrected_text)
        
        # 3. Probl√®me: Propri√©t√©s sans guillemets
        corrected_text = re.sub(r'(\s)([a-zA-Z_][a-zA-Z0-9_]*)(\s*:)', r'\1"\2"\3', corrected_text)
        
        # 4. Probl√®me: Virgules finales dans les objets ou tableaux
        corrected_text = re.sub(r',(\s*[}\]])', r'\1', corrected_text)
        
        # 5. Probl√®me: Noms de variables avant le JSON (ex: result = { ... })
        corrected_text = re.sub(r'^.*?=\s*({.*}).*$', r'\1', corrected_text, flags=re.DOTALL)
        
        # Chercher √† nouveau des structures JSON
        for pattern in json_patterns:
            matches = re.findall(pattern, corrected_text)
            if matches:
                for match in matches:
                    try:
                        # Essayer de parser le JSON corrig√©
                        data = json.loads(match)
                        # Si on arrive ici, c'est que le JSON est valide
                        json_text = match
                        break
                    except json.JSONDecodeError:
                        # Continuer avec le prochain match
                        continue
                
                if json_text:
                    break
    
    # Si nous avons une correspondance JSON valide, la charger
    if json_text:
        try:
            return json.loads(json_text)
        except json.JSONDecodeError:
            # Derni√®re tentative: essayer d'identifier et de corriger les probl√®mes restants
            try:
                # 1. Remplacer les caract√®res non ASCII probl√©matiques
                clean_json = re.sub(r'[^\x00-\x7F]+', ' ', json_text)
                # 2. √âchapper les guillemets dans les cha√Ænes
                clean_json = re.sub(r'(?<!\\)"(?=.*?:)', r'\"', clean_json)
                
                return json.loads(clean_json)
            except json.JSONDecodeError:
                # Encore une tentative: parser le JSON ligne par ligne
                try:
                    # Transformer en une seule ligne et nettoyer
                    json_oneline = json_text.replace('\n', ' ').replace('\r', '')
                    json_oneline = re.sub(r'\s+', ' ', json_oneline)
                    
                    # Corriger manuellement certaines structures
                    json_oneline = re.sub(r',\s*}', '}', json_oneline)
                    json_oneline = re.sub(r',\s*]', ']', json_oneline)
                    
                    # R√©essayer de parser
                    return json.loads(json_oneline)
                except json.JSONDecodeError:
                    logger.error("Impossible de corriger le JSON apr√®s plusieurs tentatives")
                    return None
    
    logger.error("Aucune structure JSON valide trouv√©e dans le texte")
    return None

def is_valid_menu_result(result):
    """
    V√©rifie si un r√©sultat d'extraction de menu est valide et non vide.
    Version am√©lior√©e avec validation plus stricte.
    """
    if not result or not isinstance(result, dict):
        return False
    
    # V√©rifier que les cl√©s obligatoires sont pr√©sentes
    required_keys = ["Menus Globaux", "Plats Ind√©pendants"]
    if not all(key in result for key in required_keys):
        return False
    
    # V√©rifier que les valeurs sont des listes
    if not (isinstance(result["Menus Globaux"], list) and isinstance(result["Plats Ind√©pendants"], list)):
        return False
    
    # V√©rifier qu'au moins une des listes contient quelque chose
    if not (len(result["Menus Globaux"]) > 0 or len(result["Plats Ind√©pendants"]) > 0):
        return False
    
    # V√©rifier la structure interne des menus
    for menu in result.get("Menus Globaux", []):
        if not isinstance(menu, dict) or "nom" not in menu:
            return False
    
    # V√©rifier la structure interne des plats
    for plat in result.get("Plats Ind√©pendants", []):
        if not isinstance(plat, dict) or "nom" not in plat:
            return False
    
    return True

def is_rich_enough_result(result):
    """
    V√©rifie si un r√©sultat est suffisamment riche en informations.
    """
    if not result or not isinstance(result, dict):
        return False
    
    # Compter les menus globaux
    menu_count = len(result.get("Menus Globaux", []))
    
    # Compter les plats ind√©pendants
    dish_count = len(result.get("Plats Ind√©pendants", []))
    
    # Consid√©r√© comme riche si au moins 5 √©l√©ments au total
    return (menu_count + dish_count) >= 5

def is_better_result(result1, result2):
    """
    Compare deux r√©sultats et d√©termine lequel est meilleur.
    """
    if not is_valid_menu_result(result1):
        return False
    if not is_valid_menu_result(result2):
        return True
    
    # Compter les √©l√©ments dans chaque r√©sultat
    count1 = len(result1.get("Menus Globaux", [])) + len(result1.get("Plats Ind√©pendants", []))
    count2 = len(result2.get("Menus Globaux", [])) + len(result2.get("Plats Ind√©pendants", []))
    
    # Le r√©sultat avec plus d'√©l√©ments est consid√©r√© meilleur
    if count1 > count2:
        return True
    elif count1 < count2:
        return False
    
    # Si m√™me nombre d'√©l√©ments, v√©rifier la richesse des d√©tails
    # (existence de descriptions, cat√©gories, etc.)
    details1 = sum(1 for plat in result1.get("Plats Ind√©pendants", []) 
                  if plat.get("description", "") or plat.get("cat√©gorie", ""))
    details2 = sum(1 for plat in result2.get("Plats Ind√©pendants", []) 
                  if plat.get("description", "") or plat.get("cat√©gorie", ""))
    
    return details1 >= details2

def generate_menu_structure_with_openai(text, restaurant_name, is_chunk=False, chunk_num=0, total_chunks=1):
    """
    G√©n√®re une structure de menu √† partir d'un texte en utilisant OpenAI comme fallback
    quand Mistral √©choue. G√®re la compatibilit√© avec les API OpenAI v1.0+ et <=0.28.
    
    Args:
        text (str): Le texte brut du menu
        restaurant_name (str): Nom du restaurant
        is_chunk (bool): Indique si le texte est un chunk d'un document plus grand
        chunk_num (int): Num√©ro du chunk actuel (si is_chunk=True)
        total_chunks (int): Nombre total de chunks (si is_chunk=True)
        
    Returns:
        dict: Structure du menu ou None si erreur
    """
    if not ENABLE_OPENAI_FALLBACK:
        return None
        
    try:
        # V√©rifier le cache
        chunk_suffix = f"_chunk{chunk_num}" if is_chunk else ""
        cache_key = f"openai_menu_{hashlib.md5((restaurant_name + text[:100]).encode()).hexdigest()}{chunk_suffix}"
        cached_result = get_from_cache(cache_key, max_age_hours=720, prefix="openai_menus")
        if cached_result:
            logger.info(f"Utilisation du cache OpenAI pour le menu de {restaurant_name}{' (chunk '+str(chunk_num)+')' if is_chunk else ''}")
            return cached_result
            
        logger.info(f"G√©n√©ration de structure de menu avec OpenAI pour {restaurant_name}{' (chunk '+str(chunk_num)+')' if is_chunk else ''} (fallback)")
        
        # V√©rifier si l'API OpenAI est correctement configur√©e
        if not OPENAI_API_KEY or (OPENAI_API_VERSION == "unknown"):
            logger.error("API OpenAI non configur√©e correctement, fallback impossible")
            return None
        
        # Ajuster le contenu pour les chunks
        chunk_info = f" (chunk {chunk_num}/{total_chunks})" if is_chunk else ""
        chunk_note = "\nNOTE: Ce texte n'est qu'une partie du menu complet. Extrait uniquement les plats et menus visibles dans ce fragment." if is_chunk else ""
        
        # Limiter la taille du texte pour √©viter les d√©passements de tokens
        text_to_use = text[:3500]
        
        # Cr√©er un prompt adapt√© pour OpenAI
        prompt = f"""R√âPONDEZ EN FRAN√áAIS UNIQUEMENT. N'UTILISEZ QUE L'ALPHABET LATIN.
Tu vas analyser un menu de restaurant et l'organiser en JSON structur√©.

Restaurant: "{restaurant_name}"{chunk_info}

Menu:
```
{text_to_use}
```{chunk_note}

Renvoie uniquement une structure JSON valide avec ce format exact:
{{
  "Menus Globaux": [
    {{
      "nom": "Nom du menu (ex: Menu du jour, Formule midi)",
      "prix": "Prix tel qu'indiqu√©",
      "inclus": [
        {{ "nom": "Plat inclus 1", "description": "Description si pr√©sente" }},
        {{ "nom": "Plat inclus 2", "description": "Description si pr√©sente" }}
      ]
    }}
  ],
  "Plats Ind√©pendants": [
    {{
      "nom": "Nom du plat",
      "cat√©gorie": "Entr√©e/Plat/Dessert/Boisson",
      "prix": "Prix tel qu'indiqu√©",
      "description": "Description compl√®te si pr√©sente"
    }}
  ]
}}

IMPORTANT:
1. Ta r√©ponse doit UNIQUEMENT contenir le JSON, sans aucun texte avant ou apr√®s
2. Ne JAMAIS utiliser de caract√®res cyrilliques ou non-latin
3. Si tu ne trouves pas de menu, renvoie quand m√™me la structure avec des tableaux vides
4. Assure-toi que ton JSON est parfaitement valide (accolades ferm√©es, guillemets coh√©rents)
5. Sois fid√®le au texte: pr√©serve les noms et descriptions tels quels
6. Extrait uniquement les plats et menus que tu vois dans ce texte{" (c'est seulement une partie du menu complet)" if is_chunk else ""}
"""

        # Syst√®me de multi-tentatives et multiple versions API
        for attempt in range(3):  # 3 tentatives
            try:
                if OPENAI_API_VERSION == "v1" and (attempt == 0 or attempt == 1):
                    # Premi√®re tentative: API v1.0+
                    try:
                        logger.info(f"Tentative {attempt+1} avec OpenAI API v1.0+ pour {restaurant_name}")
                        
                        # Configurer les messages pour l'API v1.0+
                        messages = [
                            {"role": "system", "content": "Tu es un assistant sp√©cialis√© dans l'extraction de donn√©es de menus de restaurants. Tu r√©ponds toujours uniquement en JSON valide, jamais en texte."},
                            {"role": "user", "content": prompt}
                        ]
                        
                        response = client.chat.completions.create(
                            model="gpt-3.5-turbo",
                            messages=messages,
                            temperature=0.3,
                            max_tokens=2000
                        )
                        
                        # Extraire le contenu de la r√©ponse avec la nouvelle structure API
                        result_text = response.choices[0].message.content
                        break  # Sortir de la boucle si succ√®s
                    except Exception as e:
                        logger.warning(f"√âchec de la tentative {attempt+1} avec API v1.0+: {e}")
                        if attempt < 2:  # Ne pas lever d'exception si ce n'est pas la derni√®re tentative
                            continue
                        raise
                else:
                    # Derni√®re tentative: API legacy (v0.x)
                    try:
                        logger.info(f"Tentative {attempt+1} avec OpenAI API legacy pour {restaurant_name}")
                        
                        # Configuration pour l'API legacy
                        openai.api_key = OPENAI_API_KEY
                        
                        # V√©rifier quelle m√©thode/classe est disponible
                        if hasattr(openai, 'ChatCompletion'):
                            # Format v0.28.x
                            response = openai.ChatCompletion.create(
                                model="gpt-3.5-turbo",
                                messages=[
                                    {"role": "system", "content": "Tu es un assistant sp√©cialis√© dans l'extraction de donn√©es de menus de restaurants. Tu r√©ponds toujours uniquement en JSON valide, jamais en texte."},
                                    {"role": "user", "content": prompt}
                                ],
                                temperature=0.3,
                                max_tokens=2000
                            )
                            result_text = response.choices[0]["message"]["content"]
                        else:
                            # Encore plus ancien
                            response = openai.Completion.create(
                                engine="text-davinci-003",
                                prompt=f"Extrait le menu du restaurant sous forme JSON:\n\n{prompt}",
                                temperature=0.3,
                                max_tokens=2000
                            )
                            result_text = response.choices[0].text
                        
                        break  # Sortir de la boucle si succ√®s
                    except Exception as e:
                        logger.error(f"√âchec de toutes les tentatives OpenAI pour {restaurant_name}: {e}")
                        return None
            except Exception as e:
                logger.warning(f"Erreur lors de la tentative {attempt+1} avec OpenAI pour {restaurant_name}: {e}")
                if attempt == 2:  # Si c'est la derni√®re tentative
                    logger.error(f"Toutes les tentatives OpenAI ont √©chou√© pour {restaurant_name}")
                    return None
        
        # Extraire et valider le JSON
        result = extract_json_from_text(result_text)
        
        if not result:
            logger.error(f"Extraction JSON OpenAI √©chou√©e pour {restaurant_name}")
            return None
            
        # V√©rifier et corriger la structure
        if "Menus Globaux" not in result:
            result["Menus Globaux"] = []
        if "Plats Ind√©pendants" not in result:
            result["Plats Ind√©pendants"] = []
            
        # Sauvegarder dans le cache
        save_to_cache(cache_key, result, prefix="openai_menus")
        
        return result
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration avec OpenAI pour {restaurant_name}: {e}")
        return None

def identify_menu_sections(text, restaurant_name, chunk_num=1, total_chunks=1):
    """
    Identifie les sections de menu dans un texte.
    
    Cette fonction utilise GPT pour trouver et segmenter les sections d'un menu.
    Si plusieurs chunks sont fournis, elle adapte son prompt pour indiquer au mod√®le
    qu'il travaille sur une partie du document.
    
    Args:
        text (str): Texte du menu √† analyser
        restaurant_name (str): Nom du restaurant (pour la contextualisation)
        chunk_num (int): Num√©ro du chunk actuel (pour les gros documents)
        total_chunks (int): Nombre total de chunks (pour les gros documents)
        
    Returns:
        dict: Sections identifi√©es avec leurs types
    """
    # Pr√©traiter le texte pour le LLM
    processed_text = preprocess_text_for_llm(text)
    
    # Cr√©er un hash pour le cache
    text_hash = hashlib.md5(processed_text.encode()).hexdigest()
    cache_key = f"sections_{text_hash}_{restaurant_name}"
    
    # V√©rifier si on a d√©j√† un r√©sultat en cache
    cached_result = get_from_cache(cache_key, max_age_hours=720, prefix="gpt_sections")
    if cached_result:
        logger.info(f"Utilisation du r√©sultat en cache pour l'identification des sections de {restaurant_name}")
        return cached_result
    
    # Construire un prompt pour identifier les sections
    prompt = f"""Voici le texte d'un menu du restaurant "{restaurant_name}". 
{'' if total_chunks == 1 else f'IMPORTANT: Ce texte est le chunk {chunk_num} sur {total_chunks}, donc il peut √™tre incomplet ou fragment√©.'}

Ta t√¢che est d'identifier toutes les SECTIONS du menu (comme entr√©es, plats principaux, desserts, boissons, etc.).
N'extrais pas encore les plats individuels, seulement les cat√©gories principales.

Pour chaque section que tu identifies, d√©termine son type parmi:
- STARTERS: entr√©es, ap√©ritifs, hors d'≈ìuvres
- MAIN_COURSES: plats principaux, sp√©cialit√©s, grillades
- SIDE_DISHES: accompagnements, garnitures
- DESSERTS: desserts, p√¢tisseries, glaces
- BEVERAGES: boissons, vins, cocktails
- BREAKFAST: petit-d√©jeuner, brunch matinal
- KIDS: menu enfant, plats pour enfants
- COMBO_MEALS: menus, formules, combinaisons
- SPECIALS: sp√©cialit√©s, suggestions du chef, plats du jour
- OTHER: toute autre section qui ne correspond pas aux cat√©gories ci-dessus

Voici le texte du menu:
---
{processed_text}
---

R√©ponds au format JSON uniquement avec la structure suivante:
{{
  "sections": [
    {{
      "name": "Nom exact de la section comme dans le texte",
      "type": "CAT√âGORIE_D√âTERMIN√âE",
      "start_index": position de d√©but approximative dans le texte,
      "end_index": position de fin approximative dans le texte
    }},
    // autres sections...
  ]
}}

Si aucune section n'est d√©tectable, r√©ponds avec:
{{ "sections": [] }}

NE FOURNIS PAS d'explications suppl√©mentaires, juste le JSON.
"""
    
    # Premier essai avec param√®tres standard
    result_text = generate_ai_response_gpt(
        prompt=prompt,
        max_tokens=800,
        temperature=0.1,
    )
    
    # Essayer d'extraire le JSON
    result = extract_json_from_text(result_text)
    
    # V√©rifier si le r√©sultat est valide
    if result and "sections" in result and isinstance(result["sections"], list):
        # Sauvegarder en cache
        save_to_cache(cache_key, result, prefix="gpt_sections")
        logger.info(f"Sections identifi√©es pour {restaurant_name}: {len(result['sections'])} sections trouv√©es")
        return result
    
    # Si l'extraction a √©chou√©, essayer avec un prompt simplifi√©
    logger.warning(f"L'extraction des sections a √©chou√© pour {restaurant_name}. Tentative simplifi√©e...")
    
    simplified_prompt = f"""Identifie les sections du menu du restaurant "{restaurant_name}" dans ce texte:
---
{processed_text[:1500]}
---

R√©ponds UNIQUEMENT en JSON:
{{
  "sections": [
    {{
      "name": "Nom de la section",
      "type": "STARTERS/MAIN_COURSES/SIDE_DISHES/DESSERTS/BEVERAGES/BREAKFAST/KIDS/COMBO_MEALS/SPECIALS/OTHER",
      "start_index": 0,
      "end_index": 0
    }}
  ]
}}"""
    
    logger.info(f"Second essai d'identification des sections pour {restaurant_name} (chunk {chunk_num}/{total_chunks})")
    
    result_text = generate_ai_response_gpt(
        prompt=simplified_prompt,
        max_tokens=600,
        temperature=0.1,
    )
    
    # Essayer d'extraire le JSON √† nouveau
    sections_result = extract_json_from_text(result_text)
    
    # V√©rifier si le r√©sultat simplifi√© est valide
    if sections_result and "sections" in sections_result and isinstance(sections_result["sections"], list):
        # Sauvegarder en cache
        save_to_cache(cache_key, sections_result, prefix="gpt_sections")
        logger.info(f"Sections identifi√©es (essai simplifi√©) pour {restaurant_name}: {len(sections_result['sections'])} sections trouv√©es")
        return sections_result
    
    # Fallback: essayer OpenAI GPT
    logger.warning(f"L'extraction des sections a √©galement √©chou√© avec un prompt simplifi√© pour {restaurant_name}")
    
    # Cr√©er un fallback manuel avec une structure vide
    fallback_result = {
        "sections": []
    }
    
    # V√©rifier si le texte est assez long pour probablement contenir un menu
    if len(processed_text) > 200:
        # Ajouter une section g√©n√©rique
        fallback_result["sections"].append({
            "name": "Menu complet",
            "type": "OTHER",
            "start_index": 0,
            "end_index": len(processed_text)
        })
    
    # Sauvegarder ce fallback en cache pour √©viter de r√©essayer
    save_to_cache(cache_key, fallback_result, prefix="gpt_sections")
    
    logger.warning(f"Utilisation d'un fallback pour les sections de {restaurant_name}")
    return fallback_result

def extract_dishes_by_section(text, restaurant_name, sections_info, chunk_num=1, total_chunks=1):
    """
    Extrait les plats par section √† partir du texte et des informations de section.
    
    Cette fonction utilise GPT pour analyser chaque section pr√©c√©demment identifi√©e
    et en extraire les plats, avec d√©tails et prix.
    
    Args:
        text (str): Texte du menu complet
        restaurant_name (str): Nom du restaurant
        sections_info (dict): Informations sur les sections identifi√©es
        chunk_num (int): Num√©ro du chunk actuel
        total_chunks (int): Nombre total de chunks
        
    Returns:
        dict: Structure compl√®te du menu avec sections et plats
    """
    # Pr√©traiter le texte pour le LLM
    processed_text = preprocess_text_for_llm(text)
    
    # Hash pour le cache
    input_hash = hashlib.md5((processed_text + str(sections_info)).encode()).hexdigest()
    cache_key = f"dishes_{input_hash}_{restaurant_name}"
    
    # V√©rifier le cache
    cached_result = get_from_cache(cache_key, max_age_hours=720, prefix="gpt_dishes")
    if cached_result:
        logger.info(f"Utilisation du r√©sultat en cache pour l'extraction des plats de {restaurant_name}")
        return cached_result
    
    # Construire un prompt pour extraire les plats, section par section
    all_sections = sections_info.get("sections", [])
    
    if not all_sections:
        logger.warning(f"Aucune section trouv√©e pour {restaurant_name}, impossible d'extraire les plats")
        return {"menu": []}
    
    # Limiter le nombre de sections √† traiter pour √©viter des prompts trop longs
    sections_to_process = all_sections[:5]  # Limiter √† 5 sections maximum
    
    # Construire le prompt avec les sections d√©tect√©es
    sections_text = ""
    for i, section in enumerate(sections_to_process):
        section_name = section.get("name", f"Section {i+1}")
        section_type = section.get("type", "OTHER")
        
        # Extraire le texte de cette section
        start_idx = max(0, section.get("start_index", 0))
        end_idx = min(len(processed_text), section.get("end_index", len(processed_text)))
        
        # V√©rifier que les indices sont valides
        if start_idx >= end_idx or start_idx >= len(processed_text):
            continue
            
        section_content = processed_text[start_idx:end_idx]
        
        # Ajouter au texte des sections
        sections_text += f"\n--- SECTION: {section_name} (TYPE: {section_type}) ---\n{section_content}\n"
    
    # Prompt principal
    prompt = f"""Tu es un expert en analyse de menus de restaurants. Analyse ce menu du restaurant "{restaurant_name}".
{'' if total_chunks == 1 else f'IMPORTANT: Ce texte est le chunk {chunk_num} sur {total_chunks}, donc il peut √™tre incomplet.'}

Je t'ai d√©j√† identifi√© les sections principales. Pour chaque section, extrais tous les plats avec leurs d√©tails.

{sections_text}

Pour chaque plat, identifie:
1. Le nom exact du plat
2. Sa description (si disponible)
3. Son prix (si disponible)
4. Ses options ou variations (si disponibles)

R√©ponds STRICTEMENT au format JSON suivant:
{{
  "menu": [
    {{
      "section_name": "Nom de la section comme fourni",
      "section_type": "TYPE_DE_SECTION comme fourni",
      "items": [
        {{
          "name": "Nom du plat",
          "description": "Description du plat ou null si non disponible",
          "price": "Prix au format texte (ex: '12,50 ‚Ç¨') ou null",
          "options": ["Option 1", "Option 2"] ou null si pas d'options
        }},
        // autres plats...
      ]
    }},
    // autres sections...
  ]
}}

Si tu ne trouves aucun plat, r√©ponds avec: {{ "menu": [] }}

IMPORTANT: 
- Pour les prix, conserve EXACTEMENT le format du texte original (symbole ‚Ç¨, virgule/point, etc.)
- Ne r√©ponds qu'avec le JSON, sans commentaire ni introduction
- Si une information est absente (description/prix/options), utilise null
"""
    
    # Premier essai avec param√®tres standard
    result_text = generate_ai_response_gpt(
        prompt=prompt,
        max_tokens=1500,  # Plus de tokens car l'extraction est plus d√©taill√©e
        temperature=0.3,
    )
    
    # Essayer d'extraire le JSON
    result = extract_json_from_text(result_text)
    
    # V√©rifier si le r√©sultat est valide
    if is_valid_menu_result(result):
        # Sauvegarder en cache
        save_to_cache(cache_key, result, prefix="gpt_dishes")
        return result
    
    # Si l'extraction a √©chou√©, essayer avec un prompt simplifi√©
    logger.warning(f"L'extraction des plats a √©chou√© pour {restaurant_name}. Tentative simplifi√©e...")
    
    # Simplifier le prompt et se concentrer sur une seule section √† la fois
    simplified_sections = []
    
    for i, section in enumerate(sections_to_process[:2]):  # Limiter √† 2 sections pour simplifier
        section_name = section.get("name", f"Section {i+1}")
        section_type = section.get("type", "OTHER")
        
        # Extraire le texte de cette section
        start_idx = max(0, section.get("start_index", 0))
        end_idx = min(len(processed_text), section.get("end_index", len(processed_text)))
        
        # V√©rifier que les indices sont valides
        if start_idx >= end_idx or start_idx >= len(processed_text):
            continue
            
        section_content = processed_text[start_idx:end_idx]
        
        simplified_sections.append({
            "name": section_name,
            "type": section_type,
            "content": section_content[:500]  # Limiter le contenu pour simplifier
        })
    
    # Si nous n'avons pas de sections valides, cr√©er une "section compl√®te"
    if not simplified_sections:
        simplified_sections.append({
            "name": "Menu complet",
            "type": "OTHER",
            "content": processed_text[:1000]  # Limiter √† 1000 caract√®res
        })
    
    # Construire le prompt simplifi√©
    sections_details_str = "\n".join([f"--- {s['name']} ({s['type']}) ---\n{s['content']}\n" for s in simplified_sections])
    simplified_prompt = f"""Analyse ces sections du menu du restaurant "{restaurant_name}" et extrais les plats.

{sections_details_str}

R√©ponds en JSON:
{{
  "menu": [
    {{
      "section_name": "Nom de la section",
      "section_type": "TYPE_DE_SECTION",
      "items": [
        {{
          "name": "Nom du plat",
          "description": "Description ou null",
          "price": "Prix ou null",
          "options": ["Option 1"] ou null
        }}
      ]
    }}
  ]
}}"""
    
    logger.info(f"Second essai d'extraction des plats pour {restaurant_name} (chunk {chunk_num}/{total_chunks})")
    
    result_text = generate_ai_response_gpt(
        prompt=simplified_prompt,
        max_tokens=1200,
        temperature=0.1,
    )
    
    # Essayer d'extraire le JSON √† nouveau
    standard_result = extract_json_from_text(result_text)
    
    # V√©rifier si le r√©sultat simplifi√© est valide
    if is_valid_menu_result(standard_result):
        # Sauvegarder en cache
        save_to_cache(cache_key, standard_result, prefix="gpt_dishes")
        logger.info(f"Plats extraits (essai simplifi√©) pour {restaurant_name}")
        return standard_result
    
    # Fallback: structurer manuellement un r√©sultat vide
    logger.warning(f"L'extraction des plats a √©galement √©chou√© avec un prompt simplifi√© pour {restaurant_name}")
    
    # Cr√©er un fallback avec une structure de menu de base
    fallback_result = {
        "menu": []
    }
    
    # Ajouter au moins une section vide pour chaque section identifi√©e
    for section in sections_to_process:
        fallback_result["menu"].append({
            "section_name": section.get("name", "Section sans nom"),
            "section_type": section.get("type", "OTHER"),
            "items": []
        })
    
    # S'il n'y a aucune section, ajouter une section g√©n√©rique
    if not fallback_result["menu"]:
        fallback_result["menu"].append({
            "section_name": "Menu complet",
            "section_type": "OTHER",
            "items": []
        })
    
    # Sauvegarder ce fallback en cache pour √©viter de r√©essayer
    save_to_cache(cache_key, fallback_result, prefix="gpt_dishes")
    
    logger.warning(f"Utilisation d'un fallback pour les plats de {restaurant_name}")
    return fallback_result

def convert_to_standard_format(dishes_result):
    """
    Convertit le r√©sultat des plats extraits en format standardis√© pour le traitement du menu.
    """
    # Les lignes suivantes semblent orphelines et doivent √™tre supprim√©es ou comment√©es
    # max_tokens=1000,
    # temperature=0.0,
    # do_sample=False
    #)
    
    # V√©rifier et nettoyer la r√©ponse
    if result_text and not contains_cyrillic(result_text):
        result = extract_json_from_text(result_text)
        if is_valid_menu_result(result):
            # Sauvegarder dans le cache
            save_to_cache(cache_key, result, prefix="mistral_direct")
            return result
    
    # Structure vide en dernier recours
    return {"Menus Globaux": [], "Plats Ind√©pendants": []}

def extract_menu_direct_minimal(text, restaurant_name):
    """
    Extraction minimale pour les cas tr√®s difficiles.
    Utilise un prompt tr√®s simple et des param√®tres conservatifs.
    """
    if not text:
        return {"Menus Globaux": [], "Plats Ind√©pendants": []}
    
    # Limiter la taille du texte
    short_text = text[:1500]
    
    # Prompt minimaliste pour extraction basique
    prompt = (
        f"[INST] FRAN√áAIS UNIQUEMENT. JSON UNIQUEMENT.\n"
        f"Extrait les plats et prix de ce menu du restaurant \"{restaurant_name}\":\n"
        f"\n{short_text}\n"
        f"\nFormat: {{ \"Plats\": [ {{ \"nom\": \"Nom\", \"prix\": \"Prix\" }} ] }}\n"
        f"\nR√àGLES: Que le JSON! Pas de texte avant/apr√®s! Pas de commentaires! [/INST]"
    )
    
    logger.info(f"Tentative d'extraction minimale pour {restaurant_name}")
    
    result_text = generate_ai_response_gpt(
        prompt=prompt,
        max_tokens=600,
        temperature=0.0,
        do_sample=False
    )
    
    # V√©rifier et corriger le r√©sultat
    if result_text and not contains_cyrillic(result_text):
        result = extract_json_from_text(result_text)
        
        if result and isinstance(result, dict) and "Plats" in result and isinstance(result["Plats"], list):
            # Convertir au format standard
            standard_result = {
                "Menus Globaux": [],
                "Plats Ind√©pendants": []
            }
            
            for plat in result["Plats"]:
                if isinstance(plat, dict) and "nom" in plat:
                    standard_result["Plats Ind√©pendants"].append({
                        "nom": plat.get("nom", ""),
                        "cat√©gorie": "Plat",
                        "prix": plat.get("prix", ""),
                        "description": ""
                    })
            
            return standard_result
    
    # Structure vide en dernier recours
    return {"Menus Globaux": [], "Plats Ind√©pendants": []}

def merge_chunk_results_enhanced(chunk_results):
    """
    Fusion intelligente des r√©sultats de chunks avec gestion am√©lior√©e des doublons.
    D√©duplique et enrichit les informations avec les d√©tails les plus complets.
    """
    if not chunk_results:
        return {"Menus Globaux": [], "Plats Ind√©pendants": []}
    
    # Si un seul chunk, le retourner directement
    if len(chunk_results) == 1:
        return chunk_results[0]
    
    merged_result = {"Menus Globaux": [], "Plats Ind√©pendants": []}
    
    # Tables de suivi pour la d√©duplication intelligente
    seen_menus = {}  # nom+prix -> {menu complet}
    seen_dishes = {}  # nom+cat√©gorie+prix -> {plat complet}
    
    # Fonction de normalisation pour comparaison
    def normalize_for_comparison(text):
        if not text:
            return ""
        # Convertir en minuscules, supprimer accents et ponctuation
        text = text.lower()
        text = re.sub(r'[√†√°√¢√£√§√•]', 'a', text)
        text = re.sub(r'[√®√©√™√´]', 'e', text)
        text = re.sub(r'[√¨√≠√Æ√Ø]', 'i', text)
        text = re.sub(r'[√≤√≥√¥√µ√∂]', 'o', text)
        text = re.sub(r'[√π√∫√ª√º]', 'u', text)
        text = re.sub(r'[√ß]', 'c', text)
        text = re.sub(r'[^\w\s]', '', text)
        return re.sub(r'\s+', ' ', text).strip()
    
    # Fonction pour fusionner deux objets en gardant les informations les plus compl√®tes
    def merge_objects(obj1, obj2):
        result = obj1.copy()
        for key, value in obj2.items():
            # Si la valeur est vide dans obj1 ou celle de obj2 est plus d√©taill√©e
            if key not in obj1 or not obj1[key] or (isinstance(value, str) and len(value) > len(obj1[key])):
                result[key] = value
            # Pour les listes (comme "inclus"), les fusionner
            elif isinstance(value, list) and isinstance(obj1[key], list):
                # Fusion intelligente des √©l√©ments des listes (avec d√©duplication)
                if key == "inclus" and all(isinstance(item, dict) for item in obj1[key] + value):
                    # Pour les plats inclus, d√©duplication bas√©e sur le nom
                    seen_included = {}
                    for item in obj1[key] + value:
                        if "nom" in item:
                            item_key = normalize_for_comparison(item["nom"])
                            if item_key not in seen_included or len(item.get("description", "")) > len(seen_included[item_key].get("description", "")):
                                seen_included[item_key] = item
                    result[key] = list(seen_included.values())
                else:
                    # Simple concat√©nation pour les autres types de listes
                    result[key] = obj1[key] + value
        return result
    
    # Parcourir tous les chunks et fusionner les r√©sultats
    for chunk_result in chunk_results:
        if not isinstance(chunk_result, dict):
            continue
        
        # Traiter les menus globaux
        for menu in chunk_result.get("Menus Globaux", []):
            if not isinstance(menu, dict) or "nom" not in menu:
                continue
            
            # Cr√©er une cl√© unique pour ce menu
            menu_key = (normalize_for_comparison(menu.get("nom", "")), 
                       normalize_for_comparison(menu.get("prix", "")))
            
            if menu_key in seen_menus:
                # Fusionner avec le menu existant
                seen_menus[menu_key] = merge_objects(seen_menus[menu_key], menu)
            else:
                seen_menus[menu_key] = menu
        
        # Traiter les plats ind√©pendants
        for dish in chunk_result.get("Plats Ind√©pendants", []):
            if not isinstance(dish, dict) or "nom" not in dish:
                continue
            
            # Cr√©er une cl√© unique pour ce plat
            dish_key = (normalize_for_comparison(dish.get("nom", "")),
                       normalize_for_comparison(dish.get("cat√©gorie", "")),
                       normalize_for_comparison(dish.get("prix", "")))
            
            if dish_key in seen_dishes:
                # Fusionner avec le plat existant
                seen_dishes[dish_key] = merge_objects(seen_dishes[dish_key], dish)
            else:
                seen_dishes[dish_key] = dish
    
    # Reconstruire le r√©sultat final √† partir des tables de d√©duplication
    merged_result["Menus Globaux"] = list(seen_menus.values())
    merged_result["Plats Ind√©pendants"] = list(seen_dishes.values())
    
    return merged_result

def merge_document_results(main_result, new_result):
    """
    Fusionne les r√©sultats de deux documents diff√©rents.
    Utilis√© pour fusionner les r√©sultats de plusieurs pages ou sources.
    """
    # Si le r√©sultat principal est vide, utiliser le nouveau
    if not main_result or not isinstance(main_result, dict):
        return new_result
    
    # Si le nouveau r√©sultat est vide, conserver le principal
    if not new_result or not isinstance(new_result, dict):
        return main_result
    
    # Utiliser la fusion intelligente d√©j√† impl√©ment√©e
    return merge_chunk_results_enhanced([main_result, new_result])

def post_process_menu_result(menu_result):
    """
    Post-traitement pour am√©liorer la qualit√© des r√©sultats.
    Nettoie, normalise et enrichit les structures de menu.
    """
    if not menu_result or not isinstance(menu_result, dict):
        return {"Menus Globaux": [], "Plats Ind√©pendants": []}
    
    # V√©rifier et corriger la structure de base
    if "Menus Globaux" not in menu_result:
        menu_result["Menus Globaux"] = []
    if "Plats Ind√©pendants" not in menu_result:
        menu_result["Plats Ind√©pendants"] = []
    
    # Nettoyage et normalisation des menus globaux
    for i, menu in enumerate(menu_result["Menus Globaux"]):
        if not isinstance(menu, dict):
            menu_result["Menus Globaux"][i] = {"nom": str(menu), "prix": "", "inclus": []}
            continue
        
        # Nettoyer les champs
        if "nom" in menu:
            menu["nom"] = menu["nom"].strip()
        else:
            menu["nom"] = "Menu non sp√©cifi√©"
            
        if "prix" in menu:
            # Standardiser le format des prix
            prix = menu["prix"]
            prix = re.sub(r'(\d+)[.,](\d+)\s*‚Ç¨', r'\1,\2 ‚Ç¨', prix)
            prix = re.sub(r'(\d+)[.,](\d+)\s*euros?', r'\1,\2 ‚Ç¨', prix)
            menu["prix"] = prix.strip()
        else:
            menu["prix"] = ""
            
        # Nettoyer les plats inclus
        if "inclus" not in menu or not isinstance(menu["inclus"], list):
            menu["inclus"] = []
        else:
            for j, plat in enumerate(menu["inclus"]):
                if not isinstance(plat, dict):
                    menu["inclus"][j] = {"nom": str(plat), "description": ""}
                    continue
                
                if "nom" in plat:
                    plat["nom"] = plat["nom"].strip()
                else:
                    plat["nom"] = "Plat non sp√©cifi√©"
                    
                if "description" in plat:
                    plat["description"] = plat["description"].strip()
                else:
                    plat["description"] = ""
    
    # Nettoyage et normalisation des plats ind√©pendants
    for i, plat in enumerate(menu_result["Plats Ind√©pendants"]):
        if not isinstance(plat, dict):
            menu_result["Plats Ind√©pendants"][i] = {
                "nom": str(plat), 
                "cat√©gorie": "Autre", 
                "prix": "", 
                "description": ""
            }
            continue
        
        # Nettoyer les champs
        if "nom" in plat:
            plat["nom"] = plat["nom"].strip()
        else:
            plat["nom"] = "Plat non sp√©cifi√©"
            
        if "cat√©gorie" in plat:
            plat["cat√©gorie"] = plat["cat√©gorie"].strip()
        else:
            plat["cat√©gorie"] = "Autre"
            
        if "prix" in plat:
            # Standardiser le format des prix
            prix = plat["prix"]
            prix = re.sub(r'(\d+)[.,](\d+)\s*‚Ç¨', r'\1,\2 ‚Ç¨', prix)
            prix = re.sub(r'(\d+)[.,](\d+)\s*euros?', r'\1,\2 ‚Ç¨', prix)
            plat["prix"] = prix.strip()
        else:
            plat["prix"] = ""
            
        if "description" in plat:
            plat["description"] = plat["description"].strip()
        else:
            plat["description"] = ""
    
    # Supprimer les plats avec des noms trop courts ou non significatifs
    menu_result["Plats Ind√©pendants"] = [
        plat for plat in menu_result["Plats Ind√©pendants"] 
        if len(plat.get("nom", "")) > 2 and 
           not plat.get("nom", "").lower() in ["le", "la", "les", "des", "un", "une"]
    ]
    
    # Normalisation des cat√©gories de plats
    categorie_mapping = {
        # Entr√©es
        "entree": "Entr√©es", "entr√©e": "Entr√©es", "entr√©es": "Entr√©es", "entrees": "Entr√©es",
        "starter": "Entr√©es", "starters": "Entr√©es", "appetizer": "Entr√©es", "appetizers": "Entr√©es",
        # Plats
        "plat": "Plats", "plats": "Plats", "main": "Plats", "dish": "Plats",
        "main course": "Plats", "main courses": "Plats", "principal": "Plats",
        # Desserts
        "dessert": "Desserts", "desserts": "Desserts", "sucr√©": "Desserts", "sucre": "Desserts",
        "sweet": "Desserts", "sweets": "Desserts", "p√¢tisserie": "Desserts",
        # Boissons
        "boisson": "Boissons", "boissons": "Boissons", "drink": "Boissons", "drinks": "Boissons",
        "beverage": "Boissons", "beverages": "Boissons", "soft": "Boissons",
        # Vins
        "vin": "Vins", "vins": "Vins", "wine": "Vins", "wines": "Vins",
        # Autres cat√©gories
        "fromage": "Fromages", "fromages": "Fromages", "cheese": "Fromages",
        "accompagnement": "Accompagnements", "side": "Accompagnements",
        "enfant": "Menu Enfant", "kids": "Menu Enfant", "children": "Menu Enfant"
    }
    
    for plat in menu_result["Plats Ind√©pendants"]:
        if "cat√©gorie" in plat:
            # Chercher des correspondances dans le mapping
            categorie_lower = plat["cat√©gorie"].lower()
            for key, value in categorie_mapping.items():
                if key == categorie_lower or key in categorie_lower:
                    plat["cat√©gorie"] = value
                    break
    
    return menu_result

def batch_structure_menus_with_gpt(raw_texts, restaurant_name, default_rating):
    """
    Analyse en lot plusieurs textes de menu avec GPT-3.5-turbo pour cr√©er une structure coh√©rente.
    """
    # Pr√©traiter tous les textes bruts
    processed_texts = [preprocess_text_for_llm(text) for text in raw_texts if text and len(text.strip()) > 50]
    if not processed_texts:
        logger.warning(f"Aucun texte valide pour {restaurant_name} apr√®s pr√©traitement")
        return {"Menus Globaux": [], "Plats Ind√©pendants": []}
    
    all_menus = {"Menus Globaux": [], "Plats Ind√©pendants": []}
    for i, processed_text in enumerate(processed_texts):
        cache_key = f"gpt_menu_structure_{restaurant_name}_{i}_{hashlib.md5(processed_text[:100].encode()).hexdigest()}"
        cached_result = get_from_cache(cache_key, max_age_hours=720, prefix="gpt_menus")
        if cached_result:
            logger.info(f"Utilisation du cache pour le menu de {restaurant_name} (batch {i+1})")
            result = cached_result
        else:
            result = generate_menu_structure_with_openai(processed_text, restaurant_name)
            if result:
                save_to_cache(cache_key, result, prefix="gpt_menus")
        if result and isinstance(result, dict):
            all_menus["Menus Globaux"].extend(result.get("Menus Globaux", []))
            all_menus["Plats Ind√©pendants"].extend(result.get("Plats Ind√©pendants", []))
    return all_menus

def categorize_item(name, description):
    """
    Cat√©gorise un plat en fonction de son nom et de sa description.
    Version am√©lior√©e avec meilleure d√©tection des cat√©gories.
    
    Args:
        name (str): Nom du plat
        description (str): Description du plat
        
    Returns:
        str: Cat√©gorie du plat (Entr√©e, Plat, Dessert, Boisson, etc.)
    """
    name_lower = name.lower() if name else ""
    desc_lower = description.lower() if description else ""
    combined = name_lower + " " + desc_lower
    
    # Mots-cl√©s pour les cat√©gories - √©largi pour une meilleure d√©tection
    entree_keywords = [
        "entr√©e", "starter", "appetizer", "soupe", "salade", "tartare", "carpaccio", "assiette", 
        "foie gras", "velout√©", "gaspacho", "terrine", "amuse-bouche", "ceviche", "gravlax",
        "hors d'oeuvre", "antipasti", "bruschetta", "charcuterie"
    ]
    
    plat_keywords = [
        "plat", "main", "principal", "burger", "steak", "viande", "poisson", "p√¢tes", "pizza", "risotto",
        "poulet", "boeuf", "volaille", "canard", "agneau", "veau", "porc", "filet", "c√¥te", "entrec√¥te",
        "saumon", "thon", "cabillaud", "lotte", "r√¥ti", "grill√©", "brais√©", "mijot√©", "curry",
        "ravioli", "lasagne", "tagliatelle", "gnocchi", "paella", "couscous", "tajine"
    ]
    
    dessert_keywords = [
        "dessert", "sucr√©", "g√¢teau", "cake", "tarte", "fondant", "glace", "sorbet", "mousse", "cr√®me",
        "chocolat", "tiramisu", "profiterole", "√©clair", "mille-feuille", "panna cotta", "bavarois",
        "cheesecake", "macaron", "p√¢tisserie", "crumble", "brownie", "cookie", "financier", "biscuit",
        "cr√™pe sucr√©e", "brioche", "pudding", "yaourt", "fruit", "fraise", "vanille", "caramel"
    ]
    
    boisson_keywords = [
        "boisson", "drink", "vin", "wine", "bi√®re", "beer", "soda", "eau", "jus", "caf√©", "th√©", 
        "cocktail", "cl", "bouteille", "verre", "carafe", "soft", "spiritueux", "digestif", 
        "champagne", "prosecco", "mocktail", "smoothie", "milkshake", "limonade", "infusion",
        "cappuccino", "espresso", "americano", "latte", "chocolat chaud"
    ]
    
    fromage_keywords = [
        "fromage", "cheese", "camembert", "brie", "comt√©", "roquefort", "ch√®vre", "goat cheese",
        "emmental", "gruy√®re", "parmesan", "bleu", "raclette", "reblochon", "tomme"
    ]
    
    # V√©rifier les fromages
    if any(kw in combined for kw in fromage_keywords):
        return "Fromages"
    
    # V√©rifier les boissons (priorit√© car souvent identifiable par la taille)
    if any(kw in combined for kw in boisson_keywords) or re.search(r"\d+\s*cl", combined):
        return "Boisson"
    
    # V√©rifier les desserts
    if any(kw in combined for kw in dessert_keywords):
        return "Dessert"
    
    # V√©rifier les entr√©es
    if any(kw in combined for kw in entree_keywords):
        return "Entr√©e"
    
    # V√©rifier les plats principaux
    if any(kw in combined for kw in plat_keywords):
        return "Plat"
    
    # Analyse contextuelle avec expressions r√©guli√®res
    if re.search(r"entr[√©e]e\s+de", combined) or re.search(r"pour commencer", combined, re.IGNORECASE):
        return "Entr√©e"
    
    if re.search(r"plat\s+principal", combined) or re.search(r"nos plats", combined, re.IGNORECASE):
        return "Plat"
    
    if re.search(r"pour terminer", combined, re.IGNORECASE) or re.search(r"douceur", combined):
        return "Dessert"
    
    # Identifier √† partir du prix - les entr√©es sont g√©n√©ralement moins ch√®res
    price_match = re.search(r"(\d+)[,.](\d+)", combined)
    if price_match:
        price = float(price_match.group(1) + "." + price_match.group(2))
        if price < 10:
            return "Entr√©e"
        elif price > 20:
            return "Plat"
    
    # Par d√©faut, consid√©rer comme un plat principal
    return "Plat"

def validate_and_enrich_items(items, default_rating):
    """
    Valide et enrichit les plats avec des valeurs par d√©faut.
    Version am√©lior√©e avec meilleure gestion des erreurs et enrichissement des informations.
    """
    validated_items = []
    
    # Protection contre None pour default_rating
    if default_rating is None or not isinstance(default_rating, (int, float)):
        default_rating = 7.0  # Valeur par d√©faut s√©curis√©e (3.5 * 2)
    
    # V√©rifier que items est bien une liste
    if not isinstance(items, list):
        logger.warning("Liste de plats invalide ou vide, retour d'une liste vide")
        return []
    
    for item in items:
        if not isinstance(item, dict):
            continue
            
        try:
            # Normalisation des champs avec protection contre None
            validated_item = {
                "nom": str(item.get("nom", "Nom non sp√©cifi√©")).strip(),
                "description": str(item.get("description", "")).strip(),
                "prix": str(item.get("prix", "Non sp√©cifi√©")).strip(),
                "note": str(item.get("note", f"{default_rating}/10")).strip(),
                "cat√©gorie": str(item.get("cat√©gorie", "Non sp√©cifi√©")).strip()
            }
            
            # Enrichissement: cat√©gorisation si manquante ou g√©n√©rique
            if validated_item["cat√©gorie"] in ["Non sp√©cifi√©", "", "non cat√©goris√©", "Autre"]:
                validated_item["cat√©gorie"] = categorize_item(validated_item["nom"], validated_item["description"])
            
            # Extraction et standardisation du prix
            if validated_item["prix"] != "Non sp√©cifi√©":
                # V√©rifier s'il y a un prix
                price_match = re.search(r"(\d+[,.]?\d*)\s*(?:‚Ç¨|EUR|euro|euros)?", validated_item["prix"])
                if price_match:
                    price = price_match.group(1)
                    # Normaliser le format (virgule pour les d√©cimales, toujours avec symbole ‚Ç¨)
                    price = price.replace('.', ',')
                    if not ',' in price:
                        price += ",00"
                    if not "‚Ç¨" in validated_item["prix"]:
                        price += " ‚Ç¨"
                    validated_item["prix"] = price
            
            # Extractions de m√©tadonn√©es utiles (taille, origine, allerg√®nes)
            # Taille pour les boissons
            taille_match = re.search(r"(\d{1,4}\s*(?:cl|ml|L|litres?|g|kg))", validated_item["description"], re.IGNORECASE)
            if taille_match:
                validated_item["taille"] = taille_match.group(1)
            
            # Origine/provenance pour viandes et fromages
            origin_match = re.search(r"(?:de|du|d[e'])\s+([A-Z][a-z√©]+(?:[- ][A-Z][a-z√©]+)?)", validated_item["description"])
            if origin_match and validated_item["cat√©gorie"] in ["Plat", "Fromages"]:
                validated_item["origine"] = origin_match.group(1)
                
            # Allerg√®nes si mentionn√©s
            if re.search(r"(?:allerg[√®√©]ne|gluten|lactose|fruit.*coque|arachide)", validated_item["description"], re.IGNORECASE):
                allergen_info = []
                for allergen in ["gluten", "lactose", "fruit.*coque", "arachide", "oeuf", "soja", "poisson", "crustac[√©e]"]:
                    if re.search(allergen, validated_item["description"], re.IGNORECASE):
                        match = re.search(f"({allergen}[^,.;]*)", validated_item["description"], re.IGNORECASE)
                        if match:
                            allergen_info.append(match.group(1).strip())
                if allergen_info:
                    validated_item["allerg√®nes"] = allergen_info
            
            validated_items.append(validated_item)
        except Exception as e:
            logger.error(f"Erreur lors de la validation d'un plat: {e}")
            # Continuer avec le plat suivant
    
    return validated_items

def deduplicate_items(items):
    """
    Supprime les doublons dans une liste d'items.
    Version am√©lior√©e avec meilleure d√©tection de similarit√©.
    """
    if not items:
        return []
    
    seen = set()
    unique_items = []
    
    # Fonction de normalisation pour comparaison
    def normalize_for_comparison(text):
        if not text:
            return ""
        # Convertir en minuscules, supprimer accents et ponctuation
        text = text.lower()
        text = re.sub(r'[√†√°√¢√£√§√•]', 'a', text)
        text = re.sub(r'[√®√©√™√´]', 'e', text)
        text = re.sub(r'[√¨√≠√Æ√Ø]', 'i', text)
        text = re.sub(r'[√≤√≥√¥√µ√∂]', 'o', text)
        text = re.sub(r'[√π√∫√ª√º]', 'u', text)
        text = re.sub(r'[√ß]', 'c', text)
        text = re.sub(r'[^\w\s]', '', text)
        return re.sub(r'\s+', ' ', text).strip()
    
    # Premi√®re passe: d√©duplication exacte et construction d'un index de similarit√©
    similar_items = {}  # nom normalis√© -> [indices d'items similaires]
    
    for item in items:
        if not isinstance(item, dict):
            continue
            
        # Cr√©er un identifiant unique (nom + prix)
        identifier = (
            str(item.get("nom", "")).strip().lower(),
            str(item.get("prix", "")).strip().lower()
        )
        
        # Nom normalis√© pour recherche de similarit√©
        norm_name = normalize_for_comparison(item.get("nom", ""))
        
        if identifier not in seen:
            seen.add(identifier)
            unique_items.append(item)
            
            # Ajouter √† l'index de similarit√©
            if norm_name not in similar_items:
                similar_items[norm_name] = [len(unique_items) - 1]
            else:
                similar_items[norm_name].append(len(unique_items) - 1)
    
    # Deuxi√®me passe: fusion des items tr√®s similaires
    final_items = []
    processed_indices = set()
    
    for i, item in enumerate(unique_items):
        if i in processed_indices:
            continue
            
        norm_name = normalize_for_comparison(item.get("nom", ""))
        
        # Chercher des items similaires
        if norm_name in similar_items and len(similar_items[norm_name]) > 1:
            # Fusionner les items similaires
            merged_item = item.copy()
            similar_indices = similar_items[norm_name]
            
            for idx in similar_indices:
                if idx != i:  # √âviter de se fusionner avec soi-m√™me
                    similar_item = unique_items[idx]
                    
                    # Conserver les meilleures informations
                    for key in ["description", "cat√©gorie"]:
                        if key in similar_item and (key not in merged_item or len(similar_item[key]) > len(merged_item[key])):
                            merged_item[key] = similar_item[key]
                    
                    processed_indices.add(idx)
            
            final_items.append(merged_item)
            processed_indices.add(i)
        else:
            # Pas de similaire, ajouter tel quel
            final_items.append(item)
            processed_indices.add(i)
    
    return final_items

def process_restaurant_menus(limit=3):
    """
    Pipeline principal pour extraire et structurer les menus.
    Version am√©lior√©e avec validation URL et gestion des erreurs renforc√©e.
    """
    restaurants = fetch_restaurant_websites(limit=limit, processed_only=False)
    
    # R√©cup√©rer l'√©tat de progression du checkpoint
    checkpoint = load_checkpoint(checkpoint_name="menu_extraction_enhanced")
    processed_ids = set()
    
    if checkpoint:
        processed_ids = set(checkpoint.get("processed_ids", []))
        logger.info(f"Reprise √† partir d'un checkpoint avec {len(processed_ids)} restaurants d√©j√† trait√©s")
    
    for restaurant in restaurants:
        restaurant_id = str(restaurant["_id"])
        
        # Ignorer les restaurants d√©j√† trait√©s
        if restaurant_id in processed_ids:
            logger.info(f"Restaurant {restaurant['name']} d√©j√† trait√©. Ignor√©.")
            continue
        
        name = restaurant["name"]
        website = restaurant["website"]
        
        # Protection renforc√©e contre les None values dans rating
        raw_rating = restaurant.get("rating")
        rating = (float(raw_rating) if raw_rating is not None else 3.5) * 2  # Conversion en notation sur 10
        
        logger.info(f"\n=== Restaurant : {name} ===")
        
        # V√âRIFICATION CRITIQUE: ne jamais retraiter un restaurant d√©j√† dans la base
        if collection.find_one({"_id": ObjectId(restaurant_id), "menus_structures": {"$exists": True}}):
            logger.info(f"Restaurant {name} d√©j√† trait√© dans la base de donn√©es. Ignor√©.")
            processed_ids.add(restaurant_id)
            continue
        
        # V√©rifier que le website est valide
        if not is_valid_url(website):
            logger.warning(f"Site web invalide ({website}) pour {name}. Impossible d'extraire les menus.")
            processed_ids.add(restaurant_id)
            continue
            
        # Extraire les liens du site
        links = extract_links_from_website(website)
        # Log de d√©bogage pour voir tous les liens extraits AVANT filtrage
        logger.debug(f"Liens extraits pour {name} AVANT filtrage: {links}") 
        menu_links = filter_menu_links(links, website)
        
        if not menu_links:
            logger.warning(f"Aucun lien de menu trouv√© pour {name}. Tentative d'extraction depuis HTML principal.")
            # Fallback: Extraire le texte directement depuis la page HTML principale
            html_text = extract_text_from_html(website)
            if html_text and len(html_text.strip()) > 100: # Minimum de contenu
                logger.info(f"Texte extrait depuis HTML principal pour {name}. Ajout aux sources.")
                # On ne peut pas √™tre s√ªr que c'est un menu, on le traite comme un texte brut
                raw_texts.append(html_text) 
            else:
                logger.warning(f"√âchec de l'extraction depuis HTML principal ou contenu trop court pour {name}.")
                processed_ids.add(restaurant_id) # Marquer comme trait√© car aucune source trouv√©e
                continue # Passer au restaurant suivant
        else:
             # Extraire le texte des menus trouv√©s via les liens
            for link in menu_links:
                logger.info(f"Extraction du menu depuis: {link['href']}")
                text = extract_text_from_link(link["href"])
                if text and len(text.strip()) > 50:  # Ignorer les textes trop courts
                    raw_texts.append(text)
            
        if not raw_texts:
            logger.warning(f"Aucun texte de menu exploitable extrait pour {name} (m√™me apr√®s fallback HTML si tent√©).")
            processed_ids.add(restaurant_id)
            continue
        
        # Analyser les menus avec Mistral (approche multi-phase)
        logger.info(f"Analyse des menus pour {name}...")
        structured_menus = batch_structure_menus_with_gpt(raw_texts, name, rating)
        
        # Validation, enrichissement et d√©duplication
        structured_menus["Plats Ind√©pendants"] = deduplicate_items(
            validate_and_enrich_items(structured_menus["Plats Ind√©pendants"], rating)
        )
        
        # Sauvegarder dans MongoDB
        if structured_menus["Menus Globaux"] or structured_menus["Plats Ind√©pendants"]:
            collection.update_one(
                {"_id": ObjectId(restaurant_id)},
                {"$set": {"menus_structures": structured_menus}}
            )
            logger.info(f"Menus sauvegard√©s pour {name}: {len(structured_menus['Menus Globaux'])} menus, {len(structured_menus['Plats Ind√©pendants'])} plats")
        else:
            logger.warning(f"Aucun menu structur√© obtenu pour {name}")
        
        # Marquer comme trait√© et sauvegarder le checkpoint
        processed_ids.add(restaurant_id)
        save_checkpoint({
            "processed_ids": list(processed_ids),
            "timestamp": time.time()
        }, checkpoint_name="menu_extraction_enhanced")

# --- CONFIGURATION GLOBALE ---
AI_ENABLED = True # Flag pour activer/d√©sactiver les fonctionnalit√©s IA (OCR, etc.)
ENABLE_OPENAI_FALLBACK = True # Flag pour activer/d√©sactiver le fallback OpenAI

# ---- Configuration des r√©pertoires ----
TMP_DIR = "tmp_files"
TMP_PDF_DIR = os.path.join(TMP_DIR, "pdf")
# ... (reste de la configuration) ...

# Lancer le processus
if __name__ == "__main__":
    import argparse
    
    # D√©finition du parser d'arguments
    parser = argparse.ArgumentParser(description='Extracteur et analyseur de menus de restaurants avec Mistral - Version optimis√©e multi-phase')
    parser.add_argument('--limit', type=int, default=100, help='Nombre de restaurants √† traiter (d√©faut: 100)')
    parser.add_argument('--no-resume', action='store_true', help='Ne pas utiliser les checkpoints, d√©marrer depuis le d√©but')
    parser.add_argument('--skip-ai', action='store_true', help="D√©sactiver l'utilisation de l'IA")
    parser.add_argument('--no-openai', action='store_true', help="D√©sactiver le fallback vers OpenAI")
    parser.add_argument('--chunk-size', type=int, default=800, help='Taille maximale des chunks (d√©faut: 800 caract√®res)')
    
    # R√©cup√©rer les arguments de ligne de commande
    args = parser.parse_args()
    
    # Configurer l'activation/d√©sactivation de l'IA
    if args.skip_ai:
        AI_ENABLED = False # Correction: assigner √† la variable globale
        logger.info("Fonctionnalit√© IA d√©sactiv√©e par option --skip-ai")
    else:
        AI_ENABLED = True # Correction: assigner √† la variable globale
        logger.info("Fonctionnalit√© IA activ√©e (utilisant le mod√®le Mistral)")
        
    # Configurer le fallback OpenAI
    if args.no_openai:
        ENABLE_OPENAI_FALLBACK = False
        logger.info("Fallback vers OpenAI d√©sactiv√© par option --no-openai")
    
    # Configurer la taille des chunks
    if args.chunk_size != 800:
        logger.info(f"Taille des chunks d√©finie √† {args.chunk_size} caract√®res")
        # Cette variable sera utilis√©e comme valeur par d√©faut dans la fonction chunk_text
        
    # V√©rifier si tous les modules requis sont install√©s
    try:
        import PIL
        import fitz
        logger.info("Tous les modules n√©cessaires sont install√©s.")
        
        # V√©rification OCR optionnelle
        try:
            import pytesseract
            logger.info("Module OCR pytesseract disponible.")
        except ImportError:
            logger.warning("Module OCR pytesseract non disponible. L'OCR local ne sera pas utilis√©.")
            logger.info("Installer avec: pip install pytesseract")
            logger.info("Et installez Tesseract OCR: https://github.com/UB-Mannheim/tesseract/wiki")
        
        try:
            # Proc√©der au traitement
            process_restaurant_menus(limit=args.limit)
        except Exception as e:
            logger.error(f"Erreur lors du traitement: {e}")
            import traceback
            logger.error(traceback.format_exc())  # Log complet de l'erreur pour debug
    
    except ImportError as e:
        logger.error(f"Module manquant: {e}")
        logger.info("Installez les d√©pendances avec: pip install pymongo requests beautifulsoup4 PyMuPDF python-dotenv pillow pytesseract openai")
