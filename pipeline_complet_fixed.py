"""
Pipeline complet rempla√ßant Google Places API pour la r√©cup√©ration de donn√©es de restaurants
Ce script combine:
1. Google Maps Nearby Search API pour la liste initiale des restaurants
2. Capture d'√©cran Google Maps + OCR
3. Recherche Bing pour trouver les liens vers TheFork/TripAdvisor
4. Scraping des plateformes avec support BrightData pour anti-bot
5. Sauvegarde en MongoDB

Auteur: Kodu
"""

import os
import time
import json
import urllib.parse
import base64
import re
import requests
import tempfile
import argparse
import functools
import uuid
from io import BytesIO
from collections import OrderedDict, defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pymongo import MongoClient
import openai
import random
from requests.auth import HTTPBasicAuth
import glob
import shutil
import atexit
from datetime import datetime, timedelta
import traceback
from urllib.parse import urlparse
from bson.objectid import ObjectId
import sys
import pandas as pd
import hashlib

# Selenium et outils web
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import TimeoutException, WebDriverException

# Traitement d'image
from PIL import Image
import pytesseract

# Parsing HTML
from bs4 import BeautifulSoup

# Configuration des API et param√®tres
# Cl√©s API en dur pour le test
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
BRIGHTDATA_TOKEN = os.getenv("BRIGHTDATA_TOKEN")
BRIGHTDATA_ENABLED = bool(BRIGHTDATA_TOKEN)
MONGODB_URI = "mongodb+srv://remibarbier:Calvi8Pierc2@lieuxrestauration.szq31.mongodb.net/?retryWrites=true&w=majority&appName=lieuxrestauration"
DB_NAME = "Restauration_Officielle"
COLLECTION_NAME = "producers"
GOOGLE_MAPS_API_KEY = os.getenv("GOOGLE_MAPS_API_KEY")  # Cl√© API pour Google Maps
NUM_THREADS = 4  # Nombre de threads par d√©faut pour le traitement parall√®le
MAPS_API_REQUEST_COUNT = 0  # Compteur de requ√™tes Google Maps API
MAX_MAPS_API_REQUESTS = 500  # Limite quotidienne de requ√™tes Google Maps API
MAX_RETRIES = 3
TIMEOUT = 30
DEBUG = False

# Configurer OpenAI API
openai.api_key = OPENAI_API_KEY

# Variables globales pour statistiques de performance
TIMING_STATS = defaultdict(list)
DEBUG_MODE = False  # Mode debug avec logs d√©taill√©s
USE_BRIGHTDATA = False  # Utilisation de BrightData pour contourner les mesures anti-bot
BRIGHTDATA_ENABLED = True  # Si le service BrightData est activ√©

# Cache pour les r√©sultats de recherche
SEARCH_CACHE = {}
HTML_CACHE = {}
BING_SEARCH_CACHE = {}
MAX_CACHE_SIZE = 1000
CACHE_TIMEOUT = 3600  # 1 heure en secondes

# D√©finition des cat√©gories de restaurant pour Google Maps API
RESTAURANT_CATEGORIES = {
    "restaurant", "cafe", "bar", "meal_takeaway", "bakery", "fast_food",
    "sushi_restaurant", "pizza_restaurant", "chinese_restaurant",
    "indian_restaurant", "french_restaurant", "italian_restaurant",
    "thai_restaurant", "mexican_restaurant", "vietnamese_restaurant",
    "seafood_restaurant", "steakhouse", "burger_restaurant",
    "ice_cream_shop", "brewery", "pub"
}

# Ajouter la variable globale de cache pour les recherches Bing
BING_SEARCH_CACHE = {}

# D√©corateur pour mesurer le temps d'ex√©cution des fonctions
def timing_decorator(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        if not DEBUG_MODE:
            return func(*args, **kwargs)
        
        # Mesurer le temps pour cette fonction
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        # Calculer la dur√©e
        duration = end_time - start_time
        
        # R√©cup√©rer le nom du restaurant si disponible dans les arguments
        restaurant_name = None
        for arg in args:
            if isinstance(arg, dict) and arg.get("name"):
                restaurant_name = arg["name"]
                break
        
        # Afficher les informations de timing
        log_prefix = f"[{restaurant_name}] " if restaurant_name else ""
        print(f"‚è±Ô∏è {log_prefix}{func.__name__}: {duration:.2f} secondes")
        
        # Stocker les stats
        TIMING_STATS[func.__name__].append(duration)
        
        return result
    return wrapper

# Configuration du navigateur Chrome pour Selenium
# Ne pas mettre dans une variable globale pour √©viter les probl√®mes avec copy()
@timing_decorator
def get_chrome_options():
    options = webdriver.ChromeOptions()
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--headless=new")
    options.add_argument("--window-size=1280,1800")
    options.add_argument("--lang=fr")  # Passer en fran√ßais pour une meilleure extraction
    return options

# Connexion √† MongoDB
@timing_decorator
def get_mongo_client():
    """
    Obtient une connexion √† MongoDB
    """
    try:
        # Utiliser l'URI MongoDB Atlas
        client = MongoClient(MONGODB_URI)
        # Tester la connexion
        client.server_info()
        return client
    except Exception as e:
        print(f"‚ùå Erreur lors de la connexion √† MongoDB: {str(e)}")
        return None

# =============================================
# √âTAPE 1: R√âCUP√âRATION DES RESTAURANTS VIA GOOGLE MAPS API
# =============================================

def generate_zones(divisions=5):
    """
    Divise Paris en une grille de zones pour les requ√™tes Google Maps API
    
    Args:
        divisions: Nombre de divisions par c√¥t√© pour la grille
    
    Returns:
        Liste de dictionnaires d√©finissant chaque zone
    """
    # Limites g√©ographiques de Paris
    lat_min, lat_max = 48.8156, 48.9022
    lng_min, lng_max = 2.2242, 2.4699
    
    lat_range = lat_max - lat_min
    lng_range = lng_max - lng_min
    
    zones = []
    for i in range(divisions):
        for j in range(divisions):
            zones.append({
                "lat_min": lat_min + (i * lat_range / divisions),
                "lat_max": lat_min + ((i + 1) * lat_range / divisions),
                "lng_min": lng_min + (j * lng_range / divisions),
                "lng_max": lng_min + ((j + 1) * lng_range / divisions)
            })
    
    return zones

@timing_decorator
def get_restaurants_in_zone(zone):
    """
    R√©cup√®re tous les restaurants dans une zone d√©finie en utilisant Google Maps Nearby Search API
    
    Args:
        zone: Dictionnaire d√©finissant les limites de la zone (lat_min, lat_max, lng_min, lng_max)
    
    Returns:
        Liste de restaurants avec leurs informations de base
    """
    global MAPS_API_REQUEST_COUNT
    
    # V√©rifier si on a atteint la limite quotidienne
    if MAPS_API_REQUEST_COUNT >= MAX_MAPS_API_REQUESTS:
        print("‚ö†Ô∏è Limite quotidienne de l'API Google Maps atteinte (500 requ√™tes)")
        return []
    
    lat_min, lat_max = zone["lat_min"], zone["lat_max"]
    lng_min, lng_max = zone["lng_min"], zone["lng_max"]
    
    # √âtape de la grille (en degr√©s) pour un espacement d'environ 500 m
    step = 0.005  # Environ 500 m√®tres
    
    # Points latitudes et longitudes
    lat_points = [lat_min + i * step for i in range(int((lat_max - lat_min) / step) + 1)]
    lng_points = [lng_min + i * step for i in range(int((lng_max - lng_min) / step) + 1)]
    
    all_restaurants = []  # Liste pour stocker tous les r√©sultats
    
    # Parcourir chaque point de la grille
    for lat in lat_points:
        for lng in lng_points:
            # V√©rifier si on a atteint la limite
            if MAPS_API_REQUEST_COUNT >= MAX_MAPS_API_REQUESTS:
                print("‚ö†Ô∏è Limite quotidienne de l'API Google Maps atteinte pendant le traitement")
                break
                
            url = f"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location={lat},{lng}&radius=200&type=restaurant&key={GOOGLE_MAPS_API_KEY}"
            MAPS_API_REQUEST_COUNT += 1
            
            try:
                response = requests.get(url)
                data = response.json()
                
                # V√©rifiez si des r√©sultats sont retourn√©s
                if 'results' in data:
                    # Filtrer les lieux pour inclure uniquement les cat√©gories pertinentes
                    filtered_places = [
                        place for place in data['results']
                        if set(place.get("types", [])).intersection(RESTAURANT_CATEGORIES)
                    ]
                    all_restaurants.extend(filtered_places)
                
                # R√©cup√©rer le token pour la page suivante, s'il existe
                next_page_token = data.get("next_page_token")
                if next_page_token and MAPS_API_REQUEST_COUNT < MAX_MAPS_API_REQUESTS:
                    # Pause pour attendre que la page suivante soit pr√™te
                    time.sleep(2)
                    url = f"https://maps.googleapis.com/maps/api/place/nearbysearch/json?pagetoken={next_page_token}&key={GOOGLE_MAPS_API_KEY}"
                    MAPS_API_REQUEST_COUNT += 1
                    
                    response = requests.get(url)
                    data = response.json()
                    
                    if 'results' in data:
                        filtered_places = [
                            place for place in data['results']
                            if set(place.get("types", [])).intersection(RESTAURANT_CATEGORIES)
                        ]
                        all_restaurants.extend(filtered_places)
            except Exception as e:
                print(f"‚ùå Erreur Google Maps API: {e}")
    
    # Supprimer les doublons en utilisant place_id comme cl√© unique
    unique_restaurants = {}
    for restaurant in all_restaurants:
        place_id = restaurant.get("place_id")
        if place_id and place_id not in unique_restaurants:
            unique_restaurants[place_id] = restaurant
    
    restaurants_list = list(unique_restaurants.values())
    print(f"‚úÖ {len(restaurants_list)} restaurants r√©cup√©r√©s via Google Maps API dans la zone")
    return restaurants_list

def convert_nearby_to_restaurant(place):
    """
    Convertit le r√©sultat de l'API Nearby Search en format restaurant compatible
    
    Args:
        place: Dictionnaire du restaurant issu de l'API Nearby Search
    
    Returns:
        Dictionnaire format√© pour le pipeline
    """
    place_id = place.get("place_id", "")
    maps_url = f"https://www.google.com/maps/place/?q=place_id:{place_id}" if place_id else None
    
    # Extraire les coordonn√©es
    location = place.get("geometry", {}).get("location", {})
    lat = location.get("lat")
    lng = location.get("lng")
    
    # Construire une structure pour chaque restaurant
    restaurant = {
        "name": place.get("name"),
        "place_id": place_id,
        "maps_url": maps_url,
        "address": place.get("vicinity", ""),
        "lat": lat,
        "lon": lng,
        "rating": place.get("rating"),
        "user_ratings_total": place.get("user_ratings_total"),
        "business_status": place.get("business_status"),
        "types": place.get("types", []),
        "open_now": place.get("opening_hours", {}).get("open_now"),
        "photos": place.get("photos", [])
    }
    
    return restaurant

def is_valid_restaurant(restaurant):
    """
    V√©rifie si un restaurant a les donn√©es minimales n√©cessaires
    
    Args:
        restaurant: Dictionnaire du restaurant
    
    Returns:
        Boolean indiquant si le restaurant est valide
    """
    # V√©rifier le nom et place_id
    if not restaurant.get("name") or not restaurant.get("place_id"):
        return False
    
    # V√©rifier les coordonn√©es GPS
    has_coordinates = restaurant.get("lat") is not None and restaurant.get("lon") is not None
    
    return has_coordinates

def get_all_paris_restaurants(max_zones=None):
    """
    R√©cup√®re tous les restaurants de Paris en divisant la ville en grille
    
    Args:
        max_zones: Nombre maximum de zones √† traiter (ou None pour toutes)
    
    Returns:
        Liste compl√®te des restaurants
    """
    all_restaurants = []
    zones = generate_zones(divisions=5)  # Diviser Paris en 25 zones (5x5)
    
    # Limiter le nombre de zones si sp√©cifi√©
    if max_zones and max_zones < len(zones):
        zones = zones[:max_zones]
        print(f"‚ÑπÔ∏è Traitement limit√© √† {max_zones} zones sur {len(zones)} disponibles")
    
    for i, zone in enumerate(zones):
        print(f"üìç Traitement de la zone {i+1}/{len(zones)}")
        zone_restaurants = get_restaurants_in_zone(zone)
        
        # Convertir au format compatible avec le reste du pipeline
        formatted_restaurants = [convert_nearby_to_restaurant(place) for place in zone_restaurants]
        
        # Filtrer les restaurants valides
        valid_restaurants = [r for r in formatted_restaurants if is_valid_restaurant(r)]
        
        all_restaurants.extend(valid_restaurants)
        
        # V√©rifier si on a atteint la limite quotidienne
        if MAPS_API_REQUEST_COUNT >= MAX_MAPS_API_REQUESTS:
            print("‚ö†Ô∏è Limite quotidienne de l'API Google Maps atteinte, arr√™t du traitement de zones")
            break
        
        time.sleep(2)  # Pause pour √©viter de surcharger l'API
    
    print(f"üìä Total: {len(all_restaurants)} restaurants uniques r√©cup√©r√©s via Google Maps API")
    print(f"üìä Requ√™tes Google Maps API utilis√©es: {MAPS_API_REQUEST_COUNT}/{MAX_MAPS_API_REQUESTS}")
    
    return all_restaurants

def is_restaurant_in_mongodb(name, maps_url=None, place_id=None):
    """
    V√©rifie si un restaurant existe d√©j√† dans MongoDB
    """
    try:
        client = get_mongo_client()
        if not client:
            print("‚ùå Impossible de se connecter √† MongoDB")
            return False
            
        # Utiliser la bonne base de donn√©es
        db = client[DB_NAME]
        collection = db[COLLECTION_NAME]
        
        print(f"üîç V√©rification dans MongoDB pour: {name}")
        print(f"  Base de donn√©es: {DB_NAME}")
        print(f"  Collection: {COLLECTION_NAME}")
        
        # Construire la requ√™te
        query = {"name": name}
        
        if place_id:
            query["place_id"] = place_id
        elif maps_url:
            query["maps_url"] = maps_url
        
        print(f"  Requ√™te: {query}")
        
        # V√©rifier si le restaurant existe
        exists = collection.find_one(query)
        
        if exists:
            print(f"  ‚úÖ Restaurant trouv√© dans MongoDB (ID: {exists.get('_id')})")
        else:
            print(f"  ‚ùå Restaurant non trouv√© dans MongoDB")
            
        client.close()
        
        return exists is not None
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification MongoDB: {str(e)}")
        traceback.print_exc()
        return False

def convert_to_12h_format(time_str):
    """Convertit une heure au format 24h en format 12h AM/PM"""
    try:
        # Nettoyer l'entr√©e
        time_str = time_str.strip().replace("h", ":").replace("H", ":")
        if ":" not in time_str:
            time_str = time_str.zfill(4)
            time_str = f"{time_str[:2]}:{time_str[2:]}"
        
        # Parsing de l'heure
        hour = int(time_str.split(":")[0])
        minute = int(time_str.split(":")[1]) if ":" in time_str else 0
        
        # Conversion en format 12h
        if hour == 0:
            return f"12:{minute:02d} AM"
        elif hour < 12:
            return f"{hour}:{minute:02d} AM"
        elif hour == 12:
            return f"12:{minute:02d} PM"
        else:
            return f"{hour-12}:{minute:02d} PM"
    except:
        return time_str

def extract_opening_hours_thefork(soup):
    """
    Extrait les horaires depuis TheFork avec le nouveau format
    """
    horaires = []
    
    # Liste des jours pour mapping
    jours = {
        "monday": "Lundi", "tuesday": "Mardi", "wednesday": "Mercredi",
        "thursday": "Jeudi", "friday": "Vendredi", "saturday": "Samedi", 
        "sunday": "Dimanche"
    }
    
    for tag in soup.select('div[data-testid^="interval-line-"]'):
        # Exemple : "interval-line-wednesday" -> "wednesday"
        testid = tag.get("data-testid")
        jour = testid.replace("interval-line-", "")
        jour_fr = jours.get(jour, jour.capitalize())
        
        heures = [div.get_text(strip=True) for div in tag.find_all("div") 
                 if div.get_text(strip=True) and not any(j in div.get_text(strip=True).lower() 
                 for j in ["aujourd'hui", "today"])]
        
        if heures:
            # Nettoyer et formater les heures
            clean_hours = []
            for h in heures:
                if "-" in h:
                    clean_hours.append(h)
            if clean_hours:
                horaires.append(f"{jour_fr} : {' / '.join(clean_hours)}")
    
    return format_opening_hours(horaires)

def extract_opening_hours_tripadvisor(soup):
    """
    Extrait les horaires depuis TripAdvisor avec le nouveau format
    """
    horaires = []
    days_order = ["Lundi", "Mardi", "Mercredi", "Jeudi", "Vendredi", "Samedi", "Dimanche"]
    
    # Chercher les horaires dans diff√©rents formats possibles
    hours_div = soup.find("div", class_=lambda x: x and "hours" in x.lower())
    if not hours_div:
        return format_opening_hours([])  # Retourner format standard si pas trouv√©
    
    # Extraire le texte et nettoyer
    hours_text = hours_div.get_text(" ", strip=True)
    
    # Parser chaque jour
    current_day = None
    for line in hours_text.split("\n"):
        line = line.strip()
        if not line:
            continue
        
        # D√©tecter le jour
        for day in days_order:
            if line.lower().startswith(day.lower()):
                current_day = day
                # Extraire les heures
                hours_part = line[len(day):].strip(" :")
                if hours_part:
                    horaires.append(f"{current_day} : {hours_part}")
                break
    
    # Trier les jours dans l'ordre
    sorted_hours = []
    for day in days_order:
        matching_hours = [h for h in horaires if h.startswith(day)]
        if matching_hours:
            sorted_hours.append(matching_hours[0])
    
    return format_opening_hours(sorted_hours)

def extract_images_and_menus(soup):
    """
    Extrait les images et menus d'une page
    
    Args:
        soup: BeautifulSoup object de la page
    
    Returns:
        Tuple (liste des URLs d'images, liste des URLs des menus)
    """
    # Extraction des images
    image_urls = []
    for img in soup.find_all("img"):
        url = img.get("src") or img.get("data-src") or img.get("data-lazyurl")
        if url and url not in image_urls:
            image_urls.append(url)
    
    # Limiter √† 5 images
    image_urls = image_urls[:5]
    
    # Extraction des menus
    menu_urls = []
    for link in soup.find_all("a", href=True):
        href = link["href"]
        if any(k in href.lower() for k in ["menu", "carte", "pdf"]):
            menu_urls.append(href)
    
    # D√©dupliquer les URLs
    menu_urls = list(set(menu_urls))
    
    return image_urls, menu_urls

@timing_decorator
def extract_platform_data(url, platform, name=None):
    """
    Extrait les donn√©es d'une plateforme sp√©cifique
    
    Args:
        url: URL de la page
        platform: Nom de la plateforme ('thefork' ou 'tripadvisor')
        name: Nom du restaurant (pour les logs)
    
    Returns:
        Dictionnaire des donn√©es extraites
    """
    log_prefix = f"[{name}] " if name else ""
    
    # Log de debug pour v√©rifier l'URL pass√©e
    print(f"{log_prefix}üîó URL utilis√©e pour {platform}: {url}")
    
    # R√©cup√©rer le HTML via BrightData
    html = fetch_html_with_brightdata(url=url, name=name, platform=platform)
    if not html:
        return {}
    
    # Parser le HTML
    soup = BeautifulSoup(html, 'html.parser')
    
    # Extraire les donn√©es selon la plateforme
    data = {}
    
    # Horaires d'ouverture
    if platform == 'thefork':
        data['opening_hours'] = extract_opening_hours_thefork(soup)
    elif platform == 'tripadvisor':
        data['opening_hours'] = extract_opening_hours_tripadvisor(soup)
    
    # Images et menus (commun aux deux plateformes)
    images, menus = extract_images_and_menus(soup)
    data['images'] = images
    data['menus'] = menus
    
    # Ajouter l'URL source
    data['source_url'] = url
    
    return data

def extract_opening_hours_structured(soup):
    """
    Extrait les horaires depuis TheFork et les convertit au format MongoDB
    
    Args:
        soup: BeautifulSoup object de la page TheFork
    
    Returns:
        Liste des horaires au format MongoDB ["Monday: 9:00 AM ‚Äì 11:00 PM", etc.]
    """
    # Ordre standard des jours pour MongoDB
    days_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
    
    # Mapping fran√ßais -> anglais
    fr_to_en = {
        "lundi": "Monday",
        "mardi": "Tuesday", 
        "mercredi": "Wednesday",
        "jeudi": "Thursday", 
        "vendredi": "Friday",
        "samedi": "Saturday",
        "dimanche": "Sunday"
    }
    
    # Initialiser les horaires par d√©faut
    formatted_hours = {day: f"{day}: Not specified" for day in days_order}
    
    # Extraire les horaires depuis les balises data-testid
    for tag in soup.select('div[data-testid^="interval-line-"]'):
        # Exemple : "interval-line-wednesday" -> "wednesday"
        testid = tag.get("data-testid")
        jour = testid.replace("interval-line-", "")
        
        # Extraire les heures en ignorant les mentions "aujourd'hui"
        heures = []
        for div in tag.find_all("div"):
            text = div.get_text(strip=True)
            if text and not any(x in text.lower() for x in ["aujourd'hui", "today"]):
                # Nettoyer et formater l'heure
                if "-" in text:
                    try:
                        start, end = text.split("-")
                        # Convertir en format 12h
                        start_time = convert_to_12h_format(start.strip())
                        end_time = convert_to_12h_format(end.strip())
                        heures.append(f"{start_time} ‚Äì {end_time}")
                    except:
                        continue
        
        if heures:
            # Convertir le jour en anglais
            en_day = None
            for fr_day, en_day_name in fr_to_en.items():
                if fr_day in jour.lower():
                    en_day = en_day_name
                    break
            
            if en_day:
                formatted_hours[en_day] = f"{en_day}: {' / '.join(heures)}"
    
    # Retourner les horaires dans l'ordre standard
    return [formatted_hours[day] for day in days_order]

def extract_thefork_data(lafourchette_url, restaurant_name=None):
    """
    Extraction des donn√©es depuis LaFourchette/TheFork via BrightData
    
    Args:
        lafourchette_url: URL de la page LaFourchette
        restaurant_name: Nom du restaurant (pour les logs)
    
    Returns:
        Dictionnaire des donn√©es extraites
    """
    log_prefix = f"[{restaurant_name}] " if restaurant_name else ""
    
    # Validation stricte de l'URL
    if not lafourchette_url or not isinstance(lafourchette_url, str):
        print(f"{log_prefix}‚ö†Ô∏è URL LaFourchette invalide ou manquante")
        return {}
    
    # S'assurer que l'URL commence par http/https
    if not lafourchette_url.lower().startswith(("http://", "https://")):
        print(f"{log_prefix}‚ö†Ô∏è URL LaFourchette invalide: {lafourchette_url}")
        return {}
    
    print(f"üç¥ Extraction des donn√©es LaFourchette pour {restaurant_name}")
    
    try:
        # Utiliser BrightData pour contourner les protections anti-bot
        html = fetch_html_with_brightdata(url=lafourchette_url, name=restaurant_name, platform="lafourchette")
        if not html:
            print(f"{log_prefix}‚ùå Erreur lors de l'extraction des donn√©es LaFourchette: Pas de HTML r√©cup√©r√©")
            return {}
        
        # Parser le HTML avec BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')
        
        # Initialiser le dictionnaire de r√©sultats
        result = {
            "opening_hours": [],
            "photos": [],
            "phone_number": "",
            "website": "",
            "rating": 0,
            "price_level": "",
            "description": ""
        }
        
        # Extraire les horaires d'ouverture (plusieurs m√©thodes possibles)
        hours_container = soup.find('div', class_=lambda c: c and ('timeslots' in c.lower() or 'hours' in c.lower() or 'horaires' in c.lower()))
        if hours_container:
            days_fr = ["Lundi", "Mardi", "Mercredi", "Jeudi", "Vendredi", "Samedi", "Dimanche"]
            days_en = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
            all_days = days_fr + days_en
            
            formatted_hours = []
            day_elements = hours_container.find_all(['div', 'p', 'li', 'span'], text=lambda t: t and any(day.lower() in t.lower() for day in all_days))
            
            if day_elements:
                for day_el in day_elements:
                    day_text = day_el.get_text(strip=True)
                    if day_text and any(day.lower() in day_text.lower() for day in all_days):
                        formatted_hours.append(day_text)
            
            if not formatted_hours:
                # M√©thode alternative: extraire tout le texte des horaires
                hours_text = hours_container.get_text(strip=True)
                if hours_text:
                    # Diviser par des s√©parateurs communs
                    for separator in [',', '.', ';', '\n']:
                        if separator in hours_text:
                            formatted_hours = [part.strip() for part in hours_text.split(separator) if part.strip()]
                            break
                    
                    if not formatted_hours:
                        formatted_hours = [hours_text]
            
            result["opening_hours"] = formatted_hours
            print(f"{log_prefix}‚úÖ Horaires extraits: {len(formatted_hours)} entr√©es")
        
        # Extraire les images (plusieurs s√©lecteurs possibles)
        photo_urls = []
        
        # 1. Photos dans le carrousel principal
        carousel_images = soup.select('.carousel img[src], .gallery img[src], .photos img[src], [data-test="restaurant-cover"] img[src]')
        if carousel_images:
            for img in carousel_images:
                src = img.get('src')
                if src:
                    if src.startswith('//'):
                        src = 'https:' + src
                    photo_urls.append(src)
        
        # 2. Photos dans les miniatures ou galeries
        thumbnail_images = soup.select('.restaurant-photos img[src], .thumbnail img[src], [data-test="restaurant-photos"] img[src]')
        if thumbnail_images:
            for img in thumbnail_images:
                src = img.get('src')
                if src:
                    if src.startswith('//'):
                        src = 'https:' + src
                    photo_urls.append(src)
        
        # 3. Recherche g√©n√©rique d'images
        if not photo_urls:
            for img in soup.find_all("img"):
                src = img.get("src") or img.get("data-src")
                if src and not src.startswith("data:"):
                    # Filtrer les petites images et les ic√¥nes
                    is_icon = "icon" in src.lower() or "logo" in src.lower() or "avatar" in src.lower()
                    if not is_icon:
                        if src.startswith("//"):
                            src = "https:" + src
                        photo_urls.append(src)
        
        # D√©dupliquer et limiter √† 5 photos (URLs)
        unique_photo_urls = list(dict.fromkeys(photo_urls))[:5]
        
        # Convertir les URLs en Base64
        photos_base64 = []
        if unique_photo_urls:
            print(f"{log_prefix}‚öôÔ∏è Conversion de {len(unique_photo_urls)} photos en Base64...")
            for photo_url in unique_photo_urls:
                base64_data = url_to_base64(photo_url)
                if base64_data:
                    photos_base64.append(base64_data)
            
            result["photos"] = photos_base64
            print(f"{log_prefix}‚úÖ Photos converties en Base64: {len(photos_base64)}")
        
        # Extraire le num√©ro de t√©l√©phone
        phone_element = soup.find('a', href=lambda h: h and h.startswith('tel:'))
        if phone_element:
            phone = phone_element.get_text(strip=True)
            result["phone_number"] = phone
            print(f"{log_prefix}‚úÖ T√©l√©phone extrait: {phone}")
        
        # Extraire le site web
        website_element = soup.find('a', href=lambda h: h and (h.startswith('http') and not ('lafourchette' in h or 'thefork' in h)))
        if website_element:
            website = website_element.get('href')
            result["website"] = website
            print(f"{log_prefix}‚úÖ Site web extrait: {website}")
        
        # Extraire la note
        rating_elements = soup.select('[data-test="restaurant-rating"], .restaurant-rating, .rating, .score')
        for rating_element in rating_elements:
            rating_text = rating_element.get_text(strip=True)
            # Chercher un nombre de 1 √† 10 ou de 1 √† 5 avec √©ventuellement une d√©cimale
            rating_match = re.search(r'(\d+[.,]\d+|\d+)(?:\s*[/|]?\s*(?:5|10))?', rating_text)
            if rating_match:
                try:
                    rating_value = float(rating_match.group(1).replace(',', '.'))
                    # Normaliser sur 5
                    if rating_value > 5:
                        rating_value /= 2
                    result["rating"] = rating_value
                    print(f"{log_prefix}‚úÖ Note extraite: {rating_value}")
                    break
                except:
                    pass
        
        # Extraire le niveau de prix (‚Ç¨, ‚Ç¨‚Ç¨, ‚Ç¨‚Ç¨‚Ç¨)
        price_elements = soup.select('[data-test="restaurant-price"], .restaurant-price, .price')
        for price_element in price_elements:
            price_text = price_element.get_text(strip=True)
            if '‚Ç¨' in price_text:
                # Compter le nombre de symboles ‚Ç¨ pour d√©terminer le niveau de prix
                price_level = price_text.count('‚Ç¨')
                result["price_level"] = price_level
                print(f"{log_prefix}‚úÖ Niveau de prix extrait: {price_level}")
                break
        
        return result  # Correction: Cette ligne doit √™tre au m√™me niveau que la boucle for, pas √† l'int√©rieur
        
    except Exception as e:
        print(f"{log_prefix}‚ùå Erreur lors de l'extraction des donn√©es TripAdvisor: {str(e)}")
        traceback.print_exc()
        return {}

def extract_tripadvisor_data(tripadvisor_url, restaurant_name=None):
    """
    Extraction des donn√©es depuis TripAdvisor via BrightData
    
    Args:
        tripadvisor_url: URL de la page TripAdvisor
        restaurant_name: Nom du restaurant (pour les logs)
    
    Returns:
        Dictionnaire des donn√©es extraites (notamment photos en Base64)
    """
    log_prefix = f"[{restaurant_name}] " if restaurant_name else ""
    
    # Validation de l'URL
    if not tripadvisor_url or not isinstance(tripadvisor_url, str) or not tripadvisor_url.lower().startswith(("http://", "https://")):
        print(f"{log_prefix}‚ö†Ô∏è URL TripAdvisor invalide ou manquante: {tripadvisor_url}")
        return {}
        
    print(f"üåê Extraction des donn√©es TripAdvisor pour {restaurant_name}")
    
    try:
        # Utiliser BrightData
        html = fetch_html_with_brightdata(url=tripadvisor_url, name=restaurant_name, platform="tripadvisor")
        if not html:
            print(f"{log_prefix}‚ùå Erreur lors de l'extraction des donn√©es TripAdvisor: Pas de HTML r√©cup√©r√©")
            return {}
            
        soup = BeautifulSoup(html, 'html.parser')
        
        result = {
            "photos": [],
            "opening_hours": [], # Placeholder, could reuse extract_opening_hours_tripadvisor if needed
            "rating": 0, # Placeholder
            # Ajouter d'autres champs si n√©cessaire
        }
        
        # Extraire les URLs des photos (TripAdvisor selectors might vary)
        photo_urls = []
        # Common selectors for TripAdvisor images
        photo_selectors = [
            'img.basicImg', 
            'div[data-gallery-photos] img', 
            'img[data-photoid]', 
            'div.photo-grid img[src]', 
            'img[data-lazyurl]'
        ]
        
        for selector in photo_selectors:
            images = soup.select(selector)
            for img in images:
                src = img.get('src') or img.get('data-src') or img.get('data-lazyurl')
                if src and not src.startswith('data:'):
                    if src.startswith('//'):
                        src = 'https:' + src
                    # Basic check to avoid tiny icons/logos
                    if 'avatar' not in src.lower() and 'icon' not in src.lower(): 
                         photo_urls.append(src)
            if photo_urls: # Stop if found with one selector
                 break 
                 
        # D√©dupliquer et limiter (e.g., √† 5 photos)
        unique_photo_urls = list(dict.fromkeys(photo_urls))[:5]
        
        # Convertir les URLs en Base64
        photos_base64 = []
        if unique_photo_urls:
            print(f"{log_prefix}‚öôÔ∏è Conversion de {len(unique_photo_urls)} photos TripAdvisor en Base64...")
            for photo_url in unique_photo_urls:
                base64_data = url_to_base64(photo_url)
                if base64_data:
                    photos_base64.append(base64_data)
            
            result["photos"] = photos_base64
            print(f"{log_prefix}‚úÖ Photos TripAdvisor converties en Base64: {len(photos_base64)}")
            
        # TODO: Ajouter l'extraction d'autres donn√©es TripAdvisor (horaires, note, etc.) ici si n√©cessaire
        # Par exemple:
        # result['opening_hours'] = extract_opening_hours_tripadvisor(soup)
        # ... extraction de la note ...
            
        return result
        
    except Exception as e:
        print(f"{log_prefix}‚ùå Erreur lors de l'extraction des donn√©es TripAdvisor: {str(e)}")
        traceback.print_exc()
        return {}

@timing_decorator
def enrich_with_platforms(structured_data, name, address):
    """
    Enrichit les donn√©es du restaurant avec les informations des plateformes
    
    Args:
        structured_data: Donn√©es existantes du restaurant
        name: Nom du restaurant
        address: Adresse du restaurant
    
    Returns:
        Donn√©es enrichies avec les informations des plateformes
    """
    # Si BrightData n'est pas activ√©, on ne fait pas d'enrichissement
    if not BRIGHTDATA_ENABLED:
        print(f"‚ÑπÔ∏è Enrichissement des plateformes d√©sactiv√© (pas de token BrightData)")
        return structured_data
    
    print(f"üîç Recherche de liens pour {name}")
    
    # Rechercher des liens via Bing
    try:
        platform_links = search_links_bing(name, address)
        if not platform_links:
            print(f"‚ö†Ô∏è Aucun lien de plateforme trouv√© pour {name}")
            return structured_data
            
        # Extraire les informations de TheFork/LaFourchette
        thefork_url = platform_links.get('thefork')
        thefork_data = {}
        if thefork_url and validate_platform_link(thefork_url, platform='thefork'):
            print(f"üç¥ Extraction des donn√©es LaFourchette pour {name}")
            thefork_data = extract_thefork_data(thefork_url, restaurant_name=name)
        
        # Extraire les informations de TripAdvisor
        tripadvisor_url = platform_links.get('tripadvisor')
        tripadvisor_data = {}
        if tripadvisor_url and validate_platform_link(tripadvisor_url, platform='tripadvisor'):
            print(f"üåê Extraction des donn√©es TripAdvisor pour {name}")
            tripadvisor_data = extract_tripadvisor_data(tripadvisor_url, restaurant_name=name)
        
        # Fusionner les donn√©es
        merged_data = structured_data.copy()
        
        # Mettre √† jour les horaires (priorit√©: TheFork, puis TripAdvisor)
        if thefork_data.get('opening_hours'):
            merged_data['opening_hours'] = thefork_data['opening_hours']
            print(f"‚úÖ Horaires extraits de LaFourchette: {len(thefork_data['opening_hours'])} entr√©es")
        elif tripadvisor_data.get('opening_hours'):
            merged_data['opening_hours'] = tripadvisor_data['opening_hours']
            print(f"‚úÖ Horaires extraits de TripAdvisor: {len(tripadvisor_data['opening_hours'])} entr√©es")
        
        # Mettre √† jour le t√©l√©phone
        if thefork_data.get('phone_number'):
            merged_data['phone_number'] = thefork_data['phone_number']
            merged_data['international_phone_number'] = thefork_data['phone_number']
            print(f"‚úÖ T√©l√©phone extrait de LaFourchette: {thefork_data['phone_number']}")
        elif tripadvisor_data.get('phone_number'):
            merged_data['phone_number'] = tripadvisor_data['phone_number']
            merged_data['international_phone_number'] = tripadvisor_data['phone_number']
            print(f"‚úÖ T√©l√©phone extrait de TripAdvisor: {tripadvisor_data['phone_number']}")
        
        # Mettre √† jour le site web
        if thefork_data.get('website'):
            merged_data['website'] = thefork_data['website']
            print(f"‚úÖ Site web extrait de LaFourchette: {thefork_data['website']}")
        elif tripadvisor_data.get('website'):
            merged_data['website'] = tripadvisor_data['website']
            print(f"‚úÖ Site web extrait de TripAdvisor: {tripadvisor_data['website']}")
        
        # Mettre √† jour la note si non d√©finie
        if merged_data.get('rating', 0) == 0:
            if thefork_data.get('rating', 0) > 0:
                merged_data['rating'] = thefork_data['rating']
                print(f"‚úÖ Note extraite de LaFourchette: {thefork_data['rating']}")
            elif tripadvisor_data.get('rating', 0) > 0:
                merged_data['rating'] = tripadvisor_data['rating']
                print(f"‚úÖ Note extraite de TripAdvisor: {tripadvisor_data['rating']}")
        
        # Mettre √† jour le niveau de prix si non d√©fini
        if not merged_data.get('price_level') and thefork_data.get('price_level'):
            merged_data['price_level'] = thefork_data['price_level']
            print(f"‚úÖ Niveau de prix extrait de LaFourchette: {thefork_data['price_level']}")
        
        # Mettre √† jour la description si non d√©finie
        if not merged_data.get('description') and thefork_data.get('description'):
            merged_data['description'] = thefork_data['description']
            print(f"‚úÖ Description extraite de LaFourchette: {len(thefork_data['description'])} caract√®res")
        
        # Mettre √† jour les photos (priorit√© aux photos de TheFork et TripAdvisor)
        photos = []
        
        # D'abord les photos de TheFork
        if thefork_data.get('photos'):
            photos.extend(thefork_data['photos'])
            print(f"‚úÖ Photos extraites de LaFourchette: {len(thefork_data['photos'])}")
        
        # Ensuite les photos de TripAdvisor
        if tripadvisor_data.get('photos'):
            photos.extend(tripadvisor_data['photos'])
            print(f"‚úÖ Photos extraites de TripAdvisor: {len(tripadvisor_data['photos'])}")
        
        # Supprimer les doublons et limiter le nombre total de photos
        photos = list(dict.fromkeys(photos))[:10]
        
        # Mise √† jour des photos seulement si nous en avons trouv√©
        if photos:
            # Si nous avons une photo principale mais pas de photos
            if merged_data.get('photo') and not merged_data.get('photos'):
                merged_data['photos'] = [merged_data['photo']] + photos
            else:
                merged_data['photos'] = photos
            
            # Utiliser la premi√®re photo comme photo principale si elle n'existe pas
            if not merged_data.get('photo') and photos:
                merged_data['photo'] = photos[0]
            
            print(f"‚úÖ Total de photos apr√®s fusion: {len(merged_data.get('photos', []))}")
        
        # Tra√ßage
        print(f"‚úÖ Enrichissement termin√© pour {name}")
        return merged_data
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'enrichissement avec des plateformes: {str(e)}")
        traceback.print_exc()
        return structured_data

@timing_decorator
def extract_with_brightdata(url, name=None, platform=None):
    """
    Extrait les donn√©es d'une plateforme via BrightData
    """
    if not url:
        return None
        
    print(f"[{name}] üåê Scraping {platform} via BrightData...")
    
    try:
        # Configuration BrightData
        brightdata_url = f"http://{BRIGHTDATA_TOKEN}:@brd.superproxy.io:22225"
        proxies = {
            "http": brightdata_url,
            "https": brightdata_url
        }
        
        # Requ√™te avec BrightData
        response = requests.get(url, proxies=proxies, timeout=120)
        response.raise_for_status()
        
        # Parsing avec BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extraction selon la plateforme
        if platform == 'thefork' or 'lafourchette' in url:
            return extract_thefork_data(url=url, restaurant_name=name)
        elif platform == 'tripadvisor' or 'tripadvisor' in url:
            return extract_tripadvisor_data(tripadvisor_url=url, restaurant_name=name)
            
    except Exception as e:
        print(f"‚ùå [{name}] Erreur BrightData pour {platform}: {str(e)}")
        return None

@timing_decorator
def save_to_mongodb(restaurant_data):
    """
    Sauvegarde les donn√©es du restaurant dans MongoDB
    """
    try:
        # Normaliser les donn√©es
        normalized_data = normalize_restaurant_data(restaurant_data)
        
        # Connexion √† MongoDB Atlas en utilisant la variable globale
        client = MongoClient(MONGODB_URI)
        db = client[DB_NAME]
        collection = db[COLLECTION_NAME]
        
        # Stocker l'ID et le retirer de l'ensemble de donn√©es pour l'update
        doc_id = normalized_data.get("_id")
        
        # Cr√©er une copie pour l'update sans modifier le champ _id
        update_data = normalized_data.copy()
        if "_id" in update_data:
            del update_data["_id"]
        
        # Utiliser update_one avec upsert=True pour √©viter les doublons
        # Place_id est utilis√© comme cl√© si disponible, sinon utiliser le nom
        if doc_id:
            identifier = {"_id": doc_id}
        else:
            identifier = {"name": normalized_data["name"]}
        
        # Ins√©rer ou mettre √† jour le restaurant
        result = collection.update_one(
            identifier,
            {"$set": update_data},
            upsert=True
        )
        
        if result.acknowledged:
            if result.matched_count > 0:
                print(f"‚úÖ {normalized_data['name']}: Restaurant mis √† jour dans MongoDB")
            else:
                print(f"‚úÖ {normalized_data['name']}: Restaurant ajout√© dans MongoDB")
            return True
        else:
            print(f"‚ùå {normalized_data['name']}: √âchec de la sauvegarde dans MongoDB")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde dans MongoDB: {str(e)}")
        traceback.print_exc()
        return False
    finally:
        if 'client' in locals():
            client.close()

# Fonction pour v√©rifier si un restaurant existe sur Google Maps
@timing_decorator
def verify_restaurant_on_maps(name, address, lat=None, lon=None, place_id=None):
    """
    V√©rifie si le restaurant existe sur Google Maps et r√©cup√®re ses donn√©es
    
    Args:
        name: Nom du restaurant
        address: Adresse du restaurant
        lat: Latitude (optionnelle)
        lon: Longitude (optionnelle)
        place_id: ID du lieu Google Maps (optionnel)
    
    Returns:
        Dictionnaire avec les donn√©es du restaurant trouv√©
        ou None si non trouv√©
    """
    try:
        # Si nous avons un place_id et des coordonn√©es, nous pouvons cr√©er directement les donn√©es
        if place_id and lat is not None and lon is not None:
            return {
                "name": name,
                "address": address,
                "maps_url": f"https://www.google.com/maps/place/?q=place_id:{place_id}",
                "place_id": place_id,
                "latitude": lat,
                "longitude": lon
            }
            
        print(f"üîç Recherche de {name} ({address}) sur Google Maps")
        
        with ChromeSessionManager() as session:
            # Utiliser le driver de la session et non la session elle-m√™me
            driver = session.driver
            
            # V√©rifier que le driver a √©t√© correctement initialis√©
            if not driver:
                print(f"‚ùå √âchec d'initialisation du driver Chrome pour {name}")
                return None
                
            # Si des coordonn√©es sont sp√©cifi√©es, rechercher directement par coordonn√©es
            if lat is not None and lon is not None:
                maps_url = f"https://www.google.com/maps/search/{lat},{lon}"
            else:
                # Sinon recherche par nom et adresse
                maps_url = search_google_maps(driver, name, address)
                
            if not maps_url:
                print(f"‚ùå {name}: URL Google Maps non trouv√©e")
                return None
                
            # Extraire l'ID de lieu et les coordonn√©es
            extracted_place_id = extract_place_id(driver.current_url)
            coordinates = extract_coordinates(driver)
            
            if not extracted_place_id and not coordinates:
                print(f"‚ùå {name}: Impossible d'extraire l'ID de lieu ou les coordonn√©es")
                return None
                
            return {
                "name": name,
                "address": address,
                "maps_url": maps_url,
                "place_id": extracted_place_id,
                "latitude": coordinates.get("lat") if coordinates else None,
                "longitude": coordinates.get("lng") if coordinates else None
            }
            
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification sur Google Maps: {str(e)}")
        traceback.print_exc()
        return None

@timing_decorator
def process_restaurant(restaurant):
    """
    Traite un restaurant complet avec toutes les √©tapes
    """
    try:
        name = restaurant.get("name", "")
        address = restaurant.get("address", "")
        place_id = restaurant.get("place_id", "")
        lat = restaurant.get("lat")
        lon = restaurant.get("lon")
        rating = restaurant.get("rating", 0)
        
        print(f"\n{'='*50}")
        print(f"Traitement de: {name}, {address}")
        print(f"{'='*50}\n")
        
        # V√©rifier si le restaurant existe d√©j√† dans MongoDB
        if is_restaurant_in_mongodb(name, restaurant.get("maps_url"), place_id):
            print(f"‚ö†Ô∏è {name}: D√©j√† dans MongoDB, on passe au suivant")
            return True
            
        # Si nous avons d√©j√† toutes les informations n√©cessaires depuis l'API Google Maps
        if place_id and lat is not None and lon is not None:
            print(f"[{name}] ‚úÖ Utilisation des donn√©es Google Maps API existantes")
            # Cr√©er directement les donn√©es √† partir des informations de l'API
            maps_data = {
                "name": name,
                "address": address,
                "maps_url": f"https://www.google.com/maps/place/?q=place_id:{place_id}",
                "place_id": place_id,
                "latitude": lat,
                "longitude": lon,
                "rating": rating
            }
        else:
            # Sinon, v√©rifier sur Google Maps via le navigateur en transmettant les informations disponibles
            print(f"[{name}] üîç V√©rification sur Google Maps...")
            maps_data = verify_restaurant_on_maps(
                name=name, 
                address=address,
                lat=lat,
                lon=lon,
                place_id=place_id
            )
            
            if not maps_data:
                # Si la recherche sur Maps a √©chou√© mais qu'on a d√©j√† les coordonn√©es, cr√©er une fiche minimale
                if lat is not None and lon is not None:
                    print(f"[{name}] ‚ö†Ô∏è Cr√©ation d'une fiche minimale avec les coordonn√©es disponibles")
                    maps_data = {
                        "name": name,
                        "address": address,
                        "maps_url": f"https://www.google.com/maps/search/{lat},{lon}",
                        "place_id": place_id if place_id else f"custom_{hashlib.md5(name.encode()).hexdigest()[:10]}_{name.replace(' ', '_')}",
                        "latitude": lat,
                        "longitude": lon,
                        "rating": rating
                    }
                else:
                    print(f"‚ùå {name}: Non trouv√© sur Google Maps")
                    return False
            
        # √âtape 2: Capture des screenshots et extraction des donn√©es additionnelles
        print(f"[{name}] üì∏ Capture des donn√©es visuelles...")
        restaurant_data = process_restaurant_with_maps_screenshots(maps_data)
        if not restaurant_data:
            print(f"‚ùå {name}: √âchec de l'extraction des donn√©es visuelles")
            return False
            
        # √âtape 3: Enrichissement avec les plateformes externes
        if USE_BRIGHTDATA:
            print(f"[{name}] üåê Enrichissement avec les plateformes externes...")
            platform_links = search_links_bing(name, address)
            if platform_links:
                restaurant_data["platform_links"] = platform_links
        
        # √âtape 4: Sauvegarde en MongoDB
        print(f"[{name}] üíæ Sauvegarde en MongoDB...")
        if save_to_mongodb(restaurant_data):
            print(f"‚úÖ {name}: Traitement r√©ussi")
            return True
        else:
            print(f"‚ùå {name}: √âchec de la sauvegarde MongoDB")
            return False
            
    except Exception as e:
        print(f"‚ùå {name}: Erreur lors du traitement: {str(e)}")
        traceback.print_exc()
        return False
    finally:
        print(f"\n{'='*50}\n")

@timing_decorator
def process_restaurants_with_threadpool(restaurants, num_threads=NUM_THREADS, skip_existing=True):
    """
    Traite une liste de restaurants en parall√®le avec un pool de threads
    
    Args:
        restaurants: Liste de restaurants √† traiter
        num_threads: Nombre de threads √† utiliser
        skip_existing: Si True, ignore les restaurants d√©j√† en base
    
    Returns:
        Tuple (nb_success, nb_total)
    """
    if not restaurants:
        print("‚ùå Aucun restaurant √† traiter")
        return 0, 0
    
    # V√©rifier les restaurants d√©j√† en base si demand√©
    if skip_existing:
        print("üîç V√©rification des restaurants d√©j√† en base...")
        restaurants_to_process = []
        
        for restaurant in restaurants:
            name = restaurant.get("name", "")
            maps_url = restaurant.get("maps_url", None)
            place_id = restaurant.get("place_id", None)
            
            if not is_restaurant_in_mongodb(name, maps_url, place_id):
                restaurants_to_process.append(restaurant)
        
        skipped = len(restaurants) - len(restaurants_to_process)
        print(f"üìä {skipped}/{len(restaurants)} restaurants d√©j√† en base, ignor√©s")
        
        restaurants = restaurants_to_process
    
    if not restaurants:
        print("‚úÖ Tous les restaurants sont d√©j√† en base, rien √† faire")
        return 0, 0
    
    # Cr√©er un pool de threads et soumettre les t√¢ches
    thread_pool = ThreadPoolExecutor(max_workers=num_threads)
    future_to_restaurant = {
        thread_pool.submit(process_restaurant, restaurant): restaurant
        for restaurant in restaurants
    }
    
    # Suivi du progr√®s et r√©sultats
    completed = 0
    success = 0
    total = len(future_to_restaurant)
    
    print("\n" + "=" * 50)
    
    # Traiter les r√©sultats au fur et √† mesure
    for future in as_completed(future_to_restaurant):
        restaurant = future_to_restaurant[future]
        name = restaurant.get("name", "Inconnu")
        
        try:
            result = future.result()
            if result:
                success += 1
                print(f"‚úÖ {name}: Traitement r√©ussi")
            else:
                print(f"‚ùå {name}: √âchec du traitement")
        except Exception as e:
            print(f"‚ùå {name}: Erreur lors du traitement: {str(e)}")
        
        completed += 1
        print(f"\rüìä Progr√®s: {completed}/{total} restaurants trait√©s ({success} r√©ussis)", end="")
    
    thread_pool.shutdown()
    
    print(f"\n\nüéâ Traitement termin√©: {success}/{total} restaurants trait√©s avec succ√®s")
    
    return success, total

def print_timing_stats():
    """Affiche les statistiques de timing pour aider √† identifier les goulots d'√©tranglement"""
    if not TIMING_STATS:
        return
    
    print("\nüìä STATISTIQUES DE PERFORMANCE:")
    print("=" * 80)
    print(f"{'FONCTION':<40} | {'APPELS':<6} | {'MOYENNE (s)':<12} | {'MIN (s)':<10} | {'MAX (s)':<10}")
    print("-" * 80)
    
    for func_name, times in sorted(TIMING_STATS.items(), key=lambda x: sum(x[1])/len(x[1]) if x[1] else 0, reverse=True):
        if not times:
            continue
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        print(f"{func_name:<40} | {len(times):<6} | {avg_time:<12.2f} | {min_time:<10.2f} | {max_time:<10.2f}")
    
    print("=" * 80)
    print("Les fonctions sont tri√©es par temps moyen d'ex√©cution (du plus lent au plus rapide)")
    print("Ces statistiques vous aideront √† identifier les goulots d'√©tranglement du pipeline")

def parse_args():
    """
    Parse les arguments en ligne de commande
    """
    global DEBUG_MODE, NUM_THREADS, USE_BRIGHTDATA, BRIGHTDATA_ENABLED
    
    parser = argparse.ArgumentParser(description="Pipeline de collecte et traitement des restaurants")
    
    # Ajouter des options de ligne de commande
    parser.add_argument("--debug", action="store_true", help="Activer le mode debug avec logs d√©taill√©s")
    parser.add_argument("--threads", type=int, default=NUM_THREADS, help=f"Nombre de threads (d√©faut: {NUM_THREADS})")
    parser.add_argument("--max", type=int, default=None, help="Nombre maximum de restaurants √† traiter")
    parser.add_argument("--start", type=int, default=0, help="Index de d√©part pour le traitement des restaurants")
    parser.add_argument("--brightdata", action="store_true", help="Utiliser BrightData pour contourner les mesures anti-bot")
    parser.add_argument("--small-area", action="store_true", help="Utiliser une zone de test plus petite")
    parser.add_argument("--skip-existing", action="store_true", help="Ignorer les restaurants d√©j√† dans MongoDB")
    parser.add_argument("--restaurant", type=str, help="Traiter un restaurant sp√©cifique (nom, adresse)")
    parser.add_argument("--test", action="store_true", help="Ex√©cuter en mode test sur quelques restaurants")
    parser.add_argument("--load-from-file", type=str, help="Charger les restaurants depuis un fichier")
    # Nouvelles options
    parser.add_argument("--zones", type=int, default=None, help="Nombre de zones g√©ographiques √† traiter")
    parser.add_argument("--max-restaurants", type=int, default=None, help="Nombre maximum de restaurants √† traiter")
    parser.add_argument("--test-area", action="store_true", help="Utiliser une petite zone de test")
    
    args = parser.parse_args()
    
    # Mettre √† jour les variables globales selon les arguments
    DEBUG_MODE = args.debug
    NUM_THREADS = args.threads
    USE_BRIGHTDATA = args.brightdata
    BRIGHTDATA_ENABLED = args.brightdata
    
    return args

# =============================================
# FONCTION PRINCIPALE
# =============================================

def main():
    """
    Fonction principale ex√©cut√©e quand le script est lanc√© directement
    """
    global USE_BRIGHTDATA, BRIGHTDATA_ENABLED
    
    # Cr√©er les dossiers n√©cessaires
    os.makedirs(SCREENSHOT_DIR, exist_ok=True)
    
    # Enregistrer la fonction de nettoyage
    atexit.register(cleanup_temp_dirs)
    
    # Parser les arguments
    args = parse_args()
    
    # Configurer les options en fonction des arguments
    USE_BRIGHTDATA = args.brightdata
    BRIGHTDATA_ENABLED = args.brightdata
    DEBUG_MODE = args.debug
    
    # V√©rifier le contenu de MongoDB avant de commencer
    check_mongodb_content()
    
    # Traitement sp√©cial pour le mode test-area
    if args.test_area:
        print("\nüìã Mode zone de test activ√©")
        test_zone = get_small_test_area()
        restaurants = get_restaurants_in_zone(test_zone)
        
        # Limiter si n√©cessaire
        if args.max_restaurants:
            restaurants = restaurants[:args.max_restaurants]
            print(f"üîÑ Nombre de restaurants limit√© √† {args.max_restaurants}")
        
        print(f"‚úÖ {len(restaurants)} restaurants trouv√©s dans la zone de test")
        process_restaurants_with_threadpool(restaurants, num_threads=args.threads, skip_existing=False)
        return
        
    # Traitement sp√©cial pour le mode zones limit√©
    if args.zones:
        print(f"\nüìã Mode zones limit√© activ√©: {args.zones} zones")
        zones = generate_zones()
        limited_zones = zones[:args.zones]
        
        all_restaurants = []
        for i, zone in enumerate(limited_zones):
            print(f"üìç Traitement de la zone {i+1}/{len(limited_zones)}")
            zone_restaurants = get_restaurants_in_zone(zone)
            all_restaurants.extend(zone_restaurants)
            
            if args.max_restaurants and len(all_restaurants) >= args.max_restaurants:
                all_restaurants = all_restaurants[:args.max_restaurants]
                print(f"üîÑ Nombre de restaurants limit√© √† {args.max_restaurants} - arr√™t du traitement de zones")
                break
                
        print(f"‚úÖ Total de {len(all_restaurants)} restaurants r√©cup√©r√©s dans {len(limited_zones)} zones")
        process_restaurants_with_threadpool(all_restaurants, num_threads=args.threads, skip_existing=args.skip_existing)
        return
    
    # Continuer avec le comportement normal pour les autres options...
    
    # Charger les donn√©es de restaurants
    print("\nüìã Chargement des donn√©es de restaurants...")
    
    # Utiliser un jeu de donn√©es plus petit pour les tests si demand√©
    if args.small_area:
        restaurants = load_test_restaurants()
    else:
        restaurants = load_restaurants_from_file()
    
    if not restaurants:
        print("‚ùå Aucun restaurant trouv√© dans les donn√©es")
        return
        
    total_restaurants = len(restaurants)
    print(f"‚úÖ {total_restaurants} restaurants charg√©s")
    
    # Appliquer les limites
    start_idx = 0  # Initialiser √† 0 par d√©faut
    if hasattr(args, 'start'):
        start_idx = min(args.start, total_restaurants - 1)
        
    if args.max:
        end_idx = min(start_idx + args.max, total_restaurants)
    else:
        end_idx = total_restaurants
        
    restaurants_to_process = restaurants[start_idx:end_idx]
    print(f"üîÑ Traitement de {len(restaurants_to_process)} restaurants (#{start_idx} √† #{end_idx-1})")
    
    # Traiter les restaurants
    start_time = time.time()
    
    if args.threads > 1:
        print(f"‚öôÔ∏è Utilisation de {args.threads} threads parall√®les")
        with ThreadPoolExecutor(max_workers=args.threads) as executor:
            results = list(executor.map(process_restaurant, restaurants_to_process))
        
        success_count = sum(1 for r in results if r)
    else:
        print("‚öôÔ∏è Traitement s√©quentiel")
        success_count = 0
        for restaurant in restaurants_to_process:
            if process_restaurant(restaurant):
                success_count += 1
    
    # Afficher le r√©sum√©
    elapsed_time = time.time() - start_time
    print(f"\n{'='*50}")
    print(f"R√©sum√© du traitement:")
    print(f"- {len(restaurants_to_process)} restaurants trait√©s")
    print(f"- {success_count} restaurants trait√©s avec succ√®s")
    print(f"- {len(restaurants_to_process) - success_count} √©checs")
    print(f"- Temps √©coul√©: {elapsed_time:.2f} secondes")
    print(f"{'='*50}\n")

def test_mongodb_results(restaurant_name):
    """
    Teste et affiche les r√©sultats d'un restaurant dans MongoDB
    """
    try:
        client = get_mongo_client()
        db = client[DB_NAME]
        collection = db[COLLECTION_NAME]
        
        # Rechercher le restaurant
        restaurant = collection.find_one({"name": restaurant_name})
        
        if not restaurant:
            print(f"‚ùå Restaurant {restaurant_name} non trouv√© dans MongoDB")
            return
            
        print(f"\n{'='*50}")
        print(f"R√©sultats MongoDB pour: {restaurant_name}")
        print(f"{'='*50}")
        
        # Afficher les informations principales
        print("\nüìã Informations principales:")
        print(f"  Nom: {restaurant.get('name', 'N/A')}")
        print(f"  Adresse: {restaurant.get('address', 'N/A')}")
        print(f"  Site web: {restaurant.get('website', 'N/A')}")
        print(f"  Prix: {restaurant.get('price_level', 'N/A')}")
        print(f"  Note: {restaurant.get('rating', 'N/A')}")
        print(f"  Nombre d'avis: {restaurant.get('user_ratings_total', 'N/A')}")
        
        # Afficher les liens des plateformes
        print("\nüîó Liens des plateformes:")
        platform_links = restaurant.get('platform_links', {})
        for platform, url in platform_links.items():
            print(f"  {platform}: {url}")
            
        # Afficher les horaires
        print("\n‚è∞ Horaires d'ouverture:")
        opening_hours = restaurant.get('opening_hours', [])
        for hours in opening_hours:
            print(f"  {hours}")
            
        # Afficher les options de service
        print("\nüõçÔ∏è Options de service:")
        service_options = restaurant.get('service_options', {})
        for option, value in service_options.items():
            print(f"  {option}: {value}")
            
        print(f"\n{'='*50}\n")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification MongoDB: {str(e)}")
    finally:
        client.close()

# Configuration globale
DEBUG_MODE = False  # Mode debug avec logs d√©taill√©s
USE_BRIGHTDATA = False  # Utilisation de BrightData pour contourner les mesures anti-bot
MAX_MAPS_API_REQUESTS = 500  # Limite quotidienne de requ√™tes Google Maps API
MAPS_API_REQUEST_COUNT = 0  # Compteur de requ√™tes Google Maps API
NUM_THREADS = 4  # Nombre de threads par d√©faut pour le traitement parall√®le

# Timeout pour les op√©rations r√©seau
NETWORK_TIMEOUT = 30  # Timeout en secondes pour les requ√™tes r√©seau

# Dictionnaire global pour stocker les statistiques de timing
TIMING_STATS = defaultdict(list)

# Dossier pour les captures d'√©cran
SCREENSHOT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "screenshots")
os.makedirs(SCREENSHOT_DIR, exist_ok=True)

# Fonction pour nettoyer les r√©pertoires temporaires
def cleanup_temp_directories():
    """Nettoie les r√©pertoires temporaires cr√©√©s par ChromeDriver"""
    try:
        temp_dir = tempfile.gettempdir()
        temp_chrome_dirs = glob.glob(os.path.join(temp_dir, "chrome_session_*"))
        for dir_path in temp_chrome_dirs:
            try:
                shutil.rmtree(dir_path, ignore_errors=True)
            except Exception as e:
                if DEBUG_MODE:
                    print(f"√âchec nettoyage r√©pertoire temp {dir_path}: {e}")
        if DEBUG_MODE:
            print(f"üßπ Nettoyage de {len(temp_chrome_dirs)} r√©pertoires temporaires")
    except Exception as e:
        if DEBUG_MODE:
            print(f"‚ùå Erreur lors du nettoyage des r√©pertoires temporaires: {e}")

# Enregistrer le nettoyage √† effectuer √† la fin du programme
atexit.register(cleanup_temp_directories)

def get_small_test_area():
    """
    Retourne une zone de test √©largie dans Paris (Montmartre et environs)
    
    Returns:
        Dictionnaire d√©finissant la zone de test
    """
    # Zone √©largie de Montmartre (convertir en format pour get_restaurants_in_zone)
    return {
        "lat_min": 49.8800,  # √âlargi vers le sud
        "lat_max": 48.8950,  # √âlargi vers le nord
        "lng_min": 2.3250,   # √âlargi vers l'ouest
        "lng_max": 2.3500    # √âlargi vers l'est
    }

# =============================================
# √âTAPE 2: CAPTURE D'√âCRAN GOOGLE MAPS + OCR
# =============================================

# Variable globale pour stocker le chemin vers le ChromeDriver
CHROME_DRIVER_PATH = None

class ChromeSessionManager:
    """
    G√®re une session Chrome pour les interactions avec les sites web
    """
    def __init__(self, headless=True):
        self.driver = None
        self.service = None
        self.temp_dir = None
        self.headless = headless
        self.max_retries = 3
    
    def __enter__(self):
        """D√©marre une session Chrome"""
        # Initialiser la liste de r√©pertoires temporaires si n√©cessaire
        if not hasattr(cleanup_temp_dirs, "temp_dirs"):
            cleanup_temp_dirs.temp_dirs = []
        
        for attempt in range(self.max_retries):
            try:
                # Cr√©er un r√©pertoire temporaire pour les donn√©es Chrome
                self.temp_dir = tempfile.mkdtemp(prefix="chrome_session_")
                cleanup_temp_dirs.temp_dirs.append(self.temp_dir)
                
                print(f"üîß Chrome utilise le r√©pertoire: {self.temp_dir}")
                
                # Options Chrome
                chrome_options = webdriver.ChromeOptions()
                
                if self.headless:
                    chrome_options.add_argument("--headless")
                    
                chrome_options.add_argument("--no-sandbox")
                chrome_options.add_argument("--disable-dev-shm-usage")
                chrome_options.add_argument("--disable-gpu")
                chrome_options.add_argument(f"--user-data-dir={self.temp_dir}")
                chrome_options.add_argument("--window-size=1920,1080")
                chrome_options.add_argument("--disable-notifications")
                chrome_options.add_argument("--disable-popup-blocking")
                chrome_options.add_argument("--disable-extensions")
                chrome_options.add_argument("--disable-infobars")
                
                # Utilisateur simul√©
                chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36")
                
                # Service Chrome
                if CHROME_DRIVER_PATH:
                    self.service = Service(executable_path=CHROME_DRIVER_PATH)
                else:
                    self.service = Service()  # Laisser Selenium trouver le pilote
                
                # Cr√©ation du driver
                self.driver = webdriver.Chrome(
                    service=self.service,
                    options=chrome_options
                )
                
                # Configurer les timeouts
                self.driver.set_page_load_timeout(30)
                self.driver.implicitly_wait(10)
                
                return self
            except Exception as e:
                print(f"‚ö†Ô∏è Tentative {attempt+1}/{self.max_retries} √©chou√©e: {str(e)}")
                self.cleanup()
                time.sleep(1)
        
        print("‚ùå Impossible de cr√©er une session Chrome apr√®s plusieurs tentatives")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Nettoie la session Chrome"""
        self.cleanup()
        
    def cleanup(self):
        """Nettoie les ressources Chrome"""
        try:
            if self.driver:
                print("  ‚Ü≥ Nettoyage des ressources Chrome")
                start_time = time.time()
                self.driver.quit()
                self.driver = None
                elapsed = time.time() - start_time
                print(f"  ‚Ü≥ Nettoyage termin√© en {elapsed:.2f} secondes")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du nettoyage Chrome: {str(e)}")

def format_address(restaurant):
    """
    Formate l'adresse d'un restaurant pour la recherche avec gestion avanc√©e des erreurs
    
    Args:
        restaurant: Dictionnaire contenant les infos du restaurant
    
    Returns:
        Adresse format√©e comme string
    """
    # V√©rification de base - si restaurant n'est pas un dict ou est None
    if not restaurant or not isinstance(restaurant, dict):
        return "Paris"  # Valeur par d√©faut s√©curis√©e
    
    # V√©rifier si l'adresse existe
    if "address" not in restaurant:
        # Utiliser les coordonn√©es si disponibles
        if "lat" in restaurant and "lon" in restaurant:
            lat = restaurant.get("lat", "")
            lon = restaurant.get("lon", "")
            if lat and lon:  # V√©rifier que les coordonn√©es ne sont pas vides
                return f"{lat}, {lon}, Paris"
        return "Paris"  # Valeur par d√©faut
    
    addr = restaurant["address"]
    
    # Si l'adresse est d√©j√† une cha√Æne, la retourner directement
    if isinstance(addr, str):
        return addr if addr.strip() else "Paris"  # Retourner Paris si la cha√Æne est vide
    
    # Si l'adresse est un dictionnaire, extraire les composants avec s√©curit√©
    if isinstance(addr, dict):
        address_parts = []
        
        # Extraire les composants avec v√©rification de type
        for field in ["housenumber", "street", "postcode", "city"]:
            if field in addr and addr[field]:
                value = addr[field]
                # Convertir en string si ce n'est pas d√©j√† le cas
                if not isinstance(value, str):
                    value = str(value)
                if value.strip():  # Ignorer les valeurs vides apr√®s nettoyage
                    address_parts.append(value)
        
        if address_parts:
            return ", ".join(address_parts)
    
    # Si pas d'adresse utilisable et coordonn√©es disponibles
    if "lat" in restaurant and "lon" in restaurant:
        lat = restaurant.get("lat", "")
        lon = restaurant.get("lon", "")
        if lat and lon:  # V√©rifier que les coordonn√©es ne sont pas vides
            return f"{lat}, {lon}, Paris"
    
    return "Paris"  # Valeur par d√©faut si rien d'autre n'est disponible

@timing_decorator
def search_google_maps(driver, name, address):
    """
    Recherche un restaurant sur Google Maps
    
    Args:
        driver: WebDriver Selenium
        name: Nom du restaurant
        address: Adresse du restaurant
    
    Returns:
        URL de la page Google Maps
    """
    query = urllib.parse.quote(f"{name} {address}")
    url = f"https://www.google.com/maps/search/{query}"
    driver.get(url)
    
    # Gestion du consentement si n√©cessaire
    try:
        WebDriverWait(driver, 5).until(EC.element_to_be_clickable(
            (By.XPATH, "//button[contains(., 'Accepter') or contains(., 'Accept')]")
        )).click()
        print("‚úÖ Consentement accept√©")
        time.sleep(2)
    except:
        print("‚ö†Ô∏è Aucun consentement √† g√©rer")
    
    # Attendre que le r√©sultat soit charg√©
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'DUwDvf')))
        time.sleep(2)
        return driver.current_url
    except:
        print("‚ö†Ô∏è R√©sultat non trouv√© sur Google Maps")
        return None

@timing_decorator
def screenshot_photo(driver, prefix, max_retries=2):
    """
    Capture la photo principale du restaurant sur Google Maps
    
    Args:
        driver: WebDriver Selenium
        prefix: Pr√©fixe pour le nom du fichier
        max_retries: Nombre maximum de tentatives
    
    Returns:
        Tuple (chemin de l'image, version base64, image PIL)
    """
    last_error = None
    for attempt in range(max_retries):
        try:
            if attempt > 0 and DEBUG_MODE:
                print(f"  ‚Ü≥ Tentative {attempt + 1}/{max_retries} de capture photo")
            
            # Attendre que la page soit compl√®tement charg√©e
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "DUwDvf"))
            )
            
            # Prendre la capture d'√©cran
            screenshot = driver.get_screenshot_as_png()
            image = Image.open(BytesIO(screenshot))
            
            # Coordonn√©es du crop (√† ajuster si n√©cessaire selon la mise en page de Google Maps)
            left = 30
            top = 70
            right = 330
            bottom = 230
            
            cropped = image.crop((left, top, right, bottom))
            
            # V√©rifier que l'image n'est pas vide ou trop petite
            if cropped.size[0] < 100 or cropped.size[1] < 100:
                raise ValueError("Image trop petite, possible erreur de capture")
            
            path = f"{prefix}_photo.png"
            cropped.save(path)
            
            if DEBUG_MODE and attempt > 0:
                print(f"  ‚Ü≥ Capture photo r√©ussie apr√®s {attempt + 1} tentative(s)")
            
            return path, encode_image_base64(cropped), cropped
            
        except Exception as e:
            last_error = e
            if DEBUG_MODE:
                print(f"‚ö†Ô∏è √âchec de capture photo (tentative {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)  # Pause avant nouvelle tentative
                continue
    
    raise RuntimeError(f"Impossible de capturer la photo apr√®s {max_retries} tentatives") from last_error

@timing_decorator
def screenshot_panel(driver, prefix, max_retries=2):
    """
    Capture le panneau d'informations lat√©ral sur Google Maps avec gestion des erreurs
    
    Args:
        driver: WebDriver Selenium
        prefix: Pr√©fixe pour le nom du fichier
        max_retries: Nombre maximum de tentatives
    
    Returns:
        Tuple (chemin de l'image, version base64, objet image)
    """
    last_error = None
    for attempt in range(max_retries):
        try:
            if attempt > 0 and DEBUG_MODE:
                print(f"  ‚Ü≥ Tentative {attempt + 1}/{max_retries} de capture du panneau")
            
            # Attendre que le panneau soit charg√©
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "DUwDvf"))
            )
            
            # Faire d√©filer pour voir toutes les informations
            scroll_to_bottom_info(driver)
            time.sleep(1)  # Attendre la fin du d√©filement
            
            # Prendre la capture d'√©cran
            screenshot = driver.get_screenshot_as_png()
            image = Image.open(BytesIO(screenshot))
            cropped = image.crop((0, 0, 600, 1700))
            
            # V√©rifier que l'image n'est pas vide
            if cropped.size[0] < 100 or cropped.size[1] < 100:
                raise ValueError("Image du panneau trop petite")
            
            path = f"{prefix}_panel.png"
            cropped.save(path)
            
            if DEBUG_MODE and attempt > 0:
                print(f"  ‚Ü≥ Capture du panneau r√©ussie apr√®s {attempt + 1} tentative(s)")
            
            return path, encode_image_base64(cropped), cropped
            
        except Exception as e:
            last_error = e
            if DEBUG_MODE:
                print(f"‚ö†Ô∏è √âchec de capture du panneau (tentative {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)  # Pause avant nouvelle tentative
                continue
    
    raise RuntimeError(f"Impossible de capturer le panneau apr√®s {max_retries} tentatives") from last_error

@timing_decorator
def screenshot_opening_hours(driver, prefix, max_retries=3):
    """
    Capture les horaires d'ouverture directement depuis la fiche Google Maps sans faire d√©filer
    
    Args:
        driver: WebDriver Selenium
        prefix: Pr√©fixe pour le nom du fichier
        max_retries: Nombre maximum de tentatives
    
    Returns:
        Tuple (chemin de l'image, version base64, objet image) ou (None, None, None)
    """
    last_error = None
    for attempt in range(max_retries):
        try:
            if attempt > 0 and DEBUG_MODE:
                print(f"  ‚Ü≥ Tentative {attempt + 1}/{max_retries} de capture des horaires")
            
            # Rechercher le bouton des horaires
            horaires_btn = None
            horaire_xpaths = [
                "//button[.//span[contains(@aria-label, 'Ore') or contains(@aria-label, 'Heures') or contains(@aria-label, 'Hours')]]",
                "//button[contains(@aria-label, 'Horaires')]",
                "//div[contains(@role, 'button')][.//span[contains(text(), 'horaires')]]",
                "//div[contains(@aria-label, 'Informations')]//div[contains(text(), 'Ferm√©') or contains(text(), 'Ouvert')]"
            ]
            
            for xpath in horaire_xpaths:
                try:
                    horaires_btn = WebDriverWait(driver, 3).until(
                        EC.element_to_be_clickable((By.XPATH, xpath))
                    )
                    if horaires_btn:
                        break
                except:
                    continue
            
            if not horaires_btn:
                # Plan B : capturer la section des horaires directement depuis le panneau lat√©ral
                print("  ‚Ü≥ Capture directe des horaires depuis le panneau principal")
                
                # Capture d'√©cran compl√®te
                screenshot = driver.get_screenshot_as_png()
                image = Image.open(BytesIO(screenshot))
                
                # Essayer de trouver la zone des horaires approximativement
                # Coordonn√©es typiques de la section des horaires
                horaires_crop = image.crop((600, 200, 1200, 800))
                
                path = f"{prefix}_horaires_direct.png"
                horaires_crop.save(path)
                
                if DEBUG_MODE:
                    print(f"  ‚Ü≥ Capture directe des horaires effectu√©e")
                
                return path, encode_image_base64(horaires_crop), horaires_crop
            
            # Faire d√©filer jusqu'au bouton si n√©cessaire
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", horaires_btn)
            time.sleep(1)
            
            # Prendre une capture d'√©cran de la section avant de cliquer
            pre_click = driver.get_screenshot_as_png()
            pre_image = Image.open(BytesIO(pre_click))
            
            # Obtenir la position du bouton
            location = horaires_btn.location
            size = horaires_btn.size
            
            # √âtendre la zone de capture pour inclure la liste des horaires 
            # qui appara√Æt souvent directement sous le bouton
            horaires_section = pre_image.crop((
                location['x'] - 50,  
                location['y'] - 20,
                location['x'] + size['width'] + 300,  # Capturer une zone plus large
                location['y'] + size['height'] + 300   # Capturer vers le bas pour les horaires
            ))
            
            # Sauvegarder cette premi√®re version
            path = f"{prefix}_horaires_section.png"
            horaires_section.save(path)
            
            # Maintenant cliquer pour voir s'il y a un popup
            try:
                horaires_btn.click()
                time.sleep(2)  # Attendre l'ouverture du popup
                
                # V√©rifier si un popup est apparu
                dialog_present = False
                try:
                    dialog = WebDriverWait(driver, 3).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div[role="dialog"]'))
                    )
                    dialog_present = True
                except:
                    dialog_present = False
                
                if dialog_present:
                    # Capturer le popup des horaires
                    screenshot = driver.get_screenshot_as_png()
                    image = Image.open(BytesIO(screenshot))
                    
                    # Coordonn√©es typiques du popup des horaires
                    horaires_popup = image.crop((500, 100, 1100, 800))
                    
                    path = f"{prefix}_horaires_popup.png"
                    horaires_popup.save(path)
                    
                    # Fermer le popup
                    try:
                        driver.find_element(By.CSS_SELECTOR, 'button[aria-label="Fermer"]').click()
                    except:
                        pass
                    
                    return path, encode_image_base64(horaires_popup), horaires_popup
            except:
                # Si le clic √©choue, on utilise d√©j√† la capture de la section
                pass
            
            # Si on arrive ici, on renvoie la section captur√©e initialement
            if DEBUG_MODE and attempt > 0:
                print(f"  ‚Ü≥ Capture des horaires r√©ussie apr√®s {attempt + 1} tentative(s)")
            
            return path, encode_image_base64(horaires_section), horaires_section
            
        except Exception as e:
            last_error = e
            if DEBUG_MODE:
                print(f"‚ö†Ô∏è √âchec de capture des horaires (tentative {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)  # Pause avant nouvelle tentative
                try:
                    # Fermer le popup s'il est rest√© ouvert
                    driver.find_element(By.CSS_SELECTOR, 'button[aria-label="Fermer"]').click()
                except:
                    pass
                continue
    
    if DEBUG_MODE:
        print(f"‚ùå Impossible de capturer les horaires apr√®s {max_retries} tentatives: {last_error}")
    return None, None, None

def encode_image_base64(image):
    """Convertit une image PIL en base64 pour stockage"""
    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
    # Ajouter le pr√©fixe pour cr√©er une URL data compl√®te
    return f"data:image/jpeg;base64,{img_base64}"

@timing_decorator
def extract_text_from_image(image, max_retries=2):
    """
    Extrait le texte d'une image avec OCR am√©lior√© et gestion des erreurs
    
    Args:
        image: Image PIL √† traiter
        max_retries: Nombre maximum de tentatives
    
    Returns:
        Texte extrait ou cha√Æne vide en cas d'√©chec
    """
    last_error = None
    for attempt in range(max_retries):
        try:
            if attempt > 0 and DEBUG_MODE:
                print(f"  ‚Ü≥ Tentative {attempt + 1}/{max_retries} d'extraction OCR")
            
            # Am√©liorer la qualit√© de l'image pour l'OCR
            enhanced = image.convert('L')  # Conversion en niveaux de gris
            enhanced = enhanced.point(lambda x: 0 if x < 128 else 255, '1')  # Binarisation
            
            # Configuration OCR optimis√©e
            custom_config = r'--oem 3 --psm 6 -l fra'
            text = pytesseract.image_to_string(enhanced, config=custom_config)
            text = text.strip()
            
            # V√©rifier que le texte n'est pas vide ou trop court
            if not text or len(text) < 10:
                if attempt < max_retries - 1:
                    raise ValueError("Texte extrait trop court ou vide")
            
            if DEBUG_MODE and attempt > 0:
                print(f"  ‚Ü≥ Extraction OCR r√©ussie apr√®s {attempt + 1} tentative(s)")
            
            return text
            
        except Exception as e:
            last_error = e
            if DEBUG_MODE:
                print(f"‚ö†Ô∏è √âchec d'extraction OCR (tentative {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(1)  # Pause avant nouvelle tentative
                continue
    
    print(f"‚ùå OCR erreur apr√®s {max_retries} tentatives: {last_error}")
    return ""

def parse_opening_hours_text(text):
    """Parse le texte des horaires extrait par OCR"""
    horaires = {}
    for line in text.splitlines():
        if re.match(r"^(luni|mar»õi|miercuri|joi|vineri|s√¢mbƒÉtƒÉ|duminicƒÉ|lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche)", line.strip().lower()):
            parts = line.split(" ")
            day = parts[0].lower()
            horaires[day] = " ".join(parts[1:])
    return horaires

@timing_decorator
def call_openai_structured_extraction(ocr_text):
    """
    Utilise OpenAI pour structurer les donn√©es extraites par OCR
    
    Args:
        ocr_text: Texte brut extrait par OCR
    
    Returns:
        Dictionnaire structur√© des informations
    """
    prompt = f"""
Voici un texte brut issu d'un screenshot Google Maps en fran√ßais :

{ocr_text}

Extrais les informations suivantes dans un dictionnaire JSON (valeurs vides si non disponibles) :
- address
- phone_number
- website
- price_level
- rating
- user_ratings_total
- service_options (ex: dine_in, takeaway, delivery)
- category (ex: 'Restaurant chinois')
    """
    
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        content = response.choices[0].message.content.strip()
        return json.loads(content)
    except Exception as e:
        print(f"‚ùå Erreur LLM : {e}")
        return {}

def scroll_to_bottom_info(driver):
    """Fait d√©filer le panneau lat√©ral pour voir toutes les informations"""
    try:
        driver.execute_script("""
            const scrollArea = document.querySelector('div[role="main"]');
            if (scrollArea) scrollArea.scrollTop = scrollArea.scrollHeight;
        """)
        time.sleep(2)
        print("üìú Scroll JS effectu√©")
    except Exception as e:
        print("‚ö†Ô∏è Scroll √©chou√© :", e)

@timing_decorator
def process_restaurant_with_maps_screenshots(maps_data):
    """
    Traite les donn√©es d'un restaurant et capture des screenshots
    
    Args:
        maps_data: Donn√©es du restaurant depuis Google Maps
    
    Returns:
        Donn√©es du restaurant enrichies
    """
    if not maps_data:
        return None
        
    # Extraire les informations de base
    name = maps_data.get("name")
    address = maps_data.get("address")
    maps_url = maps_data.get("maps_url")
    place_id = maps_data.get("place_id")
    latitude = maps_data.get("latitude", 0)
    longitude = maps_data.get("longitude", 0)
    rating = maps_data.get("rating", 0)
    
    # Cr√©er la structure de donn√©es initiale
    restaurant_data = {
        "name": name,
        "place_id": place_id,
        "address": address,
        "maps_url": maps_url,
        "location": {
            "type": "Point",
            "coordinates": [longitude, latitude]
        },
        "rating": rating,
        "image": "",
        "images": [],
        "phone": "",
        "website": "",
        "categories": [],
        "opening_hours": {},
        "price_level": 0,
        "created_at": datetime.now(),
        "updated_at": datetime.now(),
    }
    
    # Essayer de capturer les screenshots et extraire des donn√©es additionnelles
    try:
        print(f"üì∏ Capture des screenshots de Maps pour {name}")
        
        with ChromeSessionManager() as session:
            driver = session.driver
            
            if not driver:
                print(f"‚ö†Ô∏è √âchec d'initialisation du driver Chrome pour {name}")
                return restaurant_data  # Retourner les donn√©es de base sans screenshots
                
            # Essayer de capturer l'√©tat du restaurant
            try:
                driver.get(maps_url)
                time.sleep(3)
                
                # Capturer l'image principale du restaurant
                temp_name = f"temp_{hashlib.md5(name.encode()).hexdigest()[:8]}"
                try:
                    photo_path, photo_base64, _ = screenshot_photo(driver, temp_name)
                    restaurant_data["image"] = photo_base64
                    print(f"‚úÖ Screenshot captur√© pour {name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Impossible de capturer la photo de {name}: {str(e)}")
                
                # Extraire des informations additionnelles (site web, t√©l√©phone, horaires...)
                additional_info = extract_additional_info(driver, maps_url, restaurant_data)
                if additional_info:
                    # Mise √† jour des donn√©es avec les infos extraites
                    for key, value in additional_info.items():
                        if value:  # Ne mettre √† jour que si la valeur n'est pas vide
                            restaurant_data[key] = value
                
                # Si les horaires sont extraits mais pas dans le bon format, essayer de les parser
                if "opening_hours_text" in restaurant_data and not restaurant_data.get("opening_hours"):
                    hours_text = restaurant_data.get("opening_hours_text", "")
                    if hours_text:
                        try:
                            parsed_hours = parse_opening_hours_text(hours_text)
                            if parsed_hours:
                                restaurant_data["opening_hours"] = parsed_hours
                        except:
                            pass
                
                # S'assurer que certaines informations essentielles sont pr√©sentes, m√™me si vides
                for key in ["phone", "website", "categories", "opening_hours", "price_level"]:
                    if key not in restaurant_data or restaurant_data[key] is None:
                        if key == "opening_hours":
                            restaurant_data[key] = {}
                        elif key == "categories":
                            restaurant_data[key] = []
                        elif key == "price_level":
                            restaurant_data[key] = 0
                        else:
                            restaurant_data[key] = ""
                
                # Afficher un r√©sum√© des donn√©es extraites
                print(f"üì± T√©l√©phone: {restaurant_data.get('phone', 'Non trouv√©')}")
                print(f"üåê Site web: {restaurant_data.get('website', 'Non trouv√©')}")
                print(f"‚è∞ Horaires: {len(restaurant_data.get('opening_hours', {}))} jours")
                print(f"‚≠ê Note: {restaurant_data.get('rating', 0)}")
                print(f"üí∞ Prix: {restaurant_data.get('price_level', 'Non trouv√©')}")
                print(f"üè∑Ô∏è Cat√©gories: {restaurant_data.get('categories', [])}")
                
                return restaurant_data
                
            except Exception as e:
                print(f"‚ùå Erreur lors du traitement des donn√©es visuelles: {str(e)}")
                traceback.print_exc()
                # M√™me en cas d'erreur, retourner les donn√©es de base
                return restaurant_data
    
    except Exception as e:
        print(f"‚ùå Erreur lors du processus de capture d'√©cran: {str(e)}")
        traceback.print_exc()
        
    # M√™me en cas d'√©chec total, retourner les donn√©es de base
    return restaurant_data

def capture_maps_screenshot(driver, url):
    """
    Capture un screenshot de Google Maps
    
    Args:
        driver: Instance de webdriver (d√©j√† initialis√©e)
        url: URL de Google Maps √† capturer
    
    Returns:
        Base64 du screenshot
    """
    try:
        # V√©rifier que le driver est bien initialis√©
        if not driver:
            print("‚ùå Driver non initialis√© pour la capture d'√©cran")
            return None
            
        # Charger l'URL si elle n'est pas d√©j√† charg√©e
        current_url = driver.current_url
        if url != current_url:
            driver.get(url)
            # Attendre que la page se charge
            time.sleep(5)
            
            # G√©rer le consentement aux cookies si n√©cessaire
            try:
                consent_buttons = driver.find_elements(By.XPATH, 
                    "//button[contains(., 'Accept') or contains(., 'Accepter') or contains(., 'Reject') or contains(., 'Refuser')]")
                if consent_buttons:
                    for button in consent_buttons:
                        if button.is_displayed():
                            button.click()
                            time.sleep(2)
                            break
            except:
                pass
        
        # Prendre le screenshot de la page
        screenshot = driver.get_screenshot_as_base64()
        return f"data:image/png;base64,{screenshot}"
        
    except Exception as e:
        print(f"‚ùå Erreur capture screenshot: {str(e)}")
        return None

def extract_additional_info(driver, maps_url, restaurant_data):
    """
    Extrait des informations suppl√©mentaires sur un restaurant depuis Google Maps
    
    Args:
        driver: WebDriver Selenium (d√©j√† initialis√©)
        maps_url: URL Google Maps
        restaurant_data: Donn√©es existantes du restaurant
    
    Returns:
        Dictionnaire avec les informations extraites
    """
    # V√©rifier que le driver est correctement initialis√©
    if not driver:
        print("‚ùå Driver non initialis√© pour l'extraction d'informations additionnelles")
        return {}
    
    # S'assurer que nous sommes sur la bonne URL
    if driver.current_url != maps_url:
        try:
            driver.get(maps_url)
            time.sleep(4)
        except Exception as e:
            print(f"‚ùå Erreur lors de la navigation vers {maps_url}: {str(e)}")
            return {}
    
    result = {}
    
    try:
        # Extraire les informations une par une
        
        # 1. Site web
        try:
            # Diff√©rents s√©lecteurs pour trouver le site web
            website_selectors = [
                "a[data-item-id='authority']",
                "a[aria-label*='site web']",
                "a[aria-label*='website']",
                "a[data-tooltip='Ouvrir le site Web']",
                "a[data-tooltip='Open website']",
                "div[aria-label*='Site web'] a",
                "button[aria-label*='site web'] ~ a",
                "button[data-item-id='authority']",
                "a[href^='http']:not([href*='google'])",
                "a.website",
                "a[href^='http'][data-item-id]"
            ]
            
            for selector in website_selectors:
                website_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for el in website_elements:
                    website_url = el.get_attribute("href")
                    if website_url and not website_url.startswith("https://www.google.com"):
                        result["website"] = website_url
                        break
                if "website" in result:
                    break
                    
        except Exception as e:
            if DEBUG_MODE:
                print(f"‚ö†Ô∏è Erreur lors de l'extraction du site web: {str(e)}")
        
        # 2. Num√©ro de t√©l√©phone
        try:
            # Diff√©rents s√©lecteurs pour trouver le num√©ro de t√©l√©phone
            phone_selectors = [
                "button[data-tooltip='Copier le num√©ro de t√©l√©phone']",
                "button[data-tooltip='Copy phone number']",
                "button[aria-label*='phone']",
                "button[aria-label*='t√©l√©phone']",
                "div[aria-label*='t√©l√©phone'] button",
                "button[data-item-id='phone:tel']",
                "button.phone",
                "a[href^='tel:']",
                "span[aria-label*='phone']"
            ]
            
            for selector in phone_selectors:
                phone_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for el in phone_elements:
                    # Essayer plusieurs attributs
                    phone_text = None
                    for attr in ["aria-label", "data-item-id", "data-tooltip", "title"]:
                        phone_text = el.get_attribute(attr)
                        if phone_text and (re.search(r'\d', phone_text) or "phone" in phone_text.lower() or "t√©l√©phone" in phone_text.lower()):
                            # Extraire seulement les chiffres et les caract√®res de formatage
                            phone_match = re.search(r'(?:\+\d{1,3}[-.\s]?)?(?:\(\d{1,4}\)[-.\s]?)?\d{1,4}[-.\s]?\d{1,4}[-.\s]?\d{1,9}', phone_text)
                            if phone_match:
                                result["phone"] = phone_match.group(0).strip()
                                break
                                
                    # Si on n'a pas trouv√©, essayer le texte de l'√©l√©ment
                    if not result.get("phone"):
                        phone_text = el.text
                        if phone_text and re.search(r'\d', phone_text):
                            # Extraire seulement les chiffres et les caract√®res de formatage
                            phone_match = re.search(r'(?:\+\d{1,3}[-.\s]?)?(?:\(\d{1,4}\)[-.\s]?)?\d{1,4}[-.\s]?\d{1,4}[-.\s]?\d{1,9}', phone_text)
                            if phone_match:
                                result["phone"] = phone_match.group(0).strip()
                                break
                
                if "phone" in result:
                    break
                    
            # Si on n'a toujours pas trouv√©, chercher dans la page enti√®re
            if not result.get("phone"):
                page_source = driver.page_source
                phone_matches = re.findall(r'(?:\+\d{1,3}[-.\s]?)?(?:\(\d{1,4}\)[-.\s]?)?\d{1,4}[-.\s]?\d{1,4}[-.\s]?\d{1,9}', page_source)
                if phone_matches:
                    for match in phone_matches:
                        # V√©rifier que c'est bien un num√©ro de t√©l√©phone (au moins 8 chiffres)
                        digit_count = sum(c.isdigit() for c in match)
                        if digit_count >= 8 and digit_count <= 15:
                            result["phone"] = match
                            break
            
        except Exception as e:
            if DEBUG_MODE:
                print(f"‚ö†Ô∏è Erreur lors de l'extraction du num√©ro de t√©l√©phone: {str(e)}")
        
        # 3. Cat√©gories
        try:
            category_elements = driver.find_elements(By.CSS_SELECTOR, 
                "button[jsaction*='category'], span.widget-pane-link, span[jsan*='category'], button[aria-label*='restaurant'], button[jsaction*='restaurant'], span.category")
            
            categories = []
            for el in category_elements:
                category_text = el.text.strip()
                if category_text and len(category_text) > 2 and not re.search(r'^\d', category_text):
                    # √âviter les valeurs non pertinentes
                    if not category_text.startswith(("http", "www", "+", "Ouvrir", "Fermer", "Ouvert", "Ferm√©")):
                        categories.append(category_text)
            
            # Si pas de cat√©gorie trouv√©e, essayer avec une autre approche
            if not categories:
                # Chercher directement dans le titre/sous-titre du restaurant
                title_elements = driver.find_elements(By.CSS_SELECTOR, ".section-hero-header-title-description")
                for el in title_elements:
                    subtitle = el.find_elements(By.CSS_SELECTOR, "div:not(.section-hero-header-title)")
                    if subtitle:
                        subtitle_text = subtitle[0].text.strip()
                        if subtitle_text and "¬∑" in subtitle_text:
                            # Les cat√©gories sont souvent s√©par√©es par des points m√©dians
                            parts = subtitle_text.split("¬∑")
                            for part in parts:
                                clean_part = part.strip()
                                if clean_part and len(clean_part) > 2:
                                    categories.append(clean_part)
            
            # Conversion pour compatibilit√© avec le reste du code
            if categories:
                # Filtrer les doublons et les valeurs vides
                categories = list(set(filter(None, categories)))
                if not restaurant_data.get("categories"):
                    # Si aucune cat√©gorie n'existe, utiliser celles trouv√©es
                    result["categories"] = categories
                else:
                    # Sinon, fusionner avec les cat√©gories existantes
                    existing_categories = restaurant_data.get("categories", [])
                    result["categories"] = list(set(existing_categories + categories))
            
        except Exception as e:
            if DEBUG_MODE:
                print(f"‚ö†Ô∏è Erreur lors de l'extraction des cat√©gories: {str(e)}")
        
        # 4. Horaires d'ouverture
        try:
            # Diff√©rentes approches pour extraire les horaires
            
            # Approche 1: Chercher le bouton des horaires et cliquer dessus
            hours_button = None
            hours_selectors = [
                "button[data-item-id='oh']",
                "button[aria-label*='horaires']",
                "button[aria-label*='hours']",
                "div[aria-label*='horaires']",
                "button[jsaction*='open'][jsaction*='hours']",
                "div.section-info-hour-text"
            ]
            
            for selector in hours_selectors:
                hour_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if hour_elements:
                    hours_button = hour_elements[0]
                    break
            
            opening_hours = {}
            
            if hours_button:
                # Essayer de cliquer pour ouvrir les horaires d√©taill√©s
                try:
                    hours_button.click()
                    time.sleep(1)
                    
                    # Chercher les horaires dans la popup ou dans le panel
                    hours_rows = driver.find_elements(By.CSS_SELECTOR, "table tr, div.section-info-hour-row")
                    
                    if hours_rows:
                        for row in hours_rows:
                            row_text = row.text.strip()
                            # Analyser le texte pour extraire le jour et les heures
                            day_match = re.search(r'(lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche|monday|tuesday|wednesday|thursday|friday|saturday|sunday)', row_text.lower())
                            if day_match:
                                day = day_match.group(1)
                                # Normaliser le jour en fran√ßais
                                day_map = {
                                    "monday": "lundi", "tuesday": "mardi", "wednesday": "mercredi", 
                                    "thursday": "jeudi", "friday": "vendredi", "saturday": "samedi", "sunday": "dimanche"
                                }
                                day = day_map.get(day, day)
                                
                                # Extraire les heures (format: 9:00‚Äì22:00 ou 9h00-22h00)
                                hours_match = re.search(r'(\d{1,2})[h:](\d{2})\s*[-‚Äì]\s*(\d{1,2})[h:](\d{2})', row_text)
                                if hours_match:
                                    open_hour = int(hours_match.group(1))
                                    open_minute = int(hours_match.group(2))
                                    close_hour = int(hours_match.group(3))
                                    close_minute = int(hours_match.group(4))
                                    
                                    opening_hours[day] = {
                                        "open": f"{open_hour:02d}:{open_minute:02d}",
                                        "close": f"{close_hour:02d}:{close_minute:02d}"
                                    }
                except Exception as e:
                    if DEBUG_MODE:
                        print(f"‚ö†Ô∏è Erreur lors du clic sur le bouton des horaires: {str(e)}")
            
            # Si pas d'horaires trouv√©s, essayer d'extraire directement du texte
            if not opening_hours:
                page_source = driver.page_source
                # Rechercher des patterns d'horaires dans la source de la page
                hours_pattern = r'(lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche|monday|tuesday|wednesday|thursday|friday|saturday|sunday)[^\n]*?(\d{1,2})[h:](\d{2})\s*[-‚Äì]\s*(\d{1,2})[h:](\d{2})'
                hours_matches = re.findall(hours_pattern, page_source, re.IGNORECASE)
                
                if hours_matches:
                    for match in hours_matches:
                        day = match[0].lower()
                        # Normaliser le jour en fran√ßais
                        day_map = {
                            "monday": "lundi", "tuesday": "mardi", "wednesday": "mercredi", 
                            "thursday": "jeudi", "friday": "vendredi", "saturday": "samedi", "sunday": "dimanche"
                        }
                        day = day_map.get(day, day)
                        
                        open_hour = int(match[1])
                        open_minute = int(match[2])
                        close_hour = int(match[3])
                        close_minute = int(match[4])
                        
                        opening_hours[day] = {
                            "open": f"{open_hour:02d}:{open_minute:02d}",
                            "close": f"{close_hour:02d}:{close_minute:02d}"
                        }
            
            if opening_hours:
                result["opening_hours"] = opening_hours
            
        except Exception as e:
            if DEBUG_MODE:
                print(f"‚ö†Ô∏è Erreur lors de l'extraction des horaires: {str(e)}")
        
        # 5. Prix (‚Ç¨, ‚Ç¨‚Ç¨, ‚Ç¨‚Ç¨‚Ç¨)
        try:
            price_selectors = [
                "span[aria-label*='Prix'], span[aria-label*='Price']", 
                "span.price-level", 
                "span.section-rating-term span"
            ]
            
            for selector in price_selectors:
                price_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for el in price_elements:
                    price_text = el.text.strip()
                    # Compter le nombre de symboles ‚Ç¨ ou $
                    if '‚Ç¨' in price_text:
                        result["price_level"] = price_text.count('‚Ç¨')
                        break
                    elif '$' in price_text:
                        result["price_level"] = price_text.count('$')
                        break
                if "price_level" in result:
                    break
            
            # Si prix non trouv√©, chercher dans les attributs et la page source
            if "price_level" not in result:
                price_hints = {
                    "peu co√ªteux": 1, "bon march√©": 1, "abordable": 1, "√©conomique": 1, 
                    "mod√©r√©": 2, "moyen": 2, "mid-range": 2, 
                    "haut de gamme": 3, "cher": 3, "co√ªteux": 3, "luxe": 4
                }
                
                # Chercher des indices de prix dans la page
                page_text = driver.page_source.lower()
                for hint, level in price_hints.items():
                    if hint in page_text:
                        result["price_level"] = level
                        break
                        
        except Exception as e:
            if DEBUG_MODE:
                print(f"‚ö†Ô∏è Erreur lors de l'extraction du niveau de prix: {str(e)}")
                
        # 6. Note/Rating (v√©rification/mise √† jour)
        try:
            if not restaurant_data.get("rating") or restaurant_data.get("rating") == 0:
                rating_selectors = [
                    "span.section-star-display", 
                    "span.rating", 
                    "span[aria-label*='√©toile']", 
                    "span[aria-label*='star']",
                    "div.gm2-display-2"
                ]
                
                for selector in rating_selectors:
                    rating_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for el in rating_elements:
                        rating_text = el.text.strip()
                        if rating_text:
                            # Chercher un chiffre avec une d√©cimale potentielle
                            rating_match = re.search(r'([\d.]+)', rating_text)
                            if rating_match:
                                try:
                                    rating_value = float(rating_match.group(1))
                                    if 0 < rating_value <= 5:
                                        result["rating"] = rating_value
                                        break
                                except:
                                    pass
                    if "rating" in result:
                        break
        except Exception as e:
            if DEBUG_MODE:
                print(f"‚ö†Ô∏è Erreur lors de l'extraction de la note: {str(e)}")
        
        return result
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'extraction des informations additionnelles: {str(e)}")
        traceback.print_exc()
        return {}

def search_links_bing(name, address):
    """
    Recherche les liens des plateformes via Bing avec mise en cache
    
    Args:
        name: Nom du restaurant
        address: Adresse du restaurant
    
    Returns:
        Dictionnaire avec les liens trouv√©s (cl√©s: nom de plateforme, valeurs: URLs)
    """
    # Utiliser une cl√© de cache bas√©e sur le nom et l'adresse
    cache_key = f"{name}_{address}"
    
    # V√©rifier si les r√©sultats sont d√©j√† en cache
    if cache_key in BING_SEARCH_CACHE:
        print(f"‚úÖ R√©sultats Bing r√©cup√©r√©s depuis le cache pour {name}")
        return BING_SEARCH_CACHE[cache_key]
    
    query = f"{name} {address} restaurant tripadvisor lafourchette"
    print(f"üîç Recherche sur Bing: {query}")
    
    # Construire l'URL Bing avec le param√®tre cc=FR
    encoded_query = urllib.parse.quote_plus(query)
    bing_url = f"https://www.bing.com/search?q={encoded_query}&cc=FR"
    print(f"üåê URL Bing: {bing_url}")
    
    # Utiliser BrightData pour r√©cup√©rer le HTML
    html = fetch_html_with_brightdata(bing_url, name, "bing_search")
    if not html:
        print(f"‚ùå Impossible de r√©cup√©rer le HTML de Bing pour {name}")
        return {}
        
    # Parser le HTML
    soup = BeautifulSoup(html, 'html.parser')
    
    # Dictionnaire pour stocker les liens trouv√©s
    platform_links = {}
    
    # Liste des plateformes √† rechercher avec leurs patterns regex
    platforms = [
        ("tripadvisor", r'tripadvisor\.(?:fr|com)/Restaurant_Review'),
        ("lafourchette", r'lafourchette\.fr/restaurant'),
        ("facebook", r'facebook\.com'),
        ("instagram", r'instagram\.com')
    ]
    
    # Rechercher les liens pour chaque plateforme
    for platform_name, pattern in platforms:
        links = soup.find_all('a', href=re.compile(pattern))
        for link in links:
            url = link.get('href')
            if validate_platform_link(url, platform_name):
                platform_links[platform_name] = url
                print(f"  ‚úÖ Trouv√© lien {platform_name}: {url}")
                break
    
    print(f"‚úÖ Trouv√© {len(platform_links)} liens de plateformes")
    
    # Enregistrer dans le cache
    BING_SEARCH_CACHE[cache_key] = platform_links
    
    return platform_links

def load_test_restaurants():
    """
    Charge un jeu de donn√©es de test pour les restaurants de Montmartre
    """
    print("üîç Utilisation d'une petite zone de test (Montmartre)")
    
    test_restaurants = [
        {"name": "Boulangerie Chaptal", "address": "2 Rue Chaptal, Paris"},
        {"name": "Les Ap√¥tres de Pigalle", "address": "2 Rue Germain Pilon, Paris"},
        {"name": "Le Chaptal", "address": "50 Rue Jean-Baptiste Pigalle, Paris"},
        {"name": "Puce", "address": "1 Rue Chaptal, Paris"},
        {"name": "Le Pantruche", "address": "3 Rue Victor Mass√©, Paris"},
        {"name": "Bouillon Pigalle", "address": "22 Boulevard de Clichy, Paris"},
        {"name": "H√¥tel Amour", "address": "8 Rue de Navarin, Paris"}
    ]
    
    print(f"‚úÖ {len(test_restaurants)} restaurants charg√©s dans la zone de test")
    return test_restaurants

def load_restaurants_from_file():
    """
    Charge les restaurants depuis un fichier JSON
    """
    try:
        restaurant_file = os.path.join(CURRENT_DIR, "data", "restaurants.json")
        if os.path.exists(restaurant_file):
            with open(restaurant_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            print(f"‚ùå Fichier de restaurants introuvable: {restaurant_file}")
            return load_test_restaurants()  # Fallback sur les donn√©es de test
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des restaurants: {str(e)}")
        return load_test_restaurants()  # Fallback sur les donn√©es de test

def cleanup_temp_dirs():
    """
    Nettoie les r√©pertoires temporaires √† la fin de l'ex√©cution
    """
    # Utiliser une variable statique dans la fonction
    if not hasattr(cleanup_temp_dirs, "temp_dirs"):
        cleanup_temp_dirs.temp_dirs = []
    
    if not cleanup_temp_dirs.temp_dirs:
        print("‚úÖ Aucun r√©pertoire temporaire √† nettoyer")
        return
        
    print(f"üßπ Nettoyage de {len(cleanup_temp_dirs.temp_dirs)} r√©pertoires temporaires...")
    for temp_dir in cleanup_temp_dirs.temp_dirs:
        try:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du nettoyage de {temp_dir}: {str(e)}")
            
    cleanup_temp_dirs.temp_dirs = []

def check_mongodb_content():
    """
    V√©rifie le contenu de la collection MongoDB
    """
    try:
        client = get_mongo_client()
        if not client:
            print("‚ùå Impossible de se connecter √† MongoDB")
            return
            
        db = client[DB_NAME]
        collection = db[COLLECTION_NAME]
        
        print(f"\nüîç V√©rification du contenu MongoDB:")
        print(f"  Base de donn√©es: {DB_NAME}")
        print(f"  Collection: {COLLECTION_NAME}")
        
        # Compter le nombre total de documents
        total_docs = collection.count_documents({})
        print(f"  Nombre total de documents: {total_docs}")
        
        # Afficher les 5 premiers documents
        print("\nüìã 5 premiers documents:")
        for doc in collection.find().limit(5):
            print(f"  - {doc.get('name', 'Sans nom')} (ID: {doc.get('_id')})")
            
        client.close()
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification MongoDB: {str(e)}")
        traceback.print_exc()

def validate_platform_link(url, platform):
    """
    Valide un lien de plateforme
    """
    if not url:
        return False
        
    # V√©rifier le domaine
    try:
        domain = urlparse(url).netloc.lower()
        if platform == "tripadvisor":
            return "tripadvisor" in domain and ".fr" in domain
        elif platform == "lafourchette":
            return "lafourchette" in domain and ".fr" in domain
        elif platform == "facebook":
            return "facebook.com" in domain
        return False
    except:
        return False

def enrich_with_platforms(restaurant_data):
    """
    Enrichit les donn√©es d'un restaurant avec les liens des plateformes externes
    """
    try:
        print(f"[{restaurant_data['name']}] üåê Enrichissement avec les plateformes externes...")
        
        # V√©rifier si BrightData est activ√©
        if not BRIGHTDATA_ENABLED:
            print("‚ùå BrightData n'est pas activ√©, impossible de rechercher les liens")
            return restaurant_data
            
        # V√©rifier le token BrightData
        if not BRIGHTDATA_TOKEN:
            print("‚ùå Token BrightData manquant")
            return restaurant_data
            
        # Rechercher les liens sur Bing
        query = f"{restaurant_data['name']} {restaurant_data['address']} restaurant tripadvisor lafourchette"
        print(f"üîç Recherche sur Bing: {query}")
        
        # Construire l'URL Bing avec le param√®tre cc=FR
        encoded_query = urllib.parse.quote_plus(query)
        bing_url = f"https://www.bing.com/search?q={encoded_query}&cc=FR"
        print(f"üåê URL Bing: {bing_url}")
        
        # Utiliser BrightData pour r√©cup√©rer le HTML
        html = fetch_html_with_brightdata(bing_url)
        if not html:
            print("‚ùå Impossible de r√©cup√©rer le HTML de Bing")
            return restaurant_data
            
        # Parser le HTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # Dictionnaire pour stocker les liens trouv√©s
        platform_links = {}
        
        # Rechercher les liens TripAdvisor
        tripadvisor_links = soup.find_all('a', href=re.compile(r'tripadvisor\.fr/Restaurant_Review'))
        for link in tripadvisor_links:
            url = link.get('href')
            if validate_platform_link(url, "tripadvisor"):
                platform_links["tripadvisor"] = url
                print(f"‚úÖ Trouv√© lien tripadvisor: {url}")
                break
                
        # Rechercher les liens LaFourchette
        lafourchette_links = soup.find_all('a', href=re.compile(r'lafourchette\.fr/restaurant'))
        for link in lafourchette_links:
            url = link.get('href')
            if validate_platform_link(url, "lafourchette"):
                platform_links["lafourchette"] = url
                print(f"‚úÖ Trouv√© lien lafourchette: {url}")
                break
                
        # Rechercher les liens Facebook
        facebook_links = soup.find_all('a', href=re.compile(r'facebook\.com'))
        for link in facebook_links:
            url = link.get('href')
            if validate_platform_link(url, "facebook"):
                platform_links["facebook"] = url
                print(f"‚úÖ Trouv√© lien facebook: {url}")
                break
                
        print(f"‚úÖ Trouv√© {len(platform_links)} liens de plateformes")
        
        # Mettre √† jour les donn√©es du restaurant
        restaurant_data.update(platform_links)
        return restaurant_data
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'enrichissement: {str(e)}")
        return restaurant_data

def normalize_restaurant_data(restaurant_data):
    """
    Normalise les donn√©es du restaurant pour MongoDB
    - S'assure que le place_id est utilis√© comme _id
    - V√©rifie que toutes les donn√©es sont au bon format
    - Formate les coordonn√©es GPS en GeoJSON
    
    Args:
        restaurant_data: Dictionnaire des donn√©es du restaurant
    
    Returns:
        Dictionnaire normalis√© compatible avec MongoDB
    """
    # V√©rifier si le place_id existe et n'est pas vide
    place_id = restaurant_data.get('place_id')
    if not place_id or place_id == "":
        # G√©n√©rer un ID unique bas√© sur le nom et l'adresse
        custom_id = f"{restaurant_data.get('name', 'unknown')}_{restaurant_data.get('address', 'unknown')}"
        # Ajouter un UUID pour √©viter les doublons
        import uuid
        place_id = f"custom_{uuid.uuid4().hex[:10]}_{custom_id.replace(' ', '_')[:30]}"
        print(f"‚ö†Ô∏è place_id manquant, g√©n√©r√©: {place_id}")
    
    # R√©cup√©rer les coordonn√©es GPS
    lat = restaurant_data.get('latitude', 0)
    lng = restaurant_data.get('longitude', 0)
    
    # S'assurer que les coordonn√©es sont des nombres flottants (pas des cha√Ænes)
    try:
        lat = float(lat) if lat else 0
        lng = float(lng) if lng else 0
    except (ValueError, TypeError):
        lat, lng = 0, 0
    
    # Cr√©er un objet GeoJSON pour les coordonn√©es (format attendu par MongoDB)
    gps_coordinates = {
        "type": "Point",
        "coordinates": [lng, lat]  # Important: MongoDB utilise [longitude, latitude]
    }
    
    # Normaliser les donn√©es
    normalized_data = {
        "_id": place_id,
        "place_id": place_id,
        "name": restaurant_data.get('name', ''),
        "verified": restaurant_data.get('verified', False),
        "photo": restaurant_data.get('photo', ''),
        "description": restaurant_data.get('description', ''),
        "address": restaurant_data.get('address', ''),
        "gps_coordinates": gps_coordinates,
        "category": restaurant_data.get('category', []),
        "opening_hours": restaurant_data.get('opening_hours', []),
        "phone_number": restaurant_data.get('phone_number', ''),
        "website": restaurant_data.get('website', ''),
        "photos": restaurant_data.get('photos', []),
        "business_status": restaurant_data.get('business_status', 'OPERATIONAL'),
        "international_phone_number": restaurant_data.get('international_phone_number', ''),
        "maps_url": restaurant_data.get('maps_url', ''),
        "price_level": restaurant_data.get('price_level', ''),
        "rating": restaurant_data.get('rating', 0),
        "created_at": datetime.now(),
        # R√©int√©gration des champs utiles pour d'autres scripts :
        "reviews": restaurant_data.get('reviews', []),
        "images": restaurant_data.get('images', []),
        "notes_globales": restaurant_data.get('notes_globales', {}),
        "popular_times": restaurant_data.get('popular_times', {}),
        # Ajoute ici d'autres champs √† conserver si besoin
    }
    # Si tu veux normaliser/transformer ces champs, fais-le ici avant le return
    # (ex: transformer reviews en liste de dicts, images en URLs, etc.)
    return normalized_data

@timing_decorator
def fetch_html_with_brightdata(url, name=None, platform=None, max_retries=3):
    """
    R√©cup√®re le HTML d'une URL en utilisant BrightData avec mise en cache
    """
    log_prefix = f"[{name}] " if name else ""
    
    # V√©rifier d'abord dans le cache
    cache_key = f"{url}_{platform}"
    if cache_key in HTML_CACHE:
        print(f"{log_prefix}‚úÖ HTML r√©cup√©r√© depuis le cache")
        return HTML_CACHE[cache_key]
    
    # Configuration BrightData
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {BRIGHTDATA_TOKEN}"
    }
    
    payload = {
        "url": url,
        "zone": "web_unlocker1",
        "render": True,
        "format": "raw"
    }
    
    # S'assurer que max_retries est un entier
    max_retries = int(max_retries)
    
    for attempt in range(max_retries):
        try:
            print(f"{log_prefix}üîë Utilisation du token BrightData: {BRIGHTDATA_TOKEN[:8]}...")
            print(f"{log_prefix}üåê Envoi requ√™te BrightData pour: {platform or url}")
            
            response = requests.post(
                "https://api.brightdata.com/request",
                headers=headers,
                data=json.dumps(payload),
                timeout=120
            )
            
            if response.status_code == 200:
                print(f"{log_prefix}‚úÖ R√©ponse BrightData re√ßue")
                # Stocker dans le cache
                HTML_CACHE[cache_key] = response.text
                return response.text
            else:
                print(f"{log_prefix}‚ùå Erreur BrightData {response.status_code}: {response.text}")
                if attempt < max_retries - 1:
                    # Attente exponentielle plus courte si ce n'est pas une erreur 429 (rate limiting)
                    wait_time = 2 ** attempt if response.status_code == 429 else 1
                    print(f"{log_prefix}‚ö†Ô∏è Tentative {attempt + 2}/{max_retries} dans {wait_time}s")
                    time.sleep(wait_time)
                    
        except Exception as e:
            print(f"{log_prefix}‚ùå Erreur lors de la requ√™te BrightData: {str(e)}")
            if attempt < max_retries - 1:
                print(f"{log_prefix}‚ö†Ô∏è Tentative {attempt + 2}/{max_retries}")
                time.sleep(1)  # Attente plus courte en cas d'erreur de connexion
                
    return None

# Fonctions utilitaires pour l'extraction de donn√©es Google Maps

def extract_place_id(url):
    """
    Extrait le place_id d'une URL Google Maps
    
    Args:
        url: URL Google Maps
        
    Returns:
        place_id extrait ou None
    """
    if not url:
        return None
        
    # Recherche dans les param√®tres d'URL
    place_id_match = re.search(r'place_id=([^&]+)', url)
    if place_id_match:
        return place_id_match.group(1)
        
    # Si ce n'est pas trouv√©, il peut √™tre sous d'autres formats
    cid_match = re.search(r'cid=(\d+)', url)
    if cid_match:
        return f"ChIJ_{cid_match.group(1)}"
        
    # Autre format possible
    data_pid_match = re.search(r'data-pid="([^"]+)"', url)
    if data_pid_match:
        return data_pid_match.group(1)
        
    return None

def extract_coordinates(driver):
    """
    Extrait les coordonn√©es g√©ographiques depuis Google Maps
    
    Args:
        driver: Instance de WebDriver (d√©j√† sur la page Google Maps)
        
    Returns:
        Dictionnaire avec lat et lng ou None
    """
    if not driver:
        return None
        
    # M√©thode 1: Extraction depuis l'URL
    url = driver.current_url
    coords_match = re.search(r'@([-\d.]+),([-\d.]+)', url)
    if coords_match:
        return {
            "lat": float(coords_match.group(1)),
            "lng": float(coords_match.group(2))
        }
    
    # M√©thode 2: Extraction depuis la source de la page
    try:
        page_source = driver.page_source
        
        # Format standard JSON dans la page
        location_match = re.search(r'location":\s*{\s*"latitude":\s*([-\d.]+),\s*"longitude":\s*([-\d.]+)', page_source)
        if location_match:
            return {
                "lat": float(location_match.group(1)),
                "lng": float(location_match.group(2))
            }
            
        # Format alternatif
        coords_match = re.search(r'center=([^,]+),([^&]+)', page_source)
        if coords_match:
            return {
                "lat": float(coords_match.group(1)),
                "lng": float(coords_match.group(2))
            }
            
        # Format sp√©cifique √† l'API Google Maps
        latlng_match = re.search(r'LatLng\(([-\d.]+), ([-\d.]+)\)', page_source)
        if latlng_match:
            return {
                "lat": float(latlng_match.group(1)),
                "lng": float(latlng_match.group(2))
            }
    except Exception as e:
        if DEBUG_MODE:
            print(f"‚ö†Ô∏è Erreur lors de l'extraction des coordonn√©es: {str(e)}")
    
    return None

def url_to_base64(url, max_size=(800, 600), quality=90):
    """T√©l√©charge une image depuis une URL, la redimensionne et la convertit en URL data Base64."""
    if not url or not url.startswith(('http://', 'https://')):
        print(f"‚ö†Ô∏è URL d'image invalide: {url}")
        return None
    
    try:
        response = requests.get(url, stream=True, timeout=15) # Timeout court
        response.raise_for_status()
        
        # V√©rifier le type de contenu
        content_type = response.headers.get('content-type')
        if not content_type or not content_type.startswith('image/'):
            print(f"‚ö†Ô∏è L'URL ne pointe pas vers une image valide: {url} (type: {content_type})")
            return None
            
        # Lire le contenu
        image_bytes = response.content
        img = Image.open(BytesIO(image_bytes))
        
        # Redimensionner
        img.thumbnail(max_size, Image.Resampling.LANCZOS)
        
        # Convertir en RGB si n√©cessaire
        if img.mode in ('RGBA', 'LA', 'P'): # 'P' for palette mode
            # Cr√©er un fond blanc pour les images avec transparence ou palette
            alpha = img.convert('RGBA').split()[-1]
            background = Image.new("RGB", img.size, (255, 255, 255))
            background.paste(img, mask=alpha)
            img = background
        elif img.mode != 'RGB':
             img = img.convert('RGB')
             
        # Utiliser la fonction existante pour encoder (qui ajoute d√©j√† data:image/jpeg;base64,)
        base64_string = encode_image_base64(img)
        return base64_string
        
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur r√©seau lors du t√©l√©chargement de l'image {url}: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Erreur lors du traitement de l'image {url}: {e}")
        return None

if __name__ == "__main__":
    main()